{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About Me","text":"<p>Hello, I'm Sulman Khan, a data scientist with experience in end-to-end machine learning projects. I'm passionate about AI/ML, and I'll be showcasing my latest projects here. Feel free to examine my portfolio for more details. </p>"},{"location":"#writing","title":"Writing","text":"<ul> <li>Recommendation Systems: Overview</li> <li>RecSys Challenge 2024: Exploratory Data Analysis</li> <li>RecSys Challenge 2024: Model Selection</li> <li>Reddit AI Pulse (On-Prem)</li> <li>Reddit AI Pulse (Cloud)</li> <li>Concept Visualizer</li> <li>Beat Debate: AI-Powered Music Analysis Tool</li> </ul>"},{"location":"contact/","title":"Contact","text":"<p>Feel free to reach out me via email or linkedin!</p>"},{"location":"writing/","title":"Writing","text":"<ul> <li>Recommendation Systems: Overview</li> <li>RecSys Challenge 2024: Exploratory Data Analysis</li> <li>RecSys Challenge 2024: Model Selection</li> <li>From Reddit to Insights: Building an AI-Powered Data Pipeline with Gemini (On-Prem)</li> <li>From Reddit to Insights: Building an AI-Powered Data Pipeline with Gemini (Cloud)</li> <li>Concept Visualizer: Full-stack web application to build logos </li> <li>Beat Debate: AI-Powered Music Analysis Tool</li> </ul>"},{"location":"writing/2025/05/31/beatdebate-technical-deep-dive/","title":"BeatDebate \u2014 Technical Deep Dive","text":""},{"location":"writing/2025/05/31/beatdebate-technical-deep-dive/#introduction","title":"Introduction","text":"<p>Purpose</p> <p>BeatDebate is a proof-of-concept web app that shows how a large-language-model (LLM) planner can orchestrate specialised agents to deliver transparent, long-tail music recommendations in under seven seconds. </p> <p>Check out the project GitHub repository for the full code and detailed documentation. Here is the web application. Check out the AgentX course.</p> <p>Problem Statement</p> <p>Standard collaborative-filtering pipelines optimise clicks but amplify popularity bias and tell listeners nothing about why a song appears. BeatDebate flips the workflow: first an LLM writes an explicit machine-readable plan, then lightweight agents execute it, and finally a Judge agent converts plan weights into human-readable explanations.</p>"},{"location":"writing/2025/05/31/beatdebate-technical-deep-dive/#what-youll-learn","title":"What you\u2019ll learn","text":"<ul> <li>Designing an LLM-planned recommender \u2014 externalising reasoning as JSON so downstream agents become cheap and debuggable.  </li> <li>Using LangGraph for agent orchestration \u2014 a typed DAG with retries, time-outs, and state-passing.  </li> <li>Balancing novelty and relevance with dual advocate agents (Genre-Mood vs. Discovery).  </li> <li>Generating explanations by design rather than post-hoc.  </li> <li>Running at interactive speed on commodity hardware for &lt;$0.04 per query.</li> </ul>"},{"location":"writing/2025/05/31/beatdebate-technical-deep-dive/#system-architecture","title":"System Architecture","text":"<ol> <li>User query enters via a Gradio chat front-end.  </li> <li>PlannerAgent (Gemini-2.0 Flash) classifies intent, sets novelty/similarity weights, and emits a <code>planning_strategy</code> JSON.  A planner-agent system excels at translating that goal into a concrete, executable strategy.</li> <li>Genre-MoodAgent fetches stylistically coherent tracks; DiscoveryAgent hunts low-playcount gems.  </li> <li>JudgeAgent ranks and explains using the Planner\u2019s evaluation rubric.  </li> <li>Results stream back with Spotify previews.</li> </ol> <p>LangGraph moves a single <code>MusicRecommenderState</code> object through the graph, enforcing type safety and providing span-level tracing.</p>"},{"location":"writing/2025/05/31/beatdebate-technical-deep-dive/#scoring-ranking-the-judges-deliberation","title":"Scoring &amp; Ranking: The Judge's Deliberation","text":"<p>The heart of BeatDebate's decision-making lies in its two-stage evaluation process, handled by the JudgeAgent. This process is designed to be both intelligent and adaptable, moving beyond a single, rigid algorithm to dynamically define a \"good\" recommendation based on the user's specific intent.</p>"},{"location":"writing/2025/05/31/beatdebate-technical-deep-dive/#stage-1-component-based-scoring","title":"Stage 1: Component-Based Scoring","text":"<p>Before ranking, each potential track is evaluated across multiple dimensions by a ComprehensiveQualityScorer. This creates a 360-degree profile for every song:</p> <ul> <li> <p>Quality Score: Measures intrinsic musical quality using characteristics like energy, danceability, and emotional positivity (valence). This ensures we recommend well-produced and musically sound tracks.</p> </li> <li> <p>Novelty / Underground Score: This is the core \"discovery\" metric. Calculated from Last.fm listener and play-count data, a high novelty score indicates a track is likely an underground gem the user hasn't heard before. This directly combats the \"filter bubble\" of mainstream platforms.</p> </li> <li> <p>Contextual Relevance Score: Quantifies how well a track's metadata (artist, genre, tags) aligns with the user's query. If a user asks for \"sad indie,\" this score is high for tracks matching those terms.</p> </li> <li> <p>Similarity Score: For \"music like X\" queries, this score measures stylistic and sonic similarity, often using multi-hop artist connections to find nuanced and surprising links.</p> </li> </ul> <p>At the end of this stage, every candidate track has a rich profile of scores (e.g., Quality: 0.8, Novelty: 0.9, Relevance: 0.7).</p>"},{"location":"writing/2025/05/31/beatdebate-technical-deep-dive/#stage-2-intent-aware-ranking","title":"Stage 2: Intent-Aware Ranking","text":"<p>This is the system's strategic core. The \"best\" track for a discovery query is fundamentally different from the \"best\" track for a by artist query. The RankingEngine calculates a final weighted score, but the weights are provided by the PlannerAgent's strategy and change based on the user's intent. The power of the planning-centric approach becomes clear when observing how the <code>JudgeAgent</code>'s evaluation criteria adapt to different user intents. The <code>PlannerAgent</code> dynamically assigns different scoring weights based on the classified intent, ensuring the final recommendations are optimized for the user's specific goal. The table below illustrates this adaptive ranking strategy.</p> If the User Asks For... The Planner's Strategy Prioritizes... Why This is Beneficial \"Underground electronic music\"(Intent: <code>DISCOVERY</code>) Novelty: 50%  Underground: 30%  Quality: 15% The system understands the primary goal is finding something new. It heavily rewards tracks with low play-counts and intentionally de-prioritizes popular hits. \"Music by The Beatles\"(Intent: <code>BY_ARTIST</code>) Quality: 50%  Popularity: 30%  Recency: 20% The system focuses on the artist's most well-regarded and popular songs. Novelty is ignored, and popularity is rewarded to provide representative tracks. \"Music like Kendrick Lamar but jazzy\"(Intent: <code>HYBRID_SIMILARITY_GENRE</code>) Relevance: 45%  Quality: 30%  Diversity: 15% The system's main goal is to find tracks that match both constraints. It prioritizes the relevance score and ignores novelty to find the best examples of this genre fusion. \"Music for studying\"(Intent: <code>CONTEXTUAL</code>) Context Fit: 60%  Quality: 25%  Familiarity: 15% The system understands this is a functional request. It prioritizes tracks that fit the \"studying\" context (e.g., instrumental, low energy) and slightly favors familiar but not mainstream tracks, which can be less distracting."},{"location":"writing/2025/05/31/beatdebate-technical-deep-dive/#getting-started","title":"Getting Started","text":"<p><pre><code>git clone https://github.com/your-org/beatdebate\ncd beatdebate\nuv run python -m src.main\n# Point browser to http://localhost:7860\n</code></pre> Set these environment variables: <pre><code>LASTFM_API_KEY=your_lastfm_api_key_here\nLASTFM_SHARED_SECRET=your_lastfm_shared_secret_here\nSPOTIFY_CLIENT_ID=your_spotify_client_id_here\nSPOTIFY_CLIENT_SECRET=your_spotify_client_secret_here\n</code></pre></p>"},{"location":"writing/2025/05/31/beatdebate-technical-deep-dive/#1-the-backend-fastapi-langgraph-and-an-llm-planner","title":"1. The Backend: FastAPI, LangGraph, and an LLM Planner","text":"<p>The BeatDebate backend turns a chat prompt into a ranked, explained playlist in one HTTP call. Everything is Python 3.11; FastAPI exposes the <code>/recommend</code> endpoint, LangGraph orchestrates four agents, and Pydantic types the shared state.</p>"},{"location":"writing/2025/05/31/beatdebate-technical-deep-dive/#key-components-technologies","title":"Key Components &amp; Technologies","text":""},{"location":"writing/2025/05/31/beatdebate-technical-deep-dive/#fastapi","title":"FastAPI","text":"<p>Chosen for its async-first design (perfect for I/O-bound API calls to Last.fm, Spotify and Gemini) and native Pydantic validation.</p> <pre><code># src/api/backend.py (illustrative snippet)\nfrom fastapi import APIRouter, HTTPException # Corrected: FastAPI uses HTTPException\nfrom src.models.agent_models import MusicRecommenderState # Assuming this path\nfrom src.services.enhanced_recommendation_service import get_recommendation_service, RecommendationRequest # Using enhanced service\n\nrouter = APIRouter()\nrecommendation_service = get_recommendation_service() # Assuming service is initialized elsewhere\n\n@router.post(\"/recommendations\", response_model=list[str]) # Updated endpoint\nasync def recommend_tracks(request: RecommendationRequest): # Updated request model\n    if not recommendation_service:\n        raise HTTPException(status_code=503, detail=\"Recommendation service not available\")\n\n    result = await recommendation_service.get_recommendations(request)\n    return [track.name for track in result.recommendations] # Simplified for example\n</code></pre>"},{"location":"writing/2025/05/31/beatdebate-technical-deep-dive/#langgraph","title":"LangGraph","text":"<p>A tiny DAG with four nodes; advocates run in parallel.</p> <pre><code># src/services/recommendation_engine.py (excerpt based on your LangGraph design)\nfrom langgraph.graph import StateGraph, END\nfrom src.models.agent_models import MusicRecommenderState\n# ... import agent classes ...\n\nclass RecommendationEngine:\n    # ...\n    def _build_workflow_graph(self) -&gt; StateGraph: # Corrected method name\n        workflow = StateGraph(MusicRecommenderState)\n        workflow.add_node(\"planner\", self._planner_node)\n        workflow.add_node(\"genre_mood_advocate\", self._genre_mood_node)\n        workflow.add_node(\"discovery_advocate\", self._discovery_node)\n        workflow.add_node(\"judge\", self._judge_node)\n\n        workflow.set_entry_point(\"planner\")\n        # ... connect nodes ...\n        # For example:\n        workflow.add_conditional_edges(\n            \"planner\",\n            self._route_agents, # Example router\n            {\n                \"discovery_only\": \"discovery_advocate\",\n                \"genre_mood_only\": \"genre_mood_advocate\",\n                \"both_agents\": \"discovery_advocate\", # Start of parallel execution\n                \"judge_only\": \"judge\"\n            }\n        )\n        # ... other edges for parallel flow and to judge ...\n        workflow.add_edge(\"judge\", END)\n        return workflow.compile()\n    # ...\n</code></pre>"},{"location":"writing/2025/05/31/beatdebate-technical-deep-dive/#pydantic","title":"Pydantic","text":"<p>Pydantic is the backbone for data validation and settings management throughout BeatDebate. Its primary role in the backend is defining the <code>MusicRecommenderState</code> model (<code>src/models/agent_models.py</code>). <pre><code># src/models/agent_models.py (abridged - MusicRecommenderState)\nfrom typing import Dict, List, Any, Optional, Annotated\nfrom pydantic import BaseModel, Field\n\n# Reducer examples (from your agent_models.py)\ndef keep_first(x: Any, y: Any) -&gt; Any:\n    if x is None or (isinstance(x, (list, dict, str)) and len(x) == 0): return y\n    return x\n\ndef list_append_reducer(x: List, y: List) -&gt; List:\n    if x is None: x = []\n    if y is None: y = []\n    return x + y\n\nclass MusicRecommenderState(BaseModel):\n    user_query: Annotated[str, keep_first] = Field(...)\n    # ... many other fields like planning_strategy, entities, etc. ...\n    genre_mood_recommendations: Annotated[List[Dict], list_append_reducer] = Field(default_factory=list)\n    discovery_recommendations: Annotated[List[Dict], list_append_reducer] = Field(default_factory=list)\n    final_recommendations: Annotated[List[Dict], list_append_reducer] = Field(default_factory=list) # Corrected reducer based on typical usage\n    reasoning_log: Annotated[List[str], list_append_reducer] = Field(default_factory=list)\n    # ...\n</code></pre></p>"},{"location":"writing/2025/05/31/beatdebate-technical-deep-dive/#layered-architecture","title":"Layered Architecture","text":""},{"location":"writing/2025/05/31/beatdebate-technical-deep-dive/#api-layer","title":"API Layer","text":"<ul> <li>(<code>src/api/</code>):<ul> <li> <p>This layer serves as the unified gateway for all external service interactions, ensuring consistency, robustness, and maintainability. It's not just about exposing endpoints with FastAPI but also about how the application consumes external music data APIs.</p> <ul> <li> <p>(<code>src/api/backend.py</code>):</p> <ul> <li> <p>Exposes the primary HTTP endpoints (like <code>/recommendations</code>, <code>/planning</code>) for the Gradio frontend and potentially other clients.</p> </li> <li> <p>Handles request validation using Pydantic models defined in <code>src/models/</code>.</p> </li> <li> <p>Orchestrates calls to the EnhancedRecommendationService to initiate the agent workflow.</p> </li> <li> <p>Manages application lifecycle events (startup/shutdown) for initializing and closing resources like the recommendation service and API clients (as seen in lifespan manager in <code>src/api/backend.py</code>).</p> </li> </ul> <pre><code># src/api/backend.py (Illustrative Snippet)\nfrom fastapi import FastAPI, HTTPException, Depends # Ensure Depends is imported\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional, Dict, Any # Ensure necessary typing imports\n\n# Assuming these are correctly pathed in your project\nfrom src.services.enhanced_recommendation_service import (\n    EnhancedRecommendationService,\n    RecommendationRequest as ServiceRequest, # Renaming to avoid Pydantic model conflict\n    RecommendationResponse as ServiceResponse,\n    get_recommendation_service\n)\nfrom src.models.metadata_models import UnifiedTrackMetadata # For response model\n\n# Placeholder for lifespan manager and other initializations\n# async def lifespan(app: FastAPI): ...\n# app = FastAPI(lifespan=lifespan)\napp = FastAPI() # Simplified for snippet\n\n# --- Pydantic Model for API Request ---\nclass APIRecommendationRequest(BaseModel):\n    query: str = Field(..., min_length=3, max_length=500, description=\"User's music preference query\")\n    session_id: Optional[str] = Field(None, description=\"Session ID for conversation context\")\n    max_recommendations: int = Field(10, ge=1, le=20, description=\"Number of recommendations\")\n    # Add other relevant fields from your actual RecommendationRequest\n\n# --- Pydantic Model for API Response Track ---\nclass APITrackResponse(BaseModel):\n    title: str\n    artist: str\n    album: Optional[str] = None\n    preview_url: Optional[str] = None\n    explanation: Optional[str] = None\n    # Add other relevant fields from your UnifiedTrackMetadata or TrackRecommendation\n\n# --- Pydantic Model for API Response ---\nclass APIRecommendationResponse(BaseModel):\n    recommendations: List[APITrackResponse]\n    reasoning_log: List[str]\n    session_id: str\n    # Add other relevant fields like strategy_used, processing_time etc.\n\n@app.post(\"/recommendations\", response_model=APIRecommendationResponse)\nasync def get_recommendations(\n    request: APIRecommendationRequest,\n    # Using a dependency function to get the service instance\n    # This assumes get_recommendation_service is properly set up to provide an initialized service\n    recommendation_service: EnhancedRecommendationService = Depends(get_recommendation_service)\n):\n    if not recommendation_service:\n        raise HTTPException(status_code=503, detail=\"Recommendation service not available\")\n\n    try:\n        # Map API request model to service request model\n        service_request_data = ServiceRequest(\n            query=request.query,\n            session_id=request.session_id,\n            max_recommendations=request.max_recommendations\n            # Map other fields as necessary, e.g., chat_context\n        )\n\n        # Call the enhanced recommendation service\n        service_response: ServiceResponse = await recommendation_service.get_recommendations(service_request_data)\n\n        # Transform service response (List[UnifiedTrackMetadata]) to API response (List[APITrackResponse])\n        api_tracks = []\n        for track_meta in service_response.recommendations:\n            api_tracks.append(APITrackResponse(\n                title=track_meta.name,\n                artist=track_meta.artist,\n                album=track_meta.album,\n                preview_url=track_meta.preview_url,\n                explanation=track_meta.recommendation_reason\n                # Map other fields\n            ))\n\n        return APIRecommendationResponse(\n            recommendations=api_tracks,\n            reasoning_log=service_response.reasoning,\n            session_id=service_response.session_id\n            # Map other fields\n        )\n    except Exception as e:\n        # Log the error using your structured logger\n        # logger.error(\"Recommendation processing error\", exc_info=True)\n        raise HTTPException(status_code=500, detail=f\"An unexpected error occurred: {str(e)}\")\n</code></pre> </li> <li> <p>Client Abstraction &amp; Standardization:</p> <ul> <li> <p><code>base_client.py</code> (<code>BaseAPIClient</code>): This is a crucial piece for code reuse. It provides a foundational class for all external API clients (Last.fm, Spotify). Key responsibilities include:</p> </li> <li> <p>Unified HTTP request handling using <code>aiohttp.ClientSession</code> for asynchronous operations.</p> </li> <li> <p>Standardized error handling and retry mechanisms (e.g., exponential backoff).</p> </li> <li> <p>Integration with the UnifiedRateLimiter to ensure all API calls respect external service rate limits.</p> </li> <li> <p>Abstracting common logic like parsing JSON responses and handling API-specific error formats (subclasses implement <code>_extract_api_error</code>).</p> </li> <li> <p><code>lastfm_client.py</code> (<code>LastFmClient</code>) and <code>spotify_client.py</code> (<code>SpotifyClient</code>): These are concrete implementations inheriting from <code>BaseAPIClient</code>.</p> <ul> <li> <p>They encapsulate all logic specific to interacting with the Last.fm and Spotify Web APIs, respectively. This includes endpoint definitions, parameter formatting, and parsing service-specific response structures into standardized Pydantic models (like <code>TrackMetadata</code> from <code>src/api/lastfm_client.py</code> or <code>SpotifyTrack</code> from <code>src/api/spotify_client.py</code>).</p> </li> <li> <p>For example, <code>LastFmClient</code> handles searching tracks, getting track info, similar tracks, and artist info, while <code>SpotifyClient</code> manages authentication (OAuth Client Credentials flow) and fetching track details or audio features.</p> </li> </ul> <pre><code># src/api/lastfm_client.py (Illustrative snippet of _make_lastfm_request)\nclass LastFmClient(BaseAPIClient):\n    # ...\n    async def _make_lastfm_request(\n        self, \n        method: str, \n        params: Optional[Dict[str, Any]] = None,\n        # ...\n    ) -&gt; Dict[str, Any]:\n        request_params = {\n            \"method\": method,\n            \"api_key\": self.api_key,\n            \"format\": \"json\",\n            **(params or {})\n        }\n        return await self._make_request( # Calls BaseAPIClient's method\n            endpoint=\"\", # Last.fm uses query params on base URL\n            params=request_params,\n            # ...\n        )\n</code></pre> </li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"writing/2025/05/31/beatdebate-technical-deep-dive/#agent-layer","title":"Agent Layer","text":"<ul> <li> <p>(<code>src/agent/</code>):</p> <ul> <li> <p>This layer is where the core intelligence for music discovery, analysis, and decision-making resides. Instead of a monolithic AI, BeatDebate employs a team of specialized agents, each with a distinct role, working in concert to fulfill the user's request. This design promotes modularity, specialization, and transparent reasoning.</p> </li> <li> <p><code>base_agent.py</code> (<code>BaseAgent</code>):</p> <ul> <li>This abstract base class provides common scaffolding for all agents.</li> <li>It includes shared functionalities like integration with the LLM client (Gemini), standardized logging via <code>structlog</code>, performance monitoring hooks (tracking processing times, success/error counts), and basic error handling.</li> <li>A key feature is the management of a <code>_reasoning_steps</code> list, allowing each agent to document its decision-making process, contributing to the overall explainability of the system.   <pre><code># src/agents/base_agent.py (Illustrative Snippet)\nclass BaseAgent(ABC):\n    def __init__(self, config: AgentConfig, llm_client, api_service, metadata_service, rate_limiter):\n        self.config = config\n        self.agent_name = config.agent_name\n        # ... other initializations for llm_utils, logger, etc. ...\n        self._reasoning_steps: List[str] = [] # Initialize here\n\n    @abstractmethod\n    async def process(self, state: MusicRecommenderState) -&gt; MusicRecommenderState:\n        pass\n\n    def add_reasoning_step(self, step: str, confidence: float = 0.8):\n        self._reasoning_steps.append(step)\n        self.logger.debug(\"Reasoning step added\", step=step, confidence=confidence)\n</code></pre></li> </ul> </li> <li> <p>Shared Agent Components (<code>src/agents/components/</code>):</p> <ul> <li>To avoid code duplication and promote reusability within the agent layer, common utilities used by multiple agents are centralized here. This is a key design choice for maintainability.</li> <li><code>llm_utils.py</code> (<code>LLMUtils</code>): Standardizes calls to the Gemini LLM, including prompt construction, JSON response parsing, error handling, and rate limiting integration. All agents use this for their LLM interactions.</li> <li><code>entity_extraction_utils.py</code> (<code>EntityExtractionUtils</code>): Provides pattern-based methods for extracting entities like artists and genres. This serves as a fallback or supplement to LLM-based extraction.</li> <li><code>query_analysis_utils.py</code> (<code>QueryAnalysisUtils</code>): Contains helpers for analyzing query characteristics (intent, complexity, mood indicators) using pattern matching.</li> <li><code>unified_candidate_generator.py</code> (<code>UnifiedCandidateGenerator</code>): A crucial component that consolidates track candidate generation logic. Both <code>GenreMoodAgent</code> and <code>DiscoveryAgent</code> use this to fetch an initial pool of potential tracks based on different strategies (e.g., genre-focused, discovery-focused), ensuring consistent data fetching.</li> <li><code>quality_scorer.py</code> (<code>ComprehensiveQualityScorer</code>): Implements a multi-dimensional system for scoring track quality, considering audio features (placeholder for now), popularity balance, user engagement signals (placeholder), and genre/mood fit. Used by advocate agents and the judge.</li> </ul> </li> <li> <p>Specialized Agents (each in its own subdirectory like <code>src/agents/planner/</code>):</p> <ul> <li> <p><code>PlannerAgent</code> (<code>src/agents/planner/agent.py</code>):</p> <ul> <li>The strategic coordinator. It's the first agent to process the user query.</li> <li>Utilizes the <code>QueryUnderstandingEngine</code> (from <code>src/agents/planner/query_understanding_engine.py</code>) which leverages <code>LLMUtils</code> for deep query analysis.</li> <li>Its primary output is the <code>planning_strategy</code> which includes <code>task_analysis</code>, <code>coordination_strategy</code> for advocate agents, and an <code>evaluation_framework</code> for the <code>JudgeAgent</code>. This structured plan is the core of BeatDebate's explainable AI approach.</li> <li>The enhanced version also incorporates <code>EntityRecognizer</code> (which might use <code>EntityExtractionUtils</code> and <code>LLMUtils</code>) and <code>ConversationContextManager</code> for richer, context-aware planning.   <pre><code># src/agents/planner/agent.py (Illustrative Snippet)\nclass PlannerAgent(BaseAgent):\n    async def process(self, state: MusicRecommenderState) -&gt; MusicRecommenderState:\n        # Phase 1: Query Understanding (uses QueryUnderstandingEngine)\n        query_understanding = await self._understand_user_query(state.user_query)\n        state.query_understanding = query_understanding\n        state.entities = self._convert_understanding_to_entities(query_understanding)\n\n        # Phase 2: Task Analysis (uses LLMUtils or fallbacks)\n        task_analysis = await self._analyze_task_complexity(state.user_query, query_understanding)\n        state.intent_analysis = task_analysis # Critical for other agents\n\n        # Phase 3: Planning Strategy Creation (internal logic)\n        planning_strategy = await self._create_planning_strategy(query_understanding, task_analysis)\n        state.planning_strategy = planning_strategy\n        # ...\n        return state\n</code></pre></li> </ul> </li> <li> <p><code>GenreMoodAgent</code> (<code>src/agents/genre_mood/agent.py</code>):</p> <ul> <li>Focuses on finding tracks that match the user's specified or inferred genre and mood preferences.</li> <li>Takes its part of the <code>coordination_strategy</code> from the <code>PlannerAgent</code>.</li> <li>Uses the <code>UnifiedCandidateGenerator</code> with a \"genre_mood\" strategy and <code>QualityScorer</code>.</li> <li>May employ helper modules like <code>MoodLogic</code> and <code>TagGenerator</code> for specialized processing. <pre><code># src/agents/genre_mood/agent.py (Illustrative Snippet - Process Overview)\nclass GenreMoodAgent(BaseAgent):\n    async def process(self, state: MusicRecommenderState) -&gt; MusicRecommenderState:\n        self.logger.info(\"Starting genre/mood agent processing\")\n        entities = state.entities or {}\n        intent_analysis = state.intent_analysis or {}\n\n        # Adapt parameters based on detected intent (e.g., 'genre_mood', 'contextual')\n        self._adapt_to_intent(intent_analysis.get('intent', 'genre_mood'))\n\n        # Generate candidates using shared generator\n        candidates = await self.candidate_generator.generate_candidate_pool(\n            entities=entities,\n            intent_analysis=intent_analysis,\n            agent_type=\"genre_mood\",\n            target_candidates=self.target_candidates,\n            detected_intent=intent_analysis.get('intent', 'genre_mood')\n        )\n        self.logger.debug(f\"Generated {len(candidates)} candidates\")\n\n        # Score candidates for genre/mood fit and quality\n        scored_candidates = await self._score_candidates(candidates, entities, intent_analysis)\n\n        # Filter and rank based on requirements\n        filtered_candidates = await self._filter_by_genre_requirements(scored_candidates, entities, self.llm_client)\n\n        # Apply diversity based on context (handles by_artist, followup etc.)\n        context_override_data = getattr(state, 'context_override', None)\n        filtered_candidates = self._ensure_diversity(\n            filtered_candidates, entities, intent_analysis, context_override_data\n        )\n\n        recommendations = await self._create_recommendations(\n            filtered_candidates[:self.final_recommendations], entities, intent_analysis\n        )\n        state.genre_mood_recommendations = [rec.model_dump() for rec in recommendations]\n        self.logger.info(f\"Genre/mood agent completed, {len(recommendations)} recommendations.\")\n        return state\n\n    async def _calculate_genre_mood_score(self, candidate, entities, intent_analysis) -&gt; float:\n        # Uses self.api_service to check genre match via track/artist tags\n        # Considers mood_mappings, energy_mappings from MoodLogic\n        # ... detailed scoring logic ...\n        score = 0.0\n        target_genres = self._extract_target_genres(entities)\n        for genre in target_genres:\n            match_result = await self._check_genre_match(candidate, genre)\n            if match_result.get('matches'):\n                score += 0.6 * match_result.get('confidence', 0.5) # Boost by confidence\n        # ... add mood and energy scoring ...\n        return min(score, 1.0)\n</code></pre></li> </ul> </li> <li> <p><code>DiscoveryAgent</code> (<code>src/agents/discovery/agent.py</code>):</p> <ul> <li>Specializes in finding novel, under-the-radar, or serendipitous tracks.</li> <li>Also guided by the <code>PlannerAgent</code>'s <code>coordination_strategy</code>.</li> <li>Leverages <code>UnifiedCandidateGenerator</code> with a \"discovery\" strategy and <code>QualityScorer</code>.</li> <li>Its internal logic might involve components like <code>SimilarityExplorer</code> and <code>UndergroundDetector</code> for multi-hop similarity and identifying obscure artists. <pre><code># src/agents/discovery/agent.py (Illustrative Snippet - Process Overview)\nclass DiscoveryAgent(BaseAgent):\n    async def process(self, state: MusicRecommenderState) -&gt; MusicRecommenderState:\n        entities = state.entities or {}\n        intent_analysis = state.intent_analysis or {}\n        self._adapt_to_intent(intent_analysis.get('intent', 'discovery'))\n\n        # Generate candidates using discovery-focused strategy\n        candidates = await self.candidate_generator.generate_candidate_pool(\n            entities=entities,\n            intent_analysis=intent_analysis,\n            agent_type=\"discovery\",\n            # ...\n        )\n        # Score, filter, and create recommendations\n        # ...\n        state.discovery_recommendations = [rec.model_dump() for rec in recommendations]\n        return state\n</code></pre></li> </ul> </li> <li> <p><code>JudgeAgent</code> (<code>src/agents/judge/agent.py</code>):</p> <ul> <li>The final arbiter. It evaluates candidates from both advocate agents.</li> <li>Uses the <code>evaluation_framework</code> (weights, diversity targets) from the <code>PlannerAgent</code>'s strategy.</li> <li>Employs <code>RankingLogic</code> for scoring and selection, and <code>ConversationalExplainer</code> (which uses <code>LLMUtils</code>) to generate human-readable explanations for each chosen track.</li> <li>The \"Enhanced JudgeAgent\" (as per design docs) incorporates more sophisticated prompt-driven ranking, contextual relevance, and discovery appropriateness scoring. <pre><code># src/agents/judge/agent.py (Illustrative Snippet - Process Overview)\nclass JudgeAgent(BaseAgent):\n    async def process(self, state: MusicRecommenderState) -&gt; MusicRecommenderState:\n        self.logger.info(\"Starting judge agent processing\")\n        all_candidates = self._collect_all_candidates(state) # From GenreMood &amp; Discovery\n\n        if not all_candidates:\n            state.final_recommendations = []\n            return state\n\n        # Score all candidates (quality, relevance, intent alignment, etc.)\n        # This step internally uses QualityScorer and context/intent specific logic\n        scored_candidates_with_details = await self._score_all_candidates(all_candidates, state)\n\n        # Rank candidates using intent-aware RankingLogic\n        # The intent is pulled from state.query_understanding.intent\n        ranked_candidates = await self._rank_candidates(scored_candidates_with_details, state)\n\n        # Select final recommendations with diversity\n        # The diversity logic also considers the intent (e.g., allowing more from one artist if 'by_artist')\n        final_selections = self._select_with_diversity(ranked_candidates, state)\n\n        final_recommendations_with_explanations = await self._generate_explanations(final_selections, state)\n\n        # Convert to dict for state (as per MusicRecommenderState annotation)\n        state.final_recommendations = [rec.model_dump() for rec in final_recommendations_with_explanations]\n        self.logger.info(f\"Judge agent completed, {len(state.final_recommendations)} final recommendations.\")\n        return state\n\n    async def _rank_candidates(self, scored_candidates_with_details, state):\n        # Determine intent (e.g., 'artist_similarity', 'hybrid_discovery_primary')\n        intent = state.query_understanding.intent.value if state.query_understanding else 'balanced'\n        if intent == 'hybrid': # Refine to sub-type if hybrid\n            intent = self._detect_hybrid_subtype(state)\n\n        # Get weights and novelty threshold based on this refined intent\n        scoring_weights = self.ranking_logic.get_intent_weights(intent, state.entities, state.intent_analysis)\n        novelty_threshold = self.ranking_logic.get_novelty_threshold(intent)\n\n        # Rank using these intent-specific parameters\n        return self.ranking_logic.rank_recommendations(\n            candidates=scored_candidates_with_details,\n            intent=intent,\n            entities=state.entities,\n            intent_analysis=state.intent_analysis,\n            novelty_threshold=novelty_threshold,\n            scoring_weights=scoring_weights\n        )\n</code></pre></li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"writing/2025/05/31/beatdebate-technical-deep-dive/#services","title":"Services","text":"<ul> <li>(<code>src/services/</code>):<ul> <li> <p>This layer acts as an intermediary between the API/UI and the agent system, managing workflows and shared resources.</p> <ul> <li> <p><code>enhanced_recommendation_service.py</code> (<code>EnhancedRecommendationService</code>):</p> <ul> <li>This is the primary service responsible for handling a user's recommendation request from end-to-end.</li> <li>Agent Initialization: It instantiates all agents (<code>PlannerAgent</code>, <code>GenreMoodAgent</code>, <code>DiscoveryAgent</code>, <code>JudgeAgent</code>), injecting necessary dependencies like the <code>APIService</code>, <code>MetadataService</code>, LLM client, and rate limiters. This dependency injection pattern is crucial for creating testable and configurable agents.</li> <li>LangGraph Workflow Orchestration: It defines and compiles the LangGraph state graph. This graph dictates the flow of execution:<ol> <li><code>PlannerAgent</code> runs first.</li> <li>Based on the planner's output (specifically <code>agent_sequence</code> in the <code>planning_strategy</code>), it conditionally routes to either <code>DiscoveryAgent</code> only, <code>GenreMoodAgent</code> only, or both in parallel.</li> <li>Outputs from advocate agents are collected in the <code>MusicRecommenderState</code>.</li> <li><code>JudgeAgent</code> runs last to produce the final recommendations.</li> </ol> </li> <li>State Management: It initiates the <code>MusicRecommenderState</code> and manages its passage through the LangGraph workflow.</li> <li>Response Formatting: It converts the final agent outputs into a user-friendly <code>RecommendationResponse</code>, often involving calls to <code>APIService</code> to enrich track data (e.g., adding Spotify preview URLs). <pre><code># src/services/enhanced_recommendation_service.py (Illustrative Snippet - Graph Building)\nclass EnhancedRecommendationService:\n    def _build_workflow_graph(self) -&gt; StateGraph:\n        workflow = StateGraph(MusicRecommenderState)\n        workflow.add_node(\"planner\", self._planner_node)\n        # ... add advocate and judge nodes ...\n        workflow.set_entry_point(\"planner\")\n        workflow.add_conditional_edges(\"planner\", self._route_agents, ...)\n        # ... define other edges for the workflow ...\n        workflow.add_edge(\"judge\", END)\n        return workflow.compile()\n\n    async def get_recommendations(self, request: ServiceRequest) -&gt; ServiceResponse:\n        await self.initialize_agents() # Ensure agents are ready\n        # ... create initial MusicRecommenderState ...\n        # ... (NEW) Analyze context using self.context_analyzer ...\n        workflow_state = MusicRecommenderState(user_query=request.query, ..., context_override=context_override_data)\n        final_state = await self.graph.ainvoke(workflow_state)\n        # ... process final_state and convert to ServiceResponse ...\n        return ServiceResponse(...)\n</code></pre></li> </ul> </li> <li> <p><code>api_service.py</code> (<code>APIService</code>): (Described in the API Layer, but used extensively by services)</p> <ul> <li>Provides a centralized and abstracted way for all services (and subsequently agents, via dependency injection) to interact with external music APIs like Last.fm and Spotify.</li> <li>It uses the <code>APIClientFactory</code> to get configured <code>LastFmClient</code> and <code>SpotifyClient</code> instances, ensuring shared rate limiters and session management.</li> </ul> </li> <li> <p><code>metadata_service.py</code> (<code>MetadataService</code>):</p> <ul> <li>Offers unified functions for fetching and processing track and artist metadata.</li> <li>It can combine information from multiple sources (e.g., Last.fm and Spotify) into a <code>UnifiedTrackMetadata</code> or <code>UnifiedArtistMetadata</code> object, providing a consistent data view to the rest of the application.</li> </ul> </li> <li> <p><code>conversation_context_service.py</code> (<code>ConversationContextManager</code>):</p> <ul> <li>Manages the history of interactions within a user session.</li> <li>Tracks queries, extracted entities from those queries, recommendations provided, and any user feedback.</li> <li>This service is crucial for enabling multi-turn conversations and allowing the system to understand evolving user preferences.</li> </ul> </li> <li> <p><code>smart_context_manager.py</code> (<code>SmartContextManager</code>):</p> <ul> <li>Works in tandem with the <code>ConversationContextManager</code>.</li> <li>Its key responsibility is to intelligently decide whether to maintain the current conversation context, modify it, or reset it entirely based on the user's new query.</li> <li>It analyzes the new query for intent changes (e.g., switching from \"music like Artist A\" to \"music like Artist B\", or from \"workout music\" to \"study music\") and explicit reset triggers (e.g., \"never mind\").</li> <li>The <code>EnhancedRecommendationService</code> uses this manager (via its <code>ContextAwareIntentAnalyzer</code> which uses LLM capabilities) before invoking the agent workflow to determine the appropriate context to pass to the <code>PlannerAgent</code>. <pre><code># src/services/enhanced_recommendation_service.py (Illustrative usage of context_analyzer)\n# (Inside get_recommendations method)\n# ...\ncontext_override = await self.context_analyzer.analyze_context(\n    request.query, conversation_history # conversation_history fetched via self.context_manager\n)\n# ...\nworkflow_state = MusicRecommenderState(..., context_override=context_override)\n# ...\n</code></pre></li> </ul> </li> <li> <p><code>cache_manager.py</code> (<code>CacheManager</code>):</p> <ul> <li>Provides a file-based caching layer (using <code>diskcache</code>) for API responses, track metadata, and potentially agent strategies.</li> <li>It helps reduce redundant API calls, improve performance, and stay within API rate limits. Different cache types (e.g., \"lastfm\", \"spotify\", \"tracks\") with configurable TTLs are managed.</li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>Okay, here's the \"Frontend\" section for your blog post, drawing from your UI components (<code>src/ui/</code>) and the overall system design.</p>"},{"location":"writing/2025/05/31/beatdebate-technical-deep-dive/#2-the-frontend-gradio-for-interactive-music-discovery","title":"2. The Frontend: Gradio for Interactive Music Discovery","text":"<p>The user interface for BeatDebate is built using Gradio, enabling a rapid, interactive, and chat-first experience. Gradio was chosen for its simplicity in creating web UIs for machine learning models and agent-based systems, and its seamless integration with Python backends. The frontend is designed to be intuitive, providing not just recommendations but also insights into the AI's decision-making process.</p>"},{"location":"writing/2025/05/31/beatdebate-technical-deep-dive/#key-ui-components-features","title":"Key UI Components &amp; Features","text":"<ul> <li>(<code>src/ui/</code>):<ul> <li> <p>This is where the user interface components of the web application are located.</p> <ul> <li> <p><code>chat_interface.py</code> (<code>BeatDebateChatInterface</code>):</p> <ul> <li>This is the central component, creating the primary chat window where users interact with BeatDebate.</li> <li>Chat Input &amp; Display: Provides a familiar textbox for users to type their music queries and a chat area to display the conversation history (user prompts and agent responses).</li> <li>Example Prompts: To help users get started, the interface (as seen in <code>create_interface</code>) presents clickable example queries categorized by intent (Artist Similarity, Discovery, Genre/Mood, Contextual, Hybrid). This showcases the system's versatility.     <pre><code># src/ui/chat_interface.py (Illustrative - Example Button Setup)\n# (Inside create_interface method)\n# ...\nwith gr.Row():\n    with gr.Column(scale=1):\n        gr.Markdown(\"**\ud83c\udfaf Artist Similarity**\")\n        for example in QUERY_EXAMPLES[\"Artist Similarity\"]:\n            btn = gr.Button(example, elem_classes=[\"example-chip\"], ...)\n            example_buttons.append((btn, example))\n# ...\n# Example button handlers\nfor btn, example_text in example_buttons:\n    btn.click(fn=lambda x=example_text: x, inputs=[], outputs=[msg_input])\n</code></pre></li> <li>Asynchronous Processing: The <code>process_message</code> method handles user input by making an asynchronous HTTP POST request to the FastAPI backend's <code>/recommendations</code> endpoint. This ensures the UI remains responsive while the backend agents work.     <pre><code># src/ui/chat_interface.py (Illustrative - process_message)\nclass BeatDebateChatInterface:\n    async def process_message(self, message: str, history: List[Tuple[str, str]]):\n        # ...\n        recommendations_response = await self._get_recommendations(message)\n        if recommendations_response:\n            formatted_response = self.response_formatter.format_recommendations(recommendations_response)\n            history.append((message, formatted_response))\n            # ... update internal conversation_history for context ...\n            lastfm_player_html = self._create_lastfm_player_html(...)\n            return \"\", history, lastfm_player_html # Clears input, updates chat, updates player\n        # ... error handling ...\n</code></pre></li> <li>Contextual History: The <code>BeatDebateChatInterface</code> maintains <code>self.conversation_history</code>. This history (recent queries and first track of recommendations) is passed back to the backend with each new request, enabling the <code>SmartContextManager</code> and <code>ContextAwareIntentAnalyzer</code> to understand multi-turn conversations and provide contextually relevant responses.</li> </ul> </li> <li> <p><code>response_formatter.py</code> (<code>ResponseFormatter</code>):</p> <ul> <li>This class is responsible for taking the JSON response from the backend (which includes the list of <code>UnifiedTrackMetadata</code> objects and reasoning logs) and transforming it into rich, human-readable Markdown suitable for display in the Gradio chat.</li> <li>Track Display: Each recommended track is formatted with its title, artist, a confidence badge (color-coded based on the recommendation score), and the source agent.</li> <li>External Links: It generates direct links for each track to Last.fm, Spotify (search), and YouTube (search), making it easy for users to listen.</li> <li>Reasoning Visibility: It includes an expandable \"View Detailed Agent Reasoning\" section, displaying the <code>reasoning_log</code> from the backend. This directly addresses the goal of explainable AI.     <pre><code># src/ui/response_formatter.py (Illustrative - Single Recommendation Formatting)\nclass ResponseFormatter:\n    def _format_single_recommendation(self, rec: Dict[str, Any], rank: int) -&gt; str:\n        title = rec.get(\"title\", \"Unknown Title\")\n        artist = rec.get(\"artist\", \"Unknown Artist\")\n        confidence = rec.get(\"confidence\", 0.0) * 100\n        # ... logic for confidence badge color ...\n\n        # Generates links using f-strings to Last.fm, Spotify, YouTube\n        lastfm_url = f\"https://www.last.fm/search?q={artist}+{title}\".replace(\" \", \"+\")\n        # ...\n\n        markdown = [\n            f\"## {rank}. \\\"{title}\\\" by {artist}\",\n            f\"{confidence_badge_html} \u2022 *via {rec.get('source', 'unknown')}*\",\n            f\"\ud83c\udfa7 **[Listen on Last.fm]({lastfm_url})** ...\"\n        ]\n        # ... add reasoning, genres, moods ...\n        return \"\\\\n\".join(markdown)\n</code></pre></li> </ul> </li> <li> <p>Interactive Track Information Panel:</p> <ul> <li>A dedicated section of the UI (managed by <code>_create_lastfm_player_html</code> in <code>chat_interface.py</code>) dynamically updates to show details of the latest set of recommendations.</li> <li>Instead of an embedded player (which can be complex with API limitations), it provides styled cards for each track from the most recent response. Each card includes:<ul> <li>Artist and Title.</li> <li>A \"Confidence\" badge indicating the system's score for that track.</li> <li>Direct links to search for the track on Last.fm, Spotify, and YouTube.</li> </ul> </li> <li>This panel uses HTML directly rendered by a <code>gr.HTML</code> component and is updated after each successful recommendation. The styling is dark-mode compatible and uses visual cues (like border colors) to reflect recommendation confidence.</li> </ul> </li> <li> <p>Styling and Layout:</p> <ul> <li>The interface uses <code>gr.Blocks</code> for a custom layout, allowing for a main chat area alongside the track information panel and example prompts.</li> <li>Custom CSS is applied via the <code>css</code> parameter in <code>gr.Blocks</code> to achieve the \"BeatDebate\" theme (dark mode, violet/blue accents, modern feel). This includes styling for chat bubbles, input areas, buttons, and the track info panel for a polished look.</li> </ul> </li> <li> <p><code>planning_display.py</code> (<code>PlanningDisplay</code>): (Though not directly shown as a separate Gradio component in <code>chat_interface.py</code>, its formatting logic would be invoked if the backend <code>/planning</code> endpoint's output were to be displayed).</p> <ul> <li>This component is designed to take the detailed <code>planning_strategy</code> JSON from the <code>PlannerAgent</code> (via the backend) and format it into a structured, readable HTML representation.</li> <li>It would break down the strategy into sections: Task Analysis (primary goal, complexity), Agent Coordination (specific instructions for <code>GenreMoodAgent</code> and <code>DiscoveryAgent</code>), Evaluation Criteria (weights, diversity targets), and Execution Monitoring.</li> <li>This visualization is key for the AgentX competition to showcase the \"planning-centric\" nature of BeatDebate. The <code>ResponseFormatter</code> could potentially integrate this to display parts of the planning strategy alongside recommendations if desired.</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"writing/2025/05/31/beatdebate-technical-deep-dive/#conclusion","title":"Conclusion","text":"<p>BeatDebate set out to explore a novel approach to music recommendation: one where explicit, LLM-driven planning and transparent agentic workflows take center stage. By decoupling query understanding and strategic planning (PlannerAgent) from specialized execution (Advocate Agents) and reasoned evaluation (JudgeAgent), the system achieves a new level of explainability and control. While currently a proof-of-concept, the architectural patterns explored in BeatDebate\u2014planning-centric design, specialized agent collaboration, and explainable outputs\u2014offer a promising direction for the next generation of intelligent recommender systems and beyond.</p>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/","title":"Concept Visualizer: An AI-Powered Design Tool - Technical Deep Dive","text":""},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#introduction","title":"Introduction","text":"<p>Purpose</p> <p>This blog post documents the technical architecture and implementation of the Concept Visualizer, a web application designed to help users generate and refine visual concepts like logos and color palettes using AI. We'll explore the journey from an idea described in text to a set of visual assets, powered by a modern cloud-native stack.</p> <p>Check out the project GitHub repository for the full code and detailed documentation. Here is the web application. </p> <p>Problem Statement</p> <p>In the fast-paced world of branding and design, quickly translating abstract ideas into concrete visual representations is a significant challenge. Traditional design processes can be time-consuming and resource-intensive. The Concept Visualizer aims to streamline this by leveraging AI to generate initial designs and facilitate iterative refinement, making visual concept creation more accessible and efficient.</p>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#what-youll-learn","title":"What You\u2019ll Learn","text":"<ul> <li>Architecting Decoupled Full-Stack Applications: Insights into structuring applications with a FastAPI backend and a React frontend for maintainability and scalability.</li> <li>Integrating AI for Creative Generation: How the JigsawStack API is leveraged for AI-driven logo and color palette creation, including prompt engineering considerations.</li> <li>Implementing Asynchronous Task Processing: Utilizing GCP Cloud Functions (Gen 2) and Pub/Sub to handle long-running AI tasks, ensuring a non-blocking user experience.</li> <li>Mastering Infrastructure as Code (IaC): Managing a complex, reproducible cloud infrastructure on Google Cloud Platform using Terraform.</li> <li>Automating CI/CD Pipelines: Employing GitHub Actions for robust testing, Docker image building, and automated deployments to GCP and Vercel.</li> <li>Securing Cloud Resources: Best practices for credential management with GCP Secret Manager, IAM, and Workload Identity Federation.</li> <li>Designing Robust APIs: Implementing effective authentication, rate limiting (with Upstash Redis), and standardized error handling in FastAPI.</li> <li>Developing Modern Frontends: Using TanStack Query for server state, React Context API (with <code>use-context-selector</code>) for global UI state, and Tailwind CSS for styling.</li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#system-architecture-a-modular-and-scalable-design","title":"System Architecture: A Modular and Scalable Design","text":"<p>The Concept Visualizer is built upon a modular architecture, ensuring scalability, maintainability, and clear separation of concerns. The system primarily consists of a user-facing frontend, a backend API to handle business logic and AI integration, cloud infrastructure to host these components, and automated workflows for CI/CD.</p>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#build-path-diagram","title":"Build Path Diagram","text":""},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#runtime-path-diagram","title":"Runtime Path Diagram","text":"<p>The diagrams illustrates the flow:</p> <ol> <li>User Interaction: Users engage with the React Frontend Application, built with Vite and TypeScript, and hosted on Vercel. Vercel was chosen for its excellent developer experience, global CDN, and seamless Git integration for frontend deployments.</li> <li>API Communication: The frontend makes API calls, proxied by Vercel for <code>/api/*</code> paths, via HTTPS to the Backend API Server.</li> <li>Backend Processing:<ul> <li>The API Server, a FastAPI application running on a GCP Compute Engine Managed Instance Group (MIG), processes requests. FastAPI was selected for its high performance, asynchronous capabilities, and Pydantic-driven data validation.</li> <li>Supabase handles authentication (Supabase Auth) and data persistence (PostgreSQL DB). It was chosen for its integrated BaaS features, simplifying user management and database setup with Row Level Security (RLS).</li> <li>Upstash Redis serves as a high-performance cache for API rate limiting.</li> <li>The external JigsawStack API is the core AI engine for generating images and color palettes.</li> <li>Computationally intensive or long-running AI tasks are offloaded to a GCP Cloud Function (Gen 2) Worker via GCP Pub/Sub to ensure the API remains responsive.</li> </ul> </li> <li>Asynchronous Tasks:<ul> <li>The Worker, triggered by Pub/Sub messages, performs AI calls to JigsawStack, processes images, and stores results (images and palettes) in Supabase Storage.</li> <li>Task status is tracked and updated in the Supabase database.</li> </ul> </li> <li>Infrastructure &amp; DevOps:<ul> <li>The entire GCP Infrastructure is defined using Terraform, enabling version-controlled, reproducible environments.</li> <li>GitHub Actions automate the CI/CD pipeline: building Docker images (stored in GCP Artifact Registry), deploying infrastructure changes via Terraform, deploying the frontend to Vercel, and executing security and maintenance workflows.</li> <li>GCP Secret Manager securely stores all sensitive credentials.</li> <li>GCP Cloud Monitoring provides observability with logs, metrics, and alert policies.</li> <li>A GCS Bucket is dedicated to storing Terraform state (ensuring collaborative and safe IaC practices) and static assets like VM startup scripts.</li> </ul> </li> </ol>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#getting-started-local-development","title":"Getting Started &amp; Local Development","text":"<p>For those interested in running the Concept Visualizer locally or contributing to its development, here's a streamlined guide to get you up and running. For a more exhaustive setup guide, please refer to the Setup Documentation on GitHub.</p>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#1-the-backend-python-fastapi-and-asynchronous-power","title":"1. The Backend: Python, FastAPI, and Asynchronous Power","text":"<p>The backend is the application's core, handling business logic, AI integration, and data persistence. Built with Python 3.11 and FastAPI, it\u2019s designed for performance and scalability.</p>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#key-components-technologies","title":"Key Components &amp; Technologies:","text":""},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#fastapi","title":"FastAPI","text":"<ul> <li> <p>Chosen for its asynchronous nature (critical for I/O-bound operations like external API calls), high performance, and built-in data validation via Pydantic.</p> <pre><code># backend/app/api/routes/concept/generation.py (Illustrative Snippet)\nfrom fastapi import APIRouter, Depends\nfrom app.models.concept.request import PromptRequest\nfrom app.models.task.response import TaskResponse\nfrom app.api.dependencies import CommonDependencies\n\nrouter = APIRouter()\n\n@router.post(\"/generate-with-palettes\", response_model=TaskResponse)\nasync def generate_concept_with_palettes(\n    request: PromptRequest,\n    commons: CommonDependencies = Depends(),\n    # ... other params\n) -&gt; TaskResponse:\n    # ... logic to create and queue a task ...\n    user_id = commons.user_id\n    # ...\n    task = await commons.task_service.create_task(\n        user_id=user_id,\n        task_type=\"concept_generation\",\n        metadata=task_metadata\n    )\n    # ... publish to Pub/Sub ...\n    return TaskResponse(**task) # Simplified\n</code></pre> </li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#pydantic","title":"Pydantic","text":"<ul> <li>Used for defining request/response models and settings, providing robust data validation and serialization, reducing runtime errors, and improving code clarity.     <pre><code># backend/app/models/concept/request.py (Illustrative Snippet)\nfrom pydantic import BaseModel, Field\n\nclass PromptRequest(BaseModel):\n    logo_description: str = Field(\n        ...,\n        min_length=5,\n        max_length=500,\n        description=\"Description of the logo to generate\",\n    )\n    theme_description: str = Field(\n        ...,\n        min_length=5,\n        max_length=500,\n        description=\"Description of the theme/color scheme to generate\",\n    )\n</code></pre></li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#layered-architecture","title":"Layered Architecture","text":""},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#diagram","title":"Diagram","text":""},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#api-layer","title":"API Layer","text":"<ul> <li>(<code>backend/app/api/</code>):<ul> <li>Handles HTTP requests, validates inputs with Pydantic models, and formats responses.</li> <li>Routes are logically grouped by domain (e.g., <code>auth</code>, <code>concepts</code>, <code>tasks</code>). A central router in <code>backend/app/api/router.py</code> combines these.</li> <li>Middleware is crucial:<ul> <li><code>AuthMiddleware</code> (<code>backend/app/api/middleware/auth_middleware.py</code>): Integrates with Supabase Auth for JWT-based authentication.     <pre><code># backend/app/api/middleware/auth_middleware.py (Illustrative Snippet)\nclass AuthMiddleware(BaseHTTPMiddleware):\n    async def dispatch(self, request: Request, call_next: RequestResponseEndpoint) -&gt; Response:\n        if self._is_public_path(request.url.path):\n            return await call_next(request)\n        try:\n            user = self.supabase_auth.get_user_from_request(request)\n            request.state.user = user\n        except AuthenticationError as e:\n            return JSONResponse(status_code=401, content={\"detail\": e.message})\n        return await call_next(request)\n</code></pre></li> <li><code>RateLimitApplyMiddleware</code> &amp; <code>RateLimitHeadersMiddleware</code> (<code>backend/app/api/middleware/</code>): Manage API usage quotas using SlowAPI with an Upstash Redis backend. The middleware order (Auth -&gt; Rate Limiting) is vital so that authenticated users can be identified for personalized limits.     <pre><code># backend/app/api/middleware/rate_limit_apply.py (Illustrative Snippet)\n# RATE_LIMIT_RULES = { \"/concepts/generate\": \"10/month\", ... }\nclass RateLimitApplyMiddleware(BaseHTTPMiddleware):\n    async def dispatch(self, request: Request, call_next: Callable) -&gt; Response:\n        # ... skip if public or disabled ...\n        user_id = get_user_id(request)\n        limit_info = check_rate_limit(user_id, endpoint, rule)\n        if limit_info.get(\"exceeded\", False):\n            raise HTTPException(status_code=429, ...)\n        request.state.limiter_info = limit_info\n        return await call_next(request)\n</code></pre></li> </ul> </li> </ul> </li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#service-layer","title":"Service Layer","text":"<ul> <li>(<code>backend/app/services/</code>):<ul> <li>Encapsulates core business logic.</li> <li><code>ConceptService</code> (<code>backend/app/services/concept/service.py</code>): Orchestrates generation (calling <code>JigsawStackClient</code>), interacts with <code>ImageService</code>, and uses <code>PersistenceService</code>s.     <pre><code># backend/app/services/concept/service.py (Illustrative Snippet)\nclass ConceptService(ConceptServiceInterface):\n    async def generate_concept(self, logo_description: str, theme_description: str, ...) -&gt; Dict[str, Any]:\n        image_response = await self.client.generate_image(prompt=logo_description, ...)\n        image_url = image_response.get(\"url\")\n        # ... download, process, persist if user_id and not skip_persistence ...\n        return {\"image_url\": final_image_url, ...}\n</code></pre></li> <li><code>TaskService</code> (<code>backend/app/services/task/service.py</code>): Manages background task lifecycles.     <pre><code># backend/app/services/task/service.py (Illustrative Snippet)\nclass TaskService(TaskServiceInterface):\n    async def create_task(self, user_id: str, task_type: str, ...) -&gt; Dict[str, Any]:\n        task_data = {\"id\": str(uuid.uuid4()), \"user_id\": user_id, ...}\n        result = self.client.client.table(self.tasks_table).insert(task_data).execute()\n        # ... error handling ...\n        return result.data[0]\n</code></pre></li> <li>Services often implement interfaces (e.g., <code>ConceptServiceInterface</code>) for loose coupling and testability.</li> </ul> </li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#core-layer","title":"Core Layer","text":"<ul> <li>(<code>backend/app/core/</code>):<ul> <li><code>config.py</code>: Defines application settings using Pydantic's <code>BaseSettings</code>.     <pre><code># backend/app/core/config.py (Illustrative Snippet)\nclass Settings(BaseSettings):\n    JIGSAWSTACK_API_KEY: str = \"dummy_key\"\n    SUPABASE_URL: str = \"https://your-project-id.supabase.co\"\n    # ... other settings ...\n    model_config = SettingsConfigDict(env_prefix=\"CONCEPT_\")\nsettings = Settings()\n</code></pre></li> <li><code>factory.py</code>: Contains <code>create_app</code> to initialize the FastAPI app.     <pre><code># backend/app/core/factory.py (Illustrative Snippet)\ndef create_app() -&gt; FastAPI:\n    app = FastAPI(...)\n    # ... configure CORS, middleware, routes, rate limiter ...\n    app.add_middleware(AuthMiddleware, ...)\n    configure_api_routes(app)\n    setup_limiter_for_app(app)\n    return app\n</code></pre></li> <li>Custom exceptions (<code>exceptions.py</code>) provide domain-specific error handling.</li> </ul> </li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#asynchronous-task-processing","title":"Asynchronous Task Processing","text":""},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#diagram_1","title":"Diagram","text":"<ul> <li>(<code>backend/cloud_run/worker/</code>):<ul> <li>Rationale: AI image generation can be time-consuming. Offloading these tasks to an asynchronous worker prevents API timeouts and provides a better user experience.</li> </ul> </li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#gcp-cloud-function-gen-2","title":"GCP Cloud Function (Gen 2)","text":"<ul> <li>Chosen for its container support, longer execution timeout, and seamless Pub/Sub integration.<ul> <li>The worker entry point is <code>handle_pubsub</code> in <code>main.py</code>, triggered by Pub/Sub messages.     <pre><code># backend/cloud_run/worker/main.py (Illustrative Snippet)\n@functions_framework.cloud_event\ndef handle_pubsub(cloud_event: CloudEvent) -&gt; None:\n    async def _async_handle_pubsub() -&gt; None:\n        # ... decode message_payload from cloud_event ...\n        await process_pubsub_message(message_payload, SERVICES_GLOBAL)\n    asyncio.run(_async_handle_pubsub())\n</code></pre></li> <li><code>process_pubsub_message</code> instantiates task-specific processors (<code>GenerationTaskProcessor</code>, <code>RefinementTaskProcessor</code>).     <pre><code># backend/cloud_run/worker/main.py (Illustrative Snippet)\nasync def process_pubsub_message(message: Dict[str, Any], services: ServicesDict) -&gt; None:\n    task_type = message.get(\"task_type\")\n    if task_type == TASK_TYPE_GENERATION:\n        processor = GenerationTaskProcessor(...)\n    # ...\n    if processor:\n        await processor.process()\n</code></pre></li> <li>Processors like <code>GenerationTaskProcessor</code> (<code>backend/cloud_run/worker/processors/generation_processor.py</code>) manage the task lifecycle.     <pre><code># backend/cloud_run/worker/processors/generation_processor.py (Illustrative Snippet)\nclass GenerationTaskProcessor(BaseTaskProcessor):\n    async def process(self) -&gt; None:\n        if not await self._claim_task(): return\n        try:\n            concept_response = await self._generate_base_image()\n            image_data = await prepare_image_data_from_response(...)\n            # ... store image, generate palettes, create variations, store concept ...\n            await self._update_task_completed(concept_id)\n        except Exception as e:\n            await self._update_task_failed(str(e))\n</code></pre></li> </ul> </li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#pubsub-message-structure-example","title":"Pub/Sub Message Structure (Example)","text":"<ul> <li><code>json     {         \"task_id\": \"unique-task-uuid\",         \"user_id\": \"user-uuid\",         \"task_type\": \"concept_generation\",         \"logo_description\": \"A futuristic cat logo\",         \"theme_description\": \"Neon and dark mode\",         \"num_palettes\": 5     }</code></li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#database-storage-integration","title":"Database &amp; Storage Integration","text":"<ul> <li>(<code>backend/app/core/supabase/</code>):<ul> <li>Supabase: Selected as a BaaS for its integrated PostgreSQL, Auth, Storage, and Realtime features.</li> <li><code>SupabaseClient</code> and <code>SupabaseAuthClient</code> (<code>client.py</code>) provide typed interfaces.     <pre><code># backend/app/core/supabase/client.py (Illustrative Snippet)\nclass SupabaseClient:\n    def __init__(self, url: Optional[str] = None, key: Optional[str] = None, ...):\n        self.client = create_client(self.url, self.key)\n\n    def get_service_role_client(self) -&gt; Any: # Returns a new client instance\n        return create_client(self.url, settings.SUPABASE_SERVICE_ROLE)\n</code></pre></li> <li><code>ConceptStorage</code> and <code>ImageStorage</code> (<code>concept_storage.py</code>, <code>image_storage.py</code>) encapsulate data and file operations.     <pre><code># backend/app/core/supabase/concept_storage.py (Illustrative Snippet)\nclass ConceptStorage:\n    def store_concept(self, concept_data: Dict[str, Any]) -&gt; Optional[Dict[str, Any]]:\n        response = self.client.client.table(self.concepts_table).insert(insert_data).execute()\n        # ...\n        return response.data[0] if response.data else None\n</code></pre></li> <li>Security: RLS policies in Supabase (defined in <code>.sql</code> files in <code>backend/scripts/</code>) protect user data. The Cloud Function worker uses the <code>SERVICE_ROLE_KEY</code> to bypass RLS for system operations.</li> </ul> </li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#docker","title":"Docker","text":"<ul> <li>(<code>backend/Dockerfile</code>, <code>backend/Dockerfile.worker</code>):<ul> <li>The API server uses a multi-stage Dockerfile for optimized builds.     <pre><code># backend/Dockerfile (Illustrative Snippet)\nFROM python:3.11-slim AS base\n# ... install OS deps ...\nFROM base AS builder\nWORKDIR /app\nRUN pip install uv\nCOPY pyproject.toml README.md ./\nCOPY app /app/app\nRUN uv pip install --system --no-cache -e .[dev]\nFROM base\nWORKDIR /app\nCOPY --from=builder /usr/local /usr/local\n# ... copy source, CMD ...\n</code></pre></li> <li><code>Dockerfile.worker</code> includes specific dependencies like OpenCV native libraries for the Cloud Function.</li> </ul> </li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#2-the-frontend-react-vite-and-a-modern-ui","title":"2. The Frontend: React, Vite, and a Modern UI","text":"<p>The frontend is a single-page application (SPA) built with React 19, Vite, and TypeScript, focused on delivering a responsive and interactive user experience.</p>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#key-components-technologies_1","title":"Key Components &amp; Technologies:","text":""},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#react-19-vite","title":"React 19 &amp; Vite","text":"<ul> <li>Vite provides an extremely fast development server with Hot Module Replacement (HMR) and optimized production builds. React 19 (or latest stable at time of project) brings features for enhanced performance and developer experience.</li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#typescript","title":"Typescript","text":"<ul> <li>Ensures type safety across the codebase, reducing runtime errors and improving code maintainability.</li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#styling","title":"Styling","text":""},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#tailwind-css","title":"Tailwind CSS","text":"<ul> <li>Leveraged for its utility-first approach, enabling rapid and consistent UI development.     <pre><code>/* frontend/my-app/tailwind.config.js (Illustrative Snippet) */\nmodule.exports = {\n    content: [\"./src/**/*.{js,ts,jsx,tsx}\"],\n    theme: {\n    extend: {\n        colors: {\n        primary: \"#4F46E5\", // Defined from variables.css\n        \"primary-dark\": \"#4338CA\",\n        // ... other custom colors\n        },\n        boxShadow: {\n        modern: \"0 10px 30px -5px rgba(79, 70, 229, 0.2)\",\n        },\n    },\n    },\n    plugins: [require(\"@tailwindcss/forms\")],\n};\n</code></pre></li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#material-ui","title":"Material-UI","text":"<ul> <li>Utilized for some pre-built, accessible UI components (e.g., Modals, Buttons), customized via a central theme in <code>theme.tsx</code>.     <pre><code>// frontend/my-app/src/theme.tsx (Illustrative Snippet)\nimport { createTheme } from \"@mui/material/styles\";\n\nexport const theme = createTheme({\n    palette: {\n    primary: { main: \"#4F46E5\" },\n    // ... other palette settings\n    },\n    typography: { fontFamily: '\"Montserrat\", sans-serif' },\n    components: {\n    MuiButton: { styleOverrides: { root: { textTransform: \"none\" } } },\n    },\n});\n</code></pre></li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#css-modules","title":"CSS Modules","text":"<ul> <li>Applied for component-scoped styling where necessary (e.g., <code>frontend/my-app/src/components/concept/ConceptResult.module.css</code>).</li> <li>Custom animations are defined in <code>frontend/my-app/src/styles/animations.css</code>, respecting <code>prefers-reduced-motion</code>.</li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#state-management","title":"State Management","text":"<ul> <li>The frontend employs a combination of TanStack Query for server state and React Context API (optimized with <code>use-context-selector</code>) for global UI state. This provides a robust and performant approach to managing application data.</li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#diagram_2","title":"Diagram","text":""},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#tanstack-query-react-query","title":"TanStack Query (React Query)","text":"<ul> <li>The primary tool for managing server state. It handles data fetching, caching, background updates, and mutations for concepts, tasks, and API rate limits. Centralized query keys in <code>frontend/my-app/src/config/queryKeys.ts</code> ensure consistent cache management and invalidation.     <pre><code>// frontend/my-app/src/hooks/useConceptQueries.ts (Illustrative Snippet)\nimport { useQuery } from \"@tanstack/react-query\";\nimport { queryKeys } from \"../config/queryKeys\";\nimport { fetchRecentConceptsFromApi } from \"../services/conceptService\"; // Assumes this function exists\n\nexport function useRecentConcepts(userId?: string, limit: number = 10) {\n    return useQuery({\n    queryKey: queryKeys.concepts.recent(userId, limit),\n    queryFn: () =&gt; {\n        if (!userId) return Promise.resolve([]); // Or handle as needed\n        return fetchRecentConceptsFromApi(userId, limit);\n    },\n    enabled: !!userId, // Only fetch if userId is available\n    staleTime: 60 * 1000, // 1 minute\n    });\n}\n</code></pre></li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#react-context-api","title":"React Context API","text":"<ul> <li>Manages global UI-related state such as authentication (<code>AuthContext</code>), active background task tracking (<code>TaskContext</code>), and API rate limit display (<code>RateLimitContext</code>). <code>use-context-selector</code> optimizes performance by allowing components to subscribe to only specific parts of a context, preventing unnecessary re-renders.     <pre><code>// frontend/my-app/src/contexts/AuthContext.tsx (Illustrative Snippet)\nimport { createContext } from \"use-context-selector\";\n// ... other imports and AuthContextType definition ...\n\nexport const AuthContext = createContext&lt;AuthContextType | undefined&gt;(undefined);\n\nexport const AuthProvider: React.FC&lt;{ children: React.ReactNode }&gt; = ({ children }) =&gt; {\n    // ... state and logic for session, user, isLoading, error ...\n    // ... signOut, linkEmail functions ...\n    const value = useMemo(() =&gt; ({ session, user, ... }), [session, user, ...]);\n    return &lt;AuthContext.Provider value={value}&gt;{children}&lt;/AuthContext.Provider&gt;;\n};\n\n// frontend/my-app/src/hooks/useAuth.ts (Illustrative Snippet for selector hook)\nimport { useContextSelector } from \"use-context-selector\";\nimport { AuthContext } from \"../contexts/AuthContext\";\n\nexport const useUserId = () =&gt; {\n    return useContextSelector(AuthContext, (state) =&gt; state?.user?.id);\n};\n</code></pre></li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#routing","title":"Routing","text":"<ul> <li>React Router: Manages client-side navigation. Routes are defined in <code>App.tsx</code>.</li> <li>Framer Motion: Used for smooth page transitions and component animations, enhancing the user experience.</li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#api-client","title":"API Client","text":"<ul> <li>(<code>frontend/my-app/src/services/apiClient.ts</code>):<ul> <li>A centralized Axios instance handles all HTTP communication with the backend.</li> <li>Interceptors are key:<ul> <li>A request interceptor automatically fetches the current Supabase session and attaches the JWT <code>access_token</code> to the <code>Authorization</code> header.     <pre><code>// frontend/my-app/src/services/apiClient.ts (Request Interceptor Snippet)\naxiosInstance.interceptors.request.use(\n  async (config) =&gt; {\n    const { data: { session } } = await supabase.auth.getSession();\n    if (session?.access_token) {\n      config.headers = config.headers || {};\n      config.headers.Authorization = `Bearer ${session.access_token}`;\n    }\n    return config;\n  }, /* ... error handling ... */\n);\n</code></pre></li> <li>A response interceptor handles API errors, parses rate limit headers (updating <code>RateLimitContext</code>), and implements a token refresh mechanism for 401 Unauthorized errors. If token refresh fails, it dispatches a global event (<code>auth-error-needs-logout</code>) handled by <code>AuthContext</code>.     <pre><code>// frontend/my-app/src/services/apiClient.ts (Response Error Interceptor Snippet - Simplified)\naxiosInstance.interceptors.response.use(\n  (response) =&gt; { /* ... handle rate limits ... */ return response; },\n  async (error) =&gt; {\n    const originalRequest = error.config;\n    if (error.response?.status === 401 &amp;&amp; !originalRequest._retry) {\n      originalRequest._retry = true;\n      // ... attempt supabase.auth.refreshSession() ...\n      // if successful, retry originalRequest with new token\n      // if failed, dispatch 'auth-error-needs-logout'\n    }\n    // ... parse other errors into custom error classes ...\n    return Promise.reject(parsedError);\n  }\n);\n</code></pre></li> </ul> </li> <li>Custom error classes (<code>RateLimitError</code>, <code>AuthError</code>, <code>ValidationError</code>, etc.) are thrown by the client, providing structured error information.</li> </ul> </li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#component-architecture","title":"Component Architecture","text":"<ul> <li>Organized into <code>ui/</code> (primitive reusable components like <code>Button</code>, <code>Card</code>), <code>layout/</code> (Header, Footer, <code>MainLayout</code>), and <code>features/</code> (domain-specific components like <code>ConceptForm</code>, <code>ConceptDetailPage</code>).</li> <li><code>OptimizedImage</code> (<code>frontend/my-app/src/components/ui/OptimizedImage.tsx</code>) component handles lazy loading and placeholders for better perceived performance.</li> <li><code>ErrorBoundary</code> components (<code>frontend/my-app/src/components/ui/ErrorBoundary.tsx</code>) catch rendering errors.</li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#real-time-task-updates","title":"Real-time Task Updates","text":"<ul> <li>(<code>frontend/my-app/src/hooks/useTaskSubscription.ts</code>):<ul> <li>Leverages Supabase Realtime capabilities to subscribe to PostgreSQL changes on the <code>tasks</code> table.     <pre><code>// frontend/my-app/src/hooks/useTaskSubscription.ts (Illustrative Snippet)\nexport function useTaskSubscription(taskId: string | null) {\n    // ... state for taskData, error, status ...\n    useEffect(() =&gt; {\n        if (!taskId) return;\n        const channel = supabase\n            .channel(`task-updates-${taskId}`)\n            .on(\"postgres_changes\", { event: \"UPDATE\", schema: \"public\", table: tasksTable, filter: `id=eq.${taskId}` },\n                (payload) =&gt; {\n                    // Update local state and React Query cache with payload.new\n                }\n            )\n            .subscribe();\n        // ... handle subscription status, cleanup ...\n        return () =&gt; { supabase.removeChannel(channel); };\n    }, [taskId, tasksTable]);\n    // ... return { taskData, error, status } ...\n}\n</code></pre></li> <li>When a task's status is updated in the backend, the frontend receives a real-time notification, allowing for immediate UI updates in components like the <code>TaskStatusBar</code>.</li> </ul> </li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#testing","title":"Testing","text":"<ul> <li>Unit/Integration tests with Vitest and React Testing Library.</li> <li>E2E, visual regression, and accessibility tests with Playwright.</li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#example-asynchronous-concept-generation-ui-flow","title":"Example: Asynchronous Concept Generation UI Flow","text":"<ol> <li>User fills the <code>ConceptForm</code> (in <code>ConceptFormSection</code> on <code>LandingPage</code>).     <pre><code>// frontend/my-app/src/features/landing/components/ConceptFormSection.tsx (Illustrative Snippet)\n// Uses ConceptForm component\n&lt;ConceptForm\n    onSubmit={handleGenerateConcept} // handleGenerateConcept calls the mutation\n    status={formStatus}\n    // ...\n/&gt;\n</code></pre></li> <li>On submit, <code>useGenerateConceptMutation</code> (a React Query hook from <code>hooks/useConceptMutations.ts</code>) is invoked.     <pre><code>// frontend/my-app/src/hooks/useConceptMutations.ts (Illustrative Snippet)\nexport function useGenerateConceptMutation() {\n  const { setActiveTask, setIsTaskInitiating } = useTaskContext(); // Assuming TaskContext hook\n  const decrementLimit = useRateLimitsDecrement(); // Assuming RateLimit hook\n\n  return useMutation&lt;TaskResponse, Error, PromptRequest&gt;({\n    mutationFn: async (data) =&gt; {\n      setIsTaskInitiating(true);\n      decrementLimit(\"generate_concept\");\n      const response = await apiClient.post&lt;TaskResponse&gt;(API_ENDPOINTS.GENERATE_CONCEPT, data);\n      setActiveTask(response.data.task_id || response.data.id);\n      return response.data;\n    },\n    // ... onError, onSuccess, onSettled ...\n  });\n}\n</code></pre></li> <li>The hook:<ul> <li>Calls <code>apiClient.post</code> to the <code>/api/concepts/generate-with-palettes</code> endpoint.</li> <li>Optimistically decrements the <code>generate_concept</code> rate limit via <code>useRateLimitsDecrement</code>.</li> <li>Sets the <code>TaskContext</code> state to <code>isTaskInitiating: true</code>.</li> </ul> </li> <li>The backend API returns a <code>TaskResponse</code> with a <code>task_id</code> and <code>status: \"pending\"</code>.</li> <li>The <code>onSuccess</code> callback of the mutation sets this <code>task_id</code> as the <code>activeTaskId</code> in <code>TaskContext</code>. <code>isTaskInitiating</code> becomes <code>false</code>.</li> <li>The <code>TaskStatusBar</code> (subscribed to <code>TaskContext</code>) displays \"Request queued...\".     <pre><code>// frontend/my-app/src/components/TaskStatusBar.tsx (Illustrative Snippet)\nconst TaskStatusBar: React.FC = () =&gt; {\n  const { activeTaskData, isTaskInitiating, isTaskPending, isTaskProcessing } = useTaskContext();\n  // ... logic to determine message and style based on task state ...\n  if (isTaskInitiating) return &lt;div&gt;Preparing...&lt;/div&gt;;\n  if (isTaskPending) return &lt;div&gt;Queued...&lt;/div&gt;;\n  // ...\n};\n</code></pre></li> <li><code>useTaskSubscription</code> (used by <code>TaskContext</code> or directly) establishes a Supabase Realtime channel for this <code>task_id</code>.</li> <li>When the backend worker updates the task status (e.g., to \"processing\", then \"completed\"), the Realtime event triggers an update in <code>TaskContext</code>.</li> <li>The UI (e.g., <code>TaskStatusBar</code>, <code>ResultsSection</code>) re-renders to reflect the new status.</li> <li>If completed, <code>latestResultId</code> in <code>TaskContext</code> is updated. The <code>ResultsSection</code> (or <code>ConceptDetailPage</code>) uses <code>useConceptDetail</code> with this ID to fetch and display the final generated concept.</li> </ol>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#3-terraform-infrastructure-as-code-for-gcp","title":"3. Terraform: Infrastructure as Code for GCP","text":"<p>All Google Cloud Platform (GCP) infrastructure for the Concept Visualizer is provisioned and managed using Terraform. This Infrastructure as Code (IaC) approach guarantees reproducible, version-controlled, and easily adaptable environments. We selected Terraform for its declarative syntax, comprehensive GCP provider, and robust dependency management, crucial for orchestrating our multi-component cloud architecture.</p>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#key-infrastructure-resources-design-decisions","title":"Key Infrastructure Resources &amp; Design Decisions:","text":""},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#networking","title":"Networking","text":"<ul> <li>(<code>terraform/network.tf</code>):<ul> <li>Utilizes GCP's default VPC network for simplicity in this project. For larger-scale production deployments, a custom VPC would offer more granular control.</li> <li>Firewall Rules: Implements a least-privilege approach.     <pre><code># terraform/network.tf (Illustrative Snippet for API Ingress)\nresource \"google_compute_firewall\" \"allow_api_ingress\" {\n  name    = \"${var.naming_prefix}-allow-api-ingress\"\n  network = data.google_compute_network.default_network.id # Using default network\n  allow {\n    protocol = \"tcp\"\n    ports    = [\"80\", \"443\"] # For HTTP/HTTPS\n  }\n  source_ranges = [\"0.0.0.0/0\"] # Open to the internet; consider restricting or using a Load Balancer\n  target_tags   = [\"${var.naming_prefix}-api-vm-${var.environment}\"]\n}\n\n# Snippet for GCP Health Checks\ndata \"google_netblock_ip_ranges\" \"health_checkers\" {\n  range_type = \"health-checkers\"\n}\nresource \"google_compute_firewall\" \"allow_gcp_health_checks_to_api\" {\n  # ...\n  source_ranges = data.google_netblock_ip_ranges.health_checkers.cidr_blocks_ipv4\n  target_tags   = [\"${var.naming_prefix}-api-vm-${var.environment}\"]\n}\n</code></pre> <code>allow_api_ingress</code> opens HTTP/HTTPS ports for the API VMs. <code>allow_ssh_ingress</code> (not shown for brevity) is restricted to <code>var.allow_ssh_ips</code>. Crucially, <code>allow_gcp_health_checks_to_api</code> permits Google's health checkers to reach API VMs, essential for MIG health and uptime monitoring.</li> </ul> </li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#iam-service-accounts","title":"IAM &amp; Service Accounts","text":"<ul> <li>(<code>terraform/iam.tf</code>):<ul> <li>Dedicated Service Accounts: <code>api_service_account</code>, <code>worker_service_account</code>, and <code>cicd_service_account</code> are created with specific, minimal roles.     <pre><code># terraform/iam.tf (Illustrative Snippet for API Service Account)\nresource \"google_service_account\" \"api_service_account\" {\n  project      = var.project_id\n  account_id   = \"${var.naming_prefix}-api-${var.environment}\"\n  display_name = \"SA for Concept Viz API (${var.environment})\"\n}\n\nresource \"google_project_iam_member\" \"api_sa_secret_accessor\" {\n  project = var.project_id\n  role    = \"roles/secretmanager.secretAccessor\"\n  member  = \"serviceAccount:${google_service_account.api_service_account.email}\"\n}\n</code></pre></li> <li>Workload Identity Federation: This is a key security feature. <code>google_iam_workload_identity_pool</code> and <code>google_iam_workload_identity_pool_provider</code> allow GitHub Actions workflows to impersonate the <code>cicd_service_account</code> using short-lived OIDC tokens from GitHub, eliminating the need to store long-lived GCP service account keys as GitHub secrets.     <pre><code># terraform/iam.tf (Illustrative Snippet for Workload Identity)\nresource \"google_iam_workload_identity_pool\" \"github_pool\" {\n  project                   = var.project_id\n  workload_identity_pool_id = \"gh-pool-${var.environment}-${random_string.pool_suffix.result}\"\n  # ...\n}\n\nresource \"google_iam_workload_identity_pool_provider\" \"github_provider\" {\n  project                            = var.project_id\n  workload_identity_pool_id          = google_iam_workload_identity_pool.github_pool.workload_identity_pool_id\n  workload_identity_pool_provider_id = \"gh-provider-${var.environment}\"\n  oidc { issuer_uri = \"https://token.actions.githubusercontent.com\" }\n  attribute_mapping = { \"google.subject\" = \"assertion.sub\", ... }\n  attribute_condition = \"attribute.repository == \\\"${var.github_repo}\\\"\" # Scoped to specific repo\n}\n</code></pre></li> </ul> </li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#compute-engine","title":"Compute Engine","text":"<ul> <li>(<code>terraform/compute.tf</code>):<ul> <li>API Server: A <code>google_compute_instance_group_manager</code> (MIG) is used. Even with a <code>target_size = 1</code> (for cost-effectiveness in smaller environments), a MIG provides robust instance template management for updates and a clear path for future auto-scaling.</li> <li><code>google_compute_address.api_static_ip</code>: Ensures the API has a stable public IP.</li> <li><code>google_compute_instance_template.api_template</code>: Defines the VM configuration (Debian 11, machine type from <code>var.api_vm_machine_type</code>) and crucially references the startup script <code>terraform/scripts/startup-api.sh</code>. This script automates Docker installation, API image pulling from Artifact Registry, and container startup with environment variables fetched from instance metadata and GCP Secret Manager.     <pre><code># terraform/compute.tf (Illustrative Snippet for Instance Template)\nresource \"google_compute_instance_template\" \"api_template\" {\n  project      = var.project_id\n  name_prefix  = \"${var.naming_prefix}-api-tpl-${var.environment}-\"\n  machine_type = var.api_vm_machine_type\n  disk { source_image = \"debian-cloud/debian-11\" }\n  network_interface {\n    network = data.google_compute_network.default_network.id\n    access_config { nat_ip = google_compute_address.api_static_ip[0].address }\n  }\n  service_account {\n    email  = google_service_account.api_service_account.email\n    scopes = [\"cloud-platform\"]\n  }\n  metadata_startup_script = file(\"${path.module}/scripts/startup-api.sh\")\n  metadata = {\n    environment   = var.environment\n    naming_prefix = var.naming_prefix\n    # Docker image URL is passed via metadata in deploy_backend.yml during instance recreation\n  }\n}\n</code></pre></li> </ul> </li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#cloud-function","title":"Cloud Function","text":"<ul> <li>(<code>terraform/cloud_function.tf</code>, <code>terraform/cloud_function_source.tf</code>):<ul> <li>Worker Deployment: A <code>google_cloudfunctions2_function</code> resource defines the Gen 2 Cloud Function. Gen 2 was chosen for its native container support (essential for dependencies like OpenCV), longer execution times (up to 9 minutes, suitable for AI tasks), and more flexible eventing compared to Gen 1.</li> <li>Source Handling: Terraform initially deploys the function configuration using a placeholder ZIP from <code>google_storage_bucket_object.placeholder_source_zip</code>. The actual worker's container image (built by CI/CD) is deployed by the <code>deploy_backend.yml</code> GitHub Action workflow, which directly updates the Cloud Function with the new image from Artifact Registry (<code>gcloud functions deploy ... --docker-image ...</code> or similar). Terraform primarily manages the function's immutable configuration.     <pre><code># terraform/cloud_function.tf (Illustrative Snippet)\nresource \"google_cloudfunctions2_function\" \"worker_function\" {\n  name        = \"${var.naming_prefix}-worker-${var.environment}\"\n  location    = var.region\n  build_config {\n    runtime     = \"python311\" # Required even for container deployment\n    entry_point = \"handle_pubsub\"\n    source { # Placeholder for initial Terraform apply\n      storage_source {\n        bucket = google_storage_bucket.function_source_placeholder.name\n        object = google_storage_bucket_object.placeholder_source_zip.name\n      }\n    }\n  }\n  service_config {\n    max_instance_count  = var.worker_max_instances\n    service_account_email = google_service_account.worker_service_account.email\n    environment_variables = { GCP_PROJECT_ID = var.project_id, ... }\n    secret_environment_variables { # Example for one secret\n      key    = \"CONCEPT_SUPABASE_URL\"\n      secret = google_secret_manager_secret.supabase_url.secret_id\n      # ... other secrets ...\n    }\n  }\n  event_trigger {\n    trigger_region = var.region\n    event_type     = \"google.cloud.pubsub.topic.v1.messagePublished\"\n    pubsub_topic   = google_pubsub_topic.tasks_topic.id\n    retry_policy   = \"RETRY_POLICY_RETRY\"\n  }\n}\n</code></pre></li> <li>Trigger: Configured to be triggered by messages on the <code>tasks_topic</code> Pub/Sub topic.</li> </ul> </li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#pubsub","title":"Pub/Sub","text":"<ul> <li>(<code>terraform/pubsub.tf</code>):<ul> <li><code>google_pubsub_topic.tasks_topic</code>: The central message queue for decoupling the API from the worker and enabling asynchronous task processing.</li> <li><code>google_pubsub_subscription.worker_subscription</code>: Connects the worker Cloud Function to the <code>tasks_topic</code>. It's configured with <code>enable_exactly_once_delivery = true</code> for improved message processing reliability.     <pre><code># terraform/pubsub.tf (Illustrative Snippet)\nresource \"google_pubsub_topic\" \"tasks_topic\" {\n  name    = \"${var.naming_prefix}-tasks-${var.environment}\"\n}\n\nresource \"google_pubsub_subscription\" \"worker_subscription\" {\n  name  = \"${var.naming_prefix}-worker-sub-${var.environment}\"\n  topic = google_pubsub_topic.tasks_topic.name\n  ack_deadline_seconds = 60 # Example\n  retry_policy { minimum_backoff = \"10s\" }\n  enable_exactly_once_delivery = true\n}\n</code></pre></li> </ul> </li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#secret-manager","title":"Secret Manager","text":"<ul> <li>(<code>terraform/secrets.tf</code>):<ul> <li>Defines <code>google_secret_manager_secret</code> resources (as empty shells) for all sensitive credentials.</li> <li><code>google_secret_manager_secret_iam_member</code> grants specific service accounts (<code>api_service_account</code>, <code>worker_service_account</code>) the <code>roles/secretmanager.secretAccessor</code> role only for the secrets they require.     <pre><code># terraform/secrets.tf (Illustrative Snippet)\nresource \"google_secret_manager_secret\" \"supabase_url\" {\n  secret_id = \"${var.naming_prefix}-supabase-url-${var.environment}\"\n  # ... replication ...\n}\n\nresource \"google_secret_manager_secret_iam_member\" \"api_sa_can_access_supabase_url\" {\n  secret_id = google_secret_manager_secret.supabase_url.secret_id\n  role      = \"roles/secretmanager.secretAccessor\"\n  member    = \"serviceAccount:${google_service_account.api_service_account.email}\"\n}\n</code></pre></li> </ul> </li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#artifact-registry","title":"Artifact Registry","text":"<ul> <li>(<code>terraform/artifact_registry.tf</code>):<ul> <li><code>google_artifact_registry_repository.docker_repo</code>: A private Docker repository to securely store the API and worker container images built by CI/CD.     <pre><code># terraform/artifact_registry.tf (Illustrative Snippet)\nresource \"google_artifact_registry_repository\" \"docker_repo\" {\n  location      = var.region\n  repository_id = \"${var.naming_prefix}-docker-repo\"\n  format        = \"DOCKER\"\n}\n</code></pre></li> </ul> </li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#cloud-storage","title":"Cloud Storage","text":"<ul> <li>(<code>terraform/storage.tf</code>):<ul> <li><code>google_storage_bucket.assets_bucket</code>: Used to store VM startup scripts (<code>terraform/scripts/startup-api.sh</code>) and potentially other non-sensitive static assets.</li> <li>Terraform State Backend: The <code>main.tf</code> configures a GCS backend using a manually pre-created bucket (<code>var.manual_tf_state_bucket_name</code>). This is crucial for team collaboration, providing a centralized, versioned, and locked state.     <pre><code># terraform/main.tf (Backend Configuration Snippet)\nterraform {\n  backend \"gcs\" {\n    # Bucket name is passed via -backend-config during init\n    prefix = \"terraform/state\" # Workspace name (dev/prod) is appended automatically\n  }\n}\n</code></pre></li> </ul> </li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#monitoring","title":"Monitoring","text":"<ul> <li>(<code>terraform/monitoring.tf</code>):<ul> <li>Log-Based Metrics: <code>google_logging_metric.concept_task_failures</code> creates a custom metric to count specific error patterns from Cloud Function logs, enabling alerts on critical worker failures.     <pre><code># terraform/monitoring.tf (Illustrative Log-Based Metric Snippet)\nresource \"google_logging_metric\" \"concept_task_failures\" {\n  name   = \"${var.naming_prefix}-task-fails-${var.environment}\"\n  filter = \"resource.type=\\\"cloud_function\\\" AND resource.labels.function_name=\\\"${google_cloudfunctions2_function.worker_function.name}\\\" AND jsonPayload.severity=\\\"ERROR\\\" AND (jsonPayload.taskType=\\\"concept_generation\\\" OR jsonPayload.taskType=\\\"concept_refinement\\\")\"\n  metric_descriptor { metric_kind = \"DELTA\"; value_type  = \"INT64\"; }\n}\n</code></pre></li> <li>Alerting: <code>google_monitoring_alert_policy</code> resources define alerts (e.g., for task failures, API/Frontend unavailability) and send notifications via an email channel.</li> <li>Uptime Checks: <code>google_monitoring_uptime_check_config</code> resources are set up for the API's health endpoint and the Vercel-hosted frontend URL.</li> </ul> </li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#environment-management-deployment-scripts","title":"Environment Management &amp; Deployment Scripts:","text":"<ul> <li>Terraform workspaces (<code>dev</code> and <code>prod</code>) isolate state and allow for different configurations per environment using <code>.tfvars</code> files (e.g., <code>environments/dev.tfvars</code>).</li> <li>The <code>scripts/gcp_apply.sh</code> orchestrates a multi-step deployment:<ol> <li>IAM &amp; Secrets Shells First: Critical for dependency management.</li> <li>Secret Population (<code>gcp_populate_secrets.sh</code>): Reads from local <code>.env.{environment}</code> files (e.g., <code>backend/.env.develop</code>) and populates GCP Secret Manager. This keeps secrets out of Terraform state and source control.</li> <li>Full Infrastructure Apply: Deploys remaining resources.</li> <li>Docker Build &amp; Push: For the API image.</li> <li>VM Update/Restart: Ensures the API VM uses the new image.</li> <li>Startup Script Upload (<code>upload_startup_scripts.sh</code>): Updates the VM startup script in GCS.</li> <li>GitHub Secret Population (<code>gh_populate_secrets.sh</code>): Syncs Terraform outputs (like SA emails, Workload Identity Provider name) to GitHub Actions secrets.</li> <li>Vercel Config Update: Modifies <code>frontend/my-app/vercel.json</code> with the API VM's new external IP and commits/pushes it, ensuring the Vercel frontend correctly proxies API requests.</li> </ol> </li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#4-github-actions-workflows","title":"4. GitHub Actions Workflows","text":"<p>GitHub Actions automates the Concept Visualizer's entire development lifecycle\u2014from testing and builds to deployments and security scans. We chose GitHub Actions for its seamless repository integration, vast library of reusable actions, and its power in building sophisticated CI/CD pipelines directly within our version control, streamlining our development and release processes.</p>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#core-cicd-pipeline","title":"Core CI/CD Pipeline:","text":""},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#continuous-integration-tests","title":"Continuous Integration tests","text":"<ul> <li><code>ci-tests.yml</code> (CI):<ul> <li>Triggers: Runs on pushes and pull requests to <code>main</code> and <code>develop</code> branches, and can be manually dispatched (<code>workflow_dispatch</code>).</li> <li>Responsibilities:<ul> <li>Performs comprehensive backend testing (Pytest, Flake8, Mypy).     <pre><code># .github/workflows/ci-tests.yml (Backend Test Snippet)\n- name: Run Pytest\n  run: uv run pytest tests/ # Assuming uv is set up\n  env: # Mock environment variables for tests\n    CONCEPT_SUPABASE_URL: \"https://example.supabase.co\"\n    # ... other mock env vars ...\n</code></pre></li> <li>Conducts frontend testing (Vitest, ESLint, TypeScript checks).     <pre><code># .github/workflows/ci-tests.yml (Frontend Test Snippet)\n- name: Run NPM Tests\n  working-directory: ./frontend/my-app\n  run: npm test\n  env:\n    CI: true\n    VITE_API_URL: \"http://localhost:8000/api\" # Mock API URL\n    # ... other mock env vars ...\n</code></pre></li> <li>If all tests and linters pass for pushes to <code>main</code> or <code>develop</code>, it builds the API Docker image and pushes it to GCP Artifact Registry. GCP authentication for this step uses Workload Identity Federation.     <pre><code># .github/workflows/ci-tests.yml (Docker Build &amp; Push Snippet)\n- name: Authenticate to Google Cloud\n  uses: google-github-actions/auth@v2\n  with:\n    workload_identity_provider: ${{ env.WORKLOAD_IDENTITY_PROVIDER }} # From branch logic\n    service_account: ${{ env.CICD_SERVICE_ACCOUNT_EMAIL }}\n- name: Build and Push API Image\n  uses: docker/build-push-action@v5\n  with:\n    context: ./backend\n    file: ./backend/Dockerfile\n    push: true\n    tags: |\n      ${{ secrets.GCP_REGION }}-docker.pkg.dev/${{ env.GCP_PROJECT_ID }}/${{ env.ARTIFACT_REGISTRY_REPO_NAME }}/concept-api-${{ env.ENVIRONMENT }}:${{ github.sha }}\n      ${{ secrets.GCP_REGION }}-docker.pkg.dev/${{ env.GCP_PROJECT_ID }}/${{ env.ARTIFACT_REGISTRY_REPO_NAME }}/concept-api-${{ env.ENVIRONMENT }}:latest\n</code></pre></li> </ul> </li> </ul> </li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#deploy-backend","title":"Deploy Backend","text":"<ul> <li><code>deploy_backend.yml</code> (CD):<ul> <li>Triggers: Runs automatically upon the successful completion of the <code>ci-tests.yml</code> workflow on the <code>main</code> or <code>develop</code> branch (<code>workflow_run</code>).</li> <li>Responsibilities:<ol> <li>Environment Detection: Determines <code>dev</code> or <code>prod</code> GCP environment based on the Git branch of the triggering workflow.     <pre><code># .github/workflows/deploy_backend.yml (Environment Detection Snippet)\n- name: Set Environment Specifics\n  id: set_env\n  run: |\n    if [[ \"${{ github.event.workflow_run.head_branch || github.ref_name }}\" == \"develop\" ]]; then\n      echo \"ENVIRONMENT=dev\" &gt;&gt; $GITHUB_ENV\n      echo \"GCP_PROJECT_ID=${{ secrets.DEV_GCP_PROJECT_ID }}\" &gt;&gt; $GITHUB_ENV\n      # ... more dev secrets ...\n    elif [[ \"${{ github.event.workflow_run.head_branch || github.ref_name }}\" == \"main\" ]]; then\n      echo \"ENVIRONMENT=prod\" &gt;&gt; $GITHUB_ENV\n      echo \"GCP_PROJECT_ID=${{ secrets.PROD_GCP_PROJECT_ID }}\" &gt;&gt; $GITHUB_ENV\n      # ... more prod secrets ...\n    fi\n</code></pre></li> <li>GCP Authentication: Securely authenticates to GCP using Workload Identity Federation.</li> <li>Cloud Function Deployment: Deploys the worker Cloud Function (Gen 2). The <code>gcloud functions deploy</code> command is used, building from the source in <code>./backend</code> if no specific image is provided, or can be configured to use an image from Artifact Registry. This ensures the latest worker code is deployed.     <pre><code># .github/workflows/deploy_backend.yml (Cloud Function Deploy Snippet)\n- name: Deploy Cloud Function Worker\n  run: |\n    FUNCTION_NAME=\"${{ env.NAMING_PREFIX }}-worker-${{ env.ENVIRONMENT }}\"\n    # ... set SECRETS_ARG dynamically ...\n    gcloud functions deploy \"$FUNCTION_NAME\" \\\n      --gen2 \\\n      --region=\"${{ env.REGION }}\" \\\n      --runtime=python311 \\\n      --entry-point=handle_pubsub \\\n      --trigger-topic=\"$TASKS_TOPIC_NAME\" \\\n      --set-env-vars=\"...\" \\\n      --set-secrets=\"$SECRETS_ARG\" \\\n      --source=\"./backend\" # Or --docker-image if pre-built\n      # ... other flags ...\n</code></pre></li> <li>API VM Deployment/Update:<ul> <li>Creates a new instance template for the Compute Engine MIG, referencing the Docker image from Artifact Registry (pushed by <code>ci-tests.yml</code>, tagged with the commit SHA).</li> <li>Updates the MIG to use this new template, triggering a deployment strategy (e.g., rolling update) to deploy the new API version.</li> <li>Cleans up old, unused instance templates. <pre><code># .github/workflows/deploy_backend.yml (API VM Deploy Snippet - Simplified)\n- name: Deploy API to Compute Engine MIG\n  run: |\n    IMAGE_URL=\"${{ env.REGION }}-docker.pkg.dev/.../concept-api-${{ env.ENVIRONMENT }}:${{ github.sha }}\"\n    TEMPLATE_NAME=\"${{ env.NAMING_PREFIX }}-api-tpl-$(echo \"${{ github.sha }}\" | cut -c1-8)-$(date +%Y%m%d%H%M%S)\"\n    MIG_NAME=\"${{ env.NAMING_PREFIX }}-api-igm-${{ env.ENVIRONMENT }}\"\n\n    gcloud compute instance-templates create \"$TEMPLATE_NAME\" \\\n      --image-family=debian-11 --machine-type=e2-micro \\\n      --metadata=startup-script-url=gs://...,docker-image=\"$IMAGE_URL\" \\\n      # ... other template flags ...\n\n    gcloud compute instance-groups managed set-instance-template \"$MIG_NAME\" \\\n      --template=\"$TEMPLATE_NAME\" --zone=\"${{ env.GCP_ZONE }}\"\n\n    # ... logic for rolling update / recreate ...\n    # ... logic for cleaning up old templates ...\n</code></pre></li> </ul> </li> <li>Frontend Monitoring Update:<ul> <li>Dynamically fetches the latest Vercel deployment URL.</li> <li>Uses Terraform (selecting the <code>dev</code> or <code>prod</code> workspace) to update GCP Monitoring resources (<code>google_monitoring_uptime_check_config.frontend_availability</code>, <code>google_monitoring_alert_policy.frontend_availability_failure_alert</code>) to target this new URL. <pre><code># .github/workflows/deploy_backend.yml (Frontend Monitoring Update Snippet - Simplified)\n- name: Apply/Update Frontend Monitoring Resources\n  env:\n    VERCEL_URL: ${{ steps.get_vercel_url.outputs.url }}\n    # ... other TF vars ...\n  run: |\n    CLEAN_VERCEL_HOST=$(echo \"$VERCEL_URL\" | sed -E 's~^https?://~~' | sed -E 's~/.*$~~')\n    terraform -chdir=terraform init -reconfigure -backend-config=\"bucket=$TF_STATE_BUCKET\"\n    terraform -chdir=terraform workspace select \"${{ env.ENVIRONMENT }}\"\n    cat &gt; terraform/temp_frontend_monitoring.tfvars &lt;&lt;EOF\n    project_id = \"${{ env.GCP_PROJECT_ID }}\"\n    frontend_hostname = \"$CLEAN_VERCEL_HOST\"\n    # ... other necessary vars for these specific targets ...\n    EOF\n    terraform -chdir=terraform apply -auto-approve \\\n      -var-file=\"temp_frontend_monitoring.tfvars\" \\\n      -target=google_monitoring_uptime_check_config.frontend_availability \\\n      -target=google_monitoring_alert_policy.frontend_availability_failure_alert\n</code></pre></li> </ul> </li> </ol> </li> </ul> </li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#playwright","title":"Playwright","text":"<ul> <li><code>frontend/my-app/.github/workflows/playwright.yml</code> (Frontend E2E):<ul> <li>Dedicated workflow for frontend Playwright tests.</li> <li>Jobs for functional E2E, visual regression testing, and accessibility testing (using <code>axe-core</code>).     <pre><code># frontend/my-app/.github/workflows/playwright.yml (Visual Regression Test Snippet)\n- name: Run visual regression tests\n  run: npx playwright test -c visual-regression.config.ts # Uses a specific Playwright config\n- name: Upload visual test results\n  if: always()\n  uses: actions/upload-artifact@v3\n  with:\n    name: visual-regression-report\n    path: |\n      test-results/visual-regression/\n      tests/e2e/__snapshots__/ # Path to snapshots\n</code></pre></li> </ul> </li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#security-maintenance-workflows","title":"Security &amp; Maintenance Workflows:","text":""},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#environment-security","title":"Environment Security","text":"<ul> <li><code>env_check.yml</code> (Environment Security):<ul> <li>Secret Scanning: Uses GitLeaks to scan history for committed secrets.     <pre><code># .github/workflows/env_check.yml (GitLeaks Snippet)\n- name: Run GitLeaks\n  run: |\n    gitleaks detect --config=gitleaks.toml --report-format json --report-path gitleaks-report.json || true\n</code></pre></li> <li>Example File Validation: Ensures <code>.env.example</code> files contain only placeholders.</li> </ul> </li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#security-scan","title":"Security Scan","text":"<ul> <li><code>security_scan.yml</code> (Code &amp; Dependency Security):<ul> <li>Static Analysis: Runs GitHub CodeQL for Python and JavaScript/TypeScript.     <pre><code># .github/workflows/security_scan.yml (CodeQL Snippet)\n- name: Initialize CodeQL\n  uses: github/codeql-action/init@v3\n  with:\n    languages: ${{ matrix.language }} # Runs for 'javascript', 'python'\n- name: Perform CodeQL Analysis\n  uses: github/codeql-action/analyze@v3\n</code></pre></li> <li>Dependency Scanning: Uses <code>safety</code> for Python and <code>npm audit</code> for the frontend.</li> </ul> </li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#schedule-cleanup","title":"Schedule Cleanup","text":"<ul> <li><code>schedule-cleanup.yml</code> (Data Maintenance):<ul> <li>Scheduled Execution: Runs hourly via cron (<code>0 * * * *</code>).</li> <li>Triggers Supabase Edge Function: Makes an HTTP POST to the <code>cleanup-old-data</code> Supabase Edge Function URL.     <pre><code># .github/workflows/schedule-cleanup.yml (Curl Snippet)\n- name: Call Cleanup Edge Function\n  run: |\n    curl -L -X POST '${{ env.SUPABASE_URL }}' \\ # URL set based on branch\n      -H 'Authorization: Bearer ${{ env.SUPABASE_KEY }}' \\\n      -H 'Content-Type: application/json'\n</code></pre></li> </ul> </li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#key-workflow-features","title":"Key Workflow Features:","text":"<ul> <li>Environment-Specific Logic: Workflows dynamically select GCP project IDs, secrets, and configurations based on the Git branch (<code>develop</code> or <code>main</code>).</li> <li>Artifact Management: Docker images (in Artifact Registry) and test reports (GitHub artifacts) are versioned and stored.</li> <li>Permissions: Utilizes granular permissions for GitHub Actions tokens, especially for Workload Identity Federation with GCP.</li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#external-services-integration","title":"External Services Integration","text":"<p>The Concept Visualizer relies on several key external services, each playing a vital role:</p> <ul> <li>Supabase: Chosen as an integrated Backend-as-a-Service (BaaS) to accelerate development.<ul> <li>PostgreSQL Database: Stores application data like user information, concept metadata (prompts, descriptions, paths to stored images), color palette details, and background task statuses. Row Level Security (RLS) policies are defined in <code>pg-bucket-schema-policies.sql</code> scripts to ensure users can only access their own data.</li> <li>Authentication: Manages user sign-up, login, and session handling. The frontend <code>AuthContext</code> and backend <code>AuthMiddleware</code> integrate with Supabase Auth. Anonymous authentication is supported for guest users.</li> <li>Storage: Stores the actual generated image files (concepts and palette variations) in separate GCS-backed buckets (e.g., <code>concept-images-dev</code>, <code>palette-images-dev</code>). Access is controlled via RLS and signed URLs.</li> <li>Edge Functions: The <code>cleanup-old-data</code> function (TypeScript, Deno runtime) is deployed to Supabase to perform scheduled data hygiene tasks (e.g., deleting concepts older than 30 days). This is triggered by a GitHub Actions cron job.</li> </ul> </li> <li>JigsawStack API: This is the core AI engine for the application.<ul> <li>The backend's <code>JigsawStackClient</code> interacts with this third-party API to:<ul> <li>Generate initial logo/concept images from text prompts.</li> <li>Generate diverse color palettes based on theme descriptions.</li> <li>Refine existing images based on further user input.</li> </ul> </li> <li>Prompt engineering logic within the client enhances user inputs for better AI results.</li> </ul> </li> <li>Upstash Redis: Provides a managed, serverless Redis instance.<ul> <li>Primarily used by the backend's API rate limiting system (implemented with SlowAPI). It stores request counts for different users/IPs and endpoints, enabling efficient enforcement of usage quotas. Its serverless nature simplifies management and scaling.</li> </ul> </li> <li>Vercel: Hosts the static frontend application.<ul> <li>Chosen for its seamless Git integration, global CDN for fast content delivery, and easy-to-configure CI/CD for the frontend.</li> <li>The <code>vercel.json</code> file configures API rewrites/proxies, directing requests from the frontend (e.g., <code>/api/*</code>) to the backend API server running on GCP. This provides a unified domain for the application and simplifies CORS configuration. The <code>gcp_apply.sh</code> script automatically updates this file with the API VM's IP address after infrastructure changes.</li> </ul> </li> </ul>"},{"location":"writing/2025/05/19/concept-visualizer-technical-deep-dive/#conclusion","title":"Conclusion","text":"<p>The Concept Visualizer stands on three technical pillars: a modern React/Vite frontend, a high-performance FastAPI backend, and a scalable GCP infrastructure defined by Terraform. Its design prioritizes asynchronous AI processing, robust security (IAM, Secret Manager), and extensive CI/CD automation via GitHub Actions. This foundation ensures a powerful AI design tool that is both user-centric and developer-efficient, with clear avenues for growth.</p>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/","title":"RecSys Challenge 2024: Exploratory Data Analysis","text":""},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#introduction","title":"Introduction","text":"<p>Purpose</p> <p>This article will cover the exploratory data analysis of the RecSys 2024 Challenge dataset. The content will be structured into the following sections:</p> <ul> <li>Data Preprocessing</li> <li>Functions<ul> <li>Plot Functions</li> <li>Feature Functions</li> </ul> </li> <li>Feature Analysis<ul> <li>Overall Feature Analysis</li> <li>Article</li> <li>User</li> <li>Session</li> <li>Topic</li> <li>Devices</li> <li>Age</li> </ul> </li> </ul> <p>For more in-depth analysis, please check out the notebook!</p> <p>About</p> <p>This year's challenge focuses on online news recommendation, addressing both the technical and normative challenges inherent in designing effective and responsible recommender systems for news publishing. The challenge will delve into the unique aspects of news recommendation, including modeling user preferences based on implicit behavior, accounting for the influence of the news agenda on user interests, and managing the rapid decay of news items. Furthermore, our challenge embraces the normative complexities, involving investigating the effects of recommender systems on the news flow and whether they resonate with editorial values. [1]</p> <p>Challenge Task</p> <p>The Ekstra Bladet RecSys Challenge aims to predict which article a user will click on from a list of articles that were seen during a specific impression. Utilizing the user's click history, session details (like time and device used), and personal metadata (including gender and age), along with a list of candidate news articles listed in an impression log, the challenge's objective is to rank the candidate articles based on the user's personal preferences. This involves developing models that encapsulate both the users and the articles through their content and the users' interests. The models are to estimate the likelihood of a user clicking on each article by evaluating the compatibility between the article's content and the user's preferences. The articles are ranked based on these likelihood scores, and the precision of these rankings is measured against the actual selections made by users. [1]</p> <p>Dataset Information</p> <p>The Ekstra Bladet News Recommendation Dataset (EB-NeRD) was created to support advancements in news recommendation research. It was collected from user behavior logs at Ekstra Bladet. We collected behavior logs from active users during the 6 weeks from April 27 to June 8, 2023. This timeframe was selected to avoid major events, e.g., holidays or elections, that could trigger atypical behavior at Ekstra Bladet. The active users were defined as users who had at least 5 and at most 1,000 news click records in a three-week period from May 18 to June 8, 2023. To protect user privacy, every user was delinked from the production system when securely hashed into an anonymized ID using one-time salt mapping. Alongside, we provide Danish news articles published by Ekstra Bladet. Each article is enriched with textual context features such as title, abstract, body, categories, among others. Furthermore, we provide features that have been generated by proprietary models, including topics, named entity recognition (NER), and article embeddings. [2]</p> <p>For more information on the dataset.</p> <p>Key Metrics</p> <p>We need to establish specific metrics and analyze how different features impact those metrics. Our platform generates revenue through both subscriptions and advertisements. User engagement is crucial because the more time users spend reading new articles, the greater our advertisement revenue. We will need this insight for our next section on model selection for a recommendation system.</p>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#data-preprocessing","title":"Data Preprocessing","text":"<p>Let's start by importing the packages required for this section.</p> <p><pre><code># Packages\nfrom datetime import datetime\nfrom plotly.subplots import make_subplots\nimport numpy as np\nimport pandas as pd\nimport plotly\nimport plotly.express as px\nimport plotly.graph_objects as go\n</code></pre> Next, let's load in the three separate data sources of the dataset:</p> <p>Data Sources</p> <p>Articles: Detailed information of news articles</p> <p>Behaviors: Impression Logs. </p> <p>History: Click histories of users. </p> <pre><code># Load in various dataframes\n# Articles\ndf_art = pd.read_parquet(\"Data/Small/articles.parquet\")\n\n# Behaviors\ndf_bev = pd.read_parquet(\"Data/Small/train/behaviors.parquet\")\n\n# History\ndf_his = pd.read_parquet(\"Data/Small/train/history.parquet\")\n</code></pre> <p>How can we merge these data sources?</p> <p>Info</p> <p>We've noticed that there are columns shared among each of these data sources that we can use for joining.</p> <ul> <li>Articles &lt;&gt; Article ID &lt;&gt; Behavior</li> <li>History &lt;&gt; User ID &lt;&gt; Behavior</li> </ul> <p>Before merging them together, we'll need to adjust the <code>behaviors['articles_id_clicked']</code> feature.</p> <p><pre><code># Convert datatype of column first\ndf_bev['article_id'] = df_bev['article_id'].apply(lambda x: x if type(s) == str else x.astype(np.int32) )\n\n# Join bevhaiors to article\ndf = df_bev.join(df_art.set_index(\"article_id\"), on=\"article_id\")\n\n# Join bevhaiors to history\ndf = df.join(df_his.set_index(\"user_id\"), on=\"user_id\")\n\n# Drop all other dataframes from me\ndf_bev = []\ndf_his = []\ndf_art = []\n</code></pre> Finally, we'll preprocess additional columns by converting the <code>device_type</code> from integer to string, <code>gender</code> from float to string, <code>postcodes</code> from float to string, <code>article_id</code> from string to integer, and <code>age</code> from an integer to a string.</p> <pre><code>def device_(x):\n    \"\"\" \n    Changes the device input from a int to a str\n    Keyword arguments:\n        x -- int\n    Output:\n        str\n    \"\"\"\n    if x == 1:\n        return 'Desktop'\n    elif x == 2:\n        return 'Mobile'\n    else:\n        return 'Tablet'\n\ndef gender_(x):\n    \"\"\" \n    Changes the gender input from a float to a str\n    Keyword arguments:\n        x -- float\n    Output:\n        str\n    \"\"\"\n    if x == 0.0:\n        return 'Male'\n    elif x == 1.0:\n        return 'Female'\n    else:\n        return None\n\n\ndef postcodes_(x):\n    \"\"\" \n    Changes the postcodes input from a float to a str\n    Keyword arguments:\n        x -- float\n    Output:\n        str\n    \"\"\"\n    if x == 0.0:\n        return 'Metropolitan'\n    elif x == 1.0:\n        return 'Rural District'\n\n    elif x == 2.0:\n        return 'Municipality'\n\n    elif x == 3.0:\n        return 'Provincial'\n\n    elif x == 4.0:\n        return 'Big City'\n\n    else:\n        return None\n\n# Preprocessing\ndf.dropna(subset=['article_id'], inplace=True)\n\n# Change article IDs into int\ndf['article_id'] = df['article_id'].apply(lambda x: int(x))\ndf['article_id'] = df['article_id'].astype(np.int64)\n\n# Change age from int to string\ndf['device_type'] = df['device_type'].apply(lambda x: device_(x))\n\n# Change genders from float to string\ndf['gender'] = df['gender'].apply(lambda x: gender_(x))\n\n# Change age to str it's a range\ndf['age'] = df['age'].astype('Int64')\ndf['age'] = df['age'].astype(str)\ndf['age'] = df['age'].apply(\n    lambda x: x if x == '&lt;NA&gt;' else x + ' - ' + x[0] + '9')\n\n\n# Change postcodes from int to str\ndf['postcode'] = df['postcode'].apply(lambda x: postcodes_(x))\n</code></pre>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#functions","title":"Functions","text":"<p>This section is divided into two types of functions used for EDA:</p> <ol> <li>Visualization </li> <li>Feature preprocessing </li> </ol>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#plot-functions","title":"Plot Functions","text":"<p>We'll implement functions to generate the following visualizations:</p> <ul> <li>Single and Multiple Categorical Bar Plots</li> <li>Single and Multiple Categorical Histograms, Box Plots, and Bar plots</li> <li>Scatter plots to measure activity across a time period</li> </ul> <p>Below is an example of a plot function implemented. This function will generate a histogram, a box plot, and a bar plot visualization for two features: This function is useful when comparing a categorical feature (such as Age) with a numerical feature (such as Read Time).</p> <pre><code>def multiple_subset_feature_visualization(\n    df_,\n    feature_1, feature_2,\n    feature_2_title, feature_1_title,\n    histogram_xaxis_title\n    ) -&gt; \"Graph\":\n    \"\"\" \n    Displays multiple plots: Histogram, Box, and Bar plots based on multiple features given.\n    Keyword arguments:\n        df_ -- list\n        feature_1 -- str\n        feature_2 -- str\n        feature_1_title -- str\n        feature_2_title -- str\n        histogram_xaxis_title -- str\n    Output: \n        Plotly graph object!\n    \"\"\"\n\n    # Make subplots object\n    fig = make_subplots(\n        rows=3, cols=1, subplot_titles=(\"&lt;b&gt;Histogram&lt;b&gt;\", \"&lt;b&gt;Box plot&lt;b&gt;\", \"&lt;b&gt;Average {} for {}&lt;b&gt;\".format(feature_2_title, feature_1_title))\n    )\n\n    # Assign tmp_df based on feature\n    if feature_1 == 'age':\n        tmp_df = df_[df_['age'] != '&lt;NA&gt;']\n    else:\n        tmp_df = df_[~df_[feature_1].isnull()]\n\n    # Create a category list from the feature given \n    categories = [d for d in tmp_df[feature_1].unique()]\n    categories.sort()\n\n    # Iterate through each category and produce a histogram, boxplot, and bar plots for that subset of the data\n    for category_ in categories:\n        subset_feature_2 = tmp_df[tmp_df[feature_1]== category_][feature_2].values\n        avg = round(float(tmp_df[tmp_df[feature_1] == category_][feature_2].mean()), 3)\n        # Add histogram\n        fig.add_trace(\n            go.Histogram(\n                x=subset_feature_2,\n                name=str(category_) + ' Histogram',\n            ),\n            row=1, col=1\n        )\n        # Add Boxplot\n        # Need to create an array that is similar to the array used in subset_feature_2, to name the traces!\n        xo = [str(category_) for x in range(0, len(subset_feature_2))]\n        fig.add_trace(\n            go.Box(\n                y=subset_feature_2, x=xo,\n                name=str(category_) + ' Box',\n            ),\n            row=2, col=1\n        )\n\n        # Add Bar\n        fig.add_trace(\n            go.Bar(\n                x=[str(category_)], y=[avg],\n                text='&lt;b&gt;{}&lt;b&gt;'.format(avg),\n                textposition='outside',\n                name=str(category_) + ' Bar',\n                textfont=dict(\n                    family='sans serif',\n                    size=18,\n                    color='#1f77b4'\n                )\n            ),\n            row=3, col=1\n        )\n\n    # Update xaxis properties\n    fig.update_xaxes(\n        title_text='&lt;b&gt;{}&lt;b&gt;'.format(str(histogram_xaxis_title)), row=1, col=1\n    )\n    fig.update_xaxes(\n        title_text='&lt;b&gt;{}&lt;b&gt;'.format(str(feature_1_title)), row=2, col=1\n    )\n    fig.update_xaxes(\n        title_text='&lt;b&gt;{}&lt;b&gt;'.format(str(feature_1_title)), row=3, col=1\n    )\n\n    # Update yaxis properties\n    fig.update_yaxes(\n        title_text='&lt;b&gt;Count&lt;b&gt;', row=1, col=1, type = 'log'\n    )\n    fig.update_yaxes(\n        title_text='&lt;b&gt;{}&lt;b&gt;'.format(str(feature_2_title)), row=2, col=1, type ='log'\n    )\n    fig.update_yaxes(\n        title_text='&lt;b&gt;{}&lt;b&gt;'.format(str(feature_2_title)),\n        range=[0, 125], row=3, col=1\n    )\n\n    # Update subplot title sizes\n    fig.update_annotations(\n        font_size=20,\n    )\n\n    # Update title and height\n    fig.update_layout(\n        title_text=\"&lt;b&gt;Distributions of {} for {}&lt;b&gt;\".format(\n            feature_2_title, feature_1_title),\n        height=750, width=1000,\n        font=dict(\n            family=\"Courier New, monospace\",\n            size=16,\n        )\n    )\n\n    return fig\n</code></pre>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#feature-functions","title":"Feature Functions","text":"<p>These are helper functions designed to preprocess features, preparing them to be used in the previous visaulization functions. </p> <p>They are split into separate sections for specific features:</p> <ul> <li>Article</li> <li>User</li> <li>Topic </li> </ul> <p>The following code snippet demonstrates a function that will populate a dictionary.</p> <pre><code>def populate_dict(list_, dict_):\n    \"\"\" \n    Populates the dict from list indices\n    Keyword arguments:\n        list_--  list\n        dict_ -- dict: \n    Output: \n        None\n    \"\"\"\n    # Iterate through each list index and append the index as a key \n    for idx in list_:\n        if idx not in dict_:\n            dict_[idx] = 1\n        else:\n            dict_[idx] += 1\n</code></pre>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#feature-analysis","title":"Feature Analysis","text":"<p>We aim to gain insights into which features we can utilize for our recommendation system. This analysis will be brief. For more information, check out the notebook which contains more in-depth feature analysis.</p>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#main-questions","title":"Main Questions:","text":"<p>These are questions we seek to address by the end of this analysis.</p> <p>What features provide details about an article?</p> <ul> <li>Topic</li> <li>Read Time</li> <li>Scroll Percentage</li> </ul> <p>What is the behavior of the following features?</p> <ul> <li>Article</li> <li>User</li> <li>Session</li> <li>Devices</li> <li>Ages</li> <li>Postcodes</li> </ul> <p>Explain the activity of our users (activity across a time period)? Segment it based on our categorical features such as ages, devices, gender, and postcodes.</p> <ul> <li>Daily</li> <li>Hourly</li> <li>Weekly</li> <li>Day of the week</li> </ul> <p>Describe the topic distribution across our categorical features such as ages, devices, and postcodes?</p>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#overall-feature-analysis","title":"Overall Feature Analysis","text":"<p>How many impressions are there in total?</p> <p>Solution</p> <p>There are 70421 impressions in this data. <pre><code># Number of Impressions\nsingle_subset_bar(df_=df, feature_='impression_id',\n                xaxis_title='Number of Impressions', yrange=[0, 80000])\n</code></pre></p> <p></p> <p>What does the distribution of read times look like?</p> <p>Solution</p> <p>Long-tailed distribution.  <pre><code># Distribution of Read Times\nsingle_subset_feature_visualization(\n    df_=df, feature_='read_time', data_title='All Users',\n    feature_title='Read Time(s)', histogram_xaxis_title='Read Time(s)')\n</code></pre> </p> <p>How is the distribution of scroll percentages represented?</p> <p>Solution</p> <p>Long-tailed distribution. <pre><code># Distribution of Scroll Percentages\nsingle_subset_feature_visualization(\n    df_=df, feature_='scroll_percentage', data_title='All Users',\n    feature_title='Scroll Percentage(%)', histogram_xaxis_title='Scroll Percentage(%)')\n</code></pre> </p>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#article","title":"Article","text":"<p>What is the total number of articles?</p> <p>Solution</p> <p>There are 1723 articles. <pre><code># Total Number of Articles\nsingle_subset_bar(df_ = df, feature_ = 'article_id', xaxis_title = 'Number of Articles', yrange = [0, 2000])\n</code></pre> </p> <p>How many unique articles are clicked in a single session?</p> <p>Solution</p> <p>There is a distribution starting from 1 to 19 articles clicked in a single session.</p> <p><pre><code># How many unique articles are clicked in a session?\n\n# Group by sessions and get the article ids\ntmp_aps = df.groupby('session_id')['article_id'].apply(list)\n\n# Create a dict to store the count of articles per session\narticles_per_session = {k: 0 for k in range(1, 20)}\n\n# Iterate through our list previously, and record the number of articles in a session to our res dict\nfor i in tmp_aps:\n    num_articles = len(i)\n    articles_per_session[num_articles] += 1\n\n# Set as our indices / values for plot\nindices = [k for k in articles_per_session.keys()]\nvalues = [k for k in articles_per_session.values()]\n\n# Plot\nplot_bar(\n    indices_=indices, values_=values,\n    yrange_=[0, 5], xaxis_title='Number of Articles ',\n    yaxis_title='Count', title_='&lt;b&gt; Number of Articles clicked in a session&lt;b&gt;')\n</code></pre> </p> <p>What is the average read time and scroll percentage for each article?</p> <p>Solution</p> <pre><code># Get the average readtime and scroll percentages for all articles!\n\n# Unique User Ids\nunique_user_ids = df['user_id'].values[0:1000]\n# We take the set because the scroll, article per user is joined in a list for every user id (so just take the set of it!)\nunique_user_ids = set(unique_user_ids)\n# Unique Article Ids\nunique_article_ids = df['article_id'].unique()\nunique_article_ids = unique_article_ids[~np.isnan(unique_article_ids)]\n# Create dictionaries\nunique_article_read = {k: [0] for k in unique_article_ids}\nunique_article_read_avg = {k: [0] for k in unique_article_ids}\nunique_article_scroll = {k: [0] for k in unique_article_ids}\nunique_article_scroll_avg = {k: [0] for k in unique_article_ids}\n\n# Iterate across each user id\nfor id in unique_user_ids:\n    # Get the subset of that user id\n    tmp_df = df[df['user_id'] == id]\n    # Now lets go through each scroll and article\n    indices = np.array(tmp_df.index)\n    for i in indices:\n        tmp_dict = {}\n        # Select the scroll / article of that indice and\n        tmp_read = tmp_df['read_time_fixed'][i]\n        tmp_article = tmp_df['article_id_fixed'][i]\n        tmp_scroll = tmp_df['scroll_percentage_fixed'][i]\n        # Create list objects for article, read, scroll\n        read = [x for x in tmp_read]\n        scroll = [x for x in tmp_scroll]\n        articles = [np.int64(x) for x in tmp_article]\n        # Populate our unique_article_read dictionary based on the results found in our previous list objects\n        tmp_articles_read = {k: v for k, v in zip(articles, read)}\n        article_id_read_scroll(tmp_articles_read, unique_article_read)\n        # Populate our unique_article_scroll dictionary based on the results found in our previous list objects\n        tmp_articles_scroll = {k: v for k, v in zip(articles, scroll)}\n        article_id_read_scroll(tmp_articles_scroll, unique_article_scroll)\n\n# Get the average scroll percentage and read times for each article\nfor k, v in zip(unique_article_read.keys(), unique_article_read.values()):\n    unique_article_read_avg[k] = np.mean(v)\nfor k, v in zip(unique_article_scroll.keys(), unique_article_scroll.values()):\n    unique_article_scroll_avg[k] = np.mean(v)\n</code></pre>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#average-read-time","title":"Average Read Time","text":"<p>The average reading time spans from 0 to 43.29 seconds, with outliers extending beyond this range. <pre><code># Distribution of Read Times for each Article\n## Indices / Values\nindices = ['&lt;b&gt;All Unique Articles&lt;b&gt;']\nvalues = [x for x in unique_article_read_avg.values()]\n## Plot\nplot_box(\n    indices_=indices, values_=[values],\n    yrange_=[0, 3], xaxis_title='',\n    yaxis_title='Read Time(s)', title_='&lt;b&gt; Distributions of Read Times Across All Articles&lt;b&gt;')\n</code></pre> </p>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#average-scroll-percentage","title":"Average Scroll Percentage","text":"<p>The average scroll percentage ranges from 0 - 100%. <pre><code># Distribution of Scroll Percentages for each Article\n## Indices / Values\nindices = ['&lt;b&gt;All Unique Articles&lt;b&gt;']\nvalues = [x for x in unique_article_scroll_avg.values()]\n## Plot\nplot_box(\n    indices_=indices, values_=[values],\n    yrange_=[0, 2], xaxis_title='',\n    yaxis_title='Scroll Percentage (%)', title_='&lt;b&gt; Distributions of Scroll Percentage Across All Articles&lt;b&gt;')\n</code></pre> </p>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#user","title":"User","text":"<p>What is the total number of users?</p> <p>Solution</p> <p>There are 9194 users.  <pre><code># Total Number of Users\nsingle_subset_bar(df_ = df, feature_ = 'user_id', xaxis_title = 'Number of Users', yrange = [0, 11000])\n</code></pre> </p> <p>Describe the behavior of daily user growth?</p> <p>Solution</p> <p>There are significant peaks observed in the first three dates, followed by a tapering off in the subsequent dates. <pre><code># Record the daily user growth\nunique_user_ids = df['user_id'].unique()\n\n# Create dictionaries\nunique_users_daily_growth_freq= {}\nunique_users_hourly_freq = {}\nunique_users_dayofweek_freq = {}\nunique_users_weekly_freq = {}\n\n# Iterate through each user id and record the number of unique users present!\nfor id in unique_user_ids[0:1000]:\n    # Get the subset of that user id\n    tmp_df = df[df['user_id'] == id]\n    # Get the first index of that impression time\n    first_index = tmp_df['impression_time_fixed'].index[0]\n    # Record that join_date \n    tmp_datetime = pd.DatetimeIndex(tmp_df['impression_time_fixed'][first_index])\n    tmp_date = tmp_datetime[0].date()\n    join_date = tmp_date\n    # Populate our unique_user_daily_growth\n    if join_date not in unique_users_daily_growth_freq:\n        unique_users_daily_growth_freq[join_date] = 1\n    else:\n        unique_users_daily_growth_freq[join_date] +=1\n\n# Sort our dict\nunique_users_daily_growth_freq = dict(sorted(unique_users_daily_growth_freq.items()))\n\n\n# Indices / Values for Plot\nindices = [x for x in unique_users_daily_growth_freq.keys()]\nvalues = [x for x in unique_users_daily_growth_freq.values()]\n# Plot\nplot_bar(indices_=indices, values_=values, yrange_=[\n        0, 3], xaxis_title='&lt;b&gt;Dates&lt;b&gt;', yaxis_title='&lt;b&gt;Count&lt;b&gt;', title_='&lt;b&gt;Daily User Growth&lt;b&gt;')\n</code></pre></p> <p></p> <p>What is the average read time and scroll percentage across each unique user?</p> <p>Solution</p> <p>Describe the behavior of user activity?</p> <p>Solution</p> <pre><code># Record the daily, hourly, weekly, dayofweek activity across all users\n\n# Get all unique ids in a list\nunique_user_ids = df['user_id'].unique()[0:1000]\n\n# Create dictionaries\nunique_users_daily_freq = {}\nunique_users_hourly_freq = {}\nunique_users_dayofweek_freq = {}\nunique_users_weekly_freq = {}\n\n# Iterate through each user id\nfor id in unique_user_ids:\n    # Get the subset of that user id\n    tmp_df = df[df['user_id'] == id]\n\n    # Now lets go through each and populate the unique dates, hours and day of the week for each user\n    dates = []\n    hours = []\n    dayofweek = []\n    week = []\n    indices = np.array(tmp_df.index)\n\n    # Iterate through each index\n    for i in indices:\n        # Store the date, time, dayofweek, and week number\n        tmp_datetime = pd.DatetimeIndex(tmp_df['impression_time_fixed'][i])\n        tmp_date = tmp_datetime.date\n        tmp_time = tmp_datetime.time\n        tmp_dayofweek = tmp_datetime.weekday\n        tmp_week = tmp_datetime.isocalendar().week\n        # Append our dates, hours, dayofweek, week number\n        for j, k, l, m in zip(tmp_date, tmp_time, tmp_dayofweek, tmp_week):\n            dates.append(j)\n            hours.append(k)\n            dayofweek.append(l)\n            week.append(m)\n\n    # Get rid of duplicate values\n    unique_dates = list(set(dates))\n    unique_hours = list(set(hours))\n    unique_dayofweek = list(set(dayofweek))\n    unique_week = list(set(week))\n\n    # Convert to string\n    unique_hours = [x.hour for x in unique_hours]\n    unique_hours = [str(i) + ':00' if i &gt; 9 else str(0) +\n                    str(i) + ':00' for i in unique_hours]\n\n    # Convert the week int to mapping from 1++\n    unique_week = weekly_map(unique_week)\n\n    # Populate dicts\n    populate_dict(list_=unique_dates, dict_=unique_users_daily_freq)\n    populate_dict(list_=unique_hours, dict_=unique_users_hourly_freq)\n    populate_dict(list_=unique_dayofweek, dict_=unique_users_dayofweek_freq)\n    populate_dict(list_=unique_week, dict_=unique_users_weekly_freq)\n\n\n# Sort our dicts\nunique_users_daily_freq = dict(sorted(unique_users_daily_freq.items()))\nunique_users_hourly_freq = dict(sorted(unique_users_hourly_freq.items()))\n\n# Sort by integers for day of the week and then lets change the dict from int to str\nunique_users_dayofweek_freq = dict(sorted(unique_users_dayofweek_freq.items()))\nunique_users_dayofweek_freq = int_dow_dict(unique_users_dayofweek_freq)\n\nunique_users_weekly_freq = dict(sorted(unique_users_weekly_freq.items()))\n</code></pre>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#read-time","title":"Read Time","text":"<p><pre><code># Read Time per User\n\n# Group by User and Read Time\ntmp_user_df = pd.DataFrame(data=df.groupby(by='user_id')[\n                        'read_time'].mean(), columns=['read_time'])\n# Plot\nsingle_subset_feature_visualization(\n    df_=tmp_user_df,  feature_='read_time',\n    data_title='Unique Users', feature_title ='Read Time(s)',\n    histogram_xaxis_title = 'Read Time(s)')\n</code></pre> </p>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#scroll-percentage","title":"Scroll Percentage","text":"<pre><code># Scroll Percentage per User\n\n# Group by User and Scroll Percentage\ntmp_user_df = pd.DataFrame(data=df.groupby(by='user_id')[\n                        'scroll_percentage'].mean(), columns=['scroll_percentage'])\n# Plot\nsingle_subset_feature_visualization(\n    df_=tmp_user_df,  feature_='scroll_percentage',\n    data_title='Unique Users', feature_title ='Scroll Percentage(%)',\n    histogram_xaxis_title = 'Scroll Percentage(%)')\n</code></pre>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#daily-user-activity","title":"Daily User Activity","text":"<p>We notice a fluctuation between 675 and 850 users, with a substantial drop-off toward the end. <pre><code># Daily User Activity\n\n## Indices / Values for Plot\nindices = [x for x in unique_users_daily_freq.keys()]\nvalues = [x for x in unique_users_daily_freq.values()]\n\n## Plot\nplot_scatter(\n    indices_=indices, values_=values,\n    yrange_=[200, 900], xaxis_title='Date',\n    yaxis_title='Active Users', title_='&lt;b&gt;Daily Active Users&lt;b&gt;'\n)\n</code></pre> </p>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#hourly-user-activity","title":"Hourly User Activity","text":"<p>We notice a significant surge in users at 04:00 (4am), which remains relatively consistent until 21:00 (9:00 pm). Following that, there is a notable decline until 04:00.</p> <p><pre><code># Hourly User Activity\n\n## Indices / Values for Plot\nindices = [x for x in unique_users_hourly_freq.keys()]\nvalues = [x for x in unique_users_hourly_freq.values()]\n\n## Plot\nplot_scatter(\n    indices_ = indices , values_ = values,\n    yrange_ = [0, 20000], xaxis_title = 'Hour',\n    yaxis_title= 'Active Users', title_ = '&lt;b&gt;Hourly Active Users&lt;b&gt;'\n    )\n</code></pre> </p>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#weekly-user-activity","title":"Weekly User Activity","text":"<p>There is a consistent upward trend in the number of users from week 1 to week 4. </p> <pre><code># Weekly User Activity\n\n## Indices / Values for Plot\nindices = [x for x in unique_users_weekly_freq.keys()]\nvalues = [x for x in unique_users_weekly_freq.values()]\n\n## Plot\nplot_bar(\n    indices_ = indices, values_ = values,\n    yrange_ = [0, 3.5], xaxis_title = 'Week',\n    yaxis_title= 'Active Users', title_ = '&lt;b&gt; Weekly Active Users &lt;b&gt;')\n</code></pre> <p></p>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#day-of-the-week-user-activity","title":"Day of the Week User Activity","text":"<p>User activity remains consistent throughout all days of the week.</p> <pre><code># Day Of The Week Activity\n\n## Indices / Values for Plot\nindices = [x for x in unique_users_dayofweek_freq.keys()]\nvalues = [x for x in unique_users_dayofweek_freq.values()]\n\n## Plot\nplot_bar(\n    indices_ = indices, values_ = values,\n    yrange_ = [0, 3.5], xaxis_title = 'Day',\n    yaxis_title= 'Active Users', title_ = '&lt;b&gt; Day of the Week Activity  &lt;b&gt;')\n</code></pre> <p></p>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#session","title":"Session","text":"<p>What are the total number of sessions?</p> <p>Solution</p> <p>There are 36795 unique sessions. <pre><code># Toal Number of Sessions\nsingle_subset_bar(df_=df, feature_='session_id',\n                xaxis_title='Number of Sessions', yrange=[0, 40000])\n</code></pre> </p> <p>What are the number of unique sessions per day?</p> <p>Solution</p> <p>The session count remains relatively stable within the range of 4000 to 6000 sessions until the final date, where a notable decline is observed.</p> <pre><code># Number of unique sessions per day\n\n# Make a copy of the dataframe and extract the time as a str\ncopy_df = df.copy()\ncopy_df['impression_time'] = copy_df['impression_time'].apply(\n    lambda x: x.date())\n\n# Group by the session ids with the impression time\nunique_sessions_per_day = copy_df.groupby(\n    by='session_id')['impression_time'].min()\ntmp_dau_df = pd.DataFrame(data=unique_sessions_per_day.values,\n                        index=unique_sessions_per_day.keys(), columns=['Session Dates'])\n\n# Plot\nmultiple_subset_bar(\n    df_=tmp_dau_df, feature_='Session Dates',\n    yrange=[0, 4.5], xaxis_title = 'Session Dates')\n</code></pre> <p></p> <p>What is average read time and scroll percentage for each unique session?</p> <p>Solution</p>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#read-time_1","title":"Read Time","text":"<p><pre><code># Read Time per Session\n## Group by session ids and read_time \ntmp_session_df = pd.DataFrame(data=df.groupby(by='session_id')[\n                            'read_time'].mean(), columns=['read_time'])\n## Plot\nsingle_subset_feature_visualization(\n    df_=tmp_session_df,  feature_='read_time',\n    data_title='Unique Sessions', feature_title = 'Read Time(s)',\n    histogram_xaxis_title ='Read Time(s)')\n</code></pre> </p>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#scroll-percentage_1","title":"Scroll Percentage","text":"<pre><code># Scroll Percentage per Session\n## Group by session ids and scroll percentage\ntmp_session_df = pd.DataFrame(data=df.groupby(by='session_id')[\n                            'scroll_percentage'].mean(), columns=['scroll_percentage'])\n## Plot\nsingle_subset_feature_visualization(\n    df_=tmp_session_df,  feature_='scroll_percentage',\n    data_title='Unique Sessions', feature_title = 'Scroll Percentage(%)',\n    histogram_xaxis_title ='Scroll Percentage(%)')\n</code></pre>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#topic","title":"Topic","text":"<p>How many topics are there in total?</p> <p>Solution</p> <p>There are 78 unique topics.</p> <p><pre><code># Number of Topics!\n# Unique Topics\ntopic_list = unique_subset_topics(df)\n# Plot\ntmp_topic_df = pd.DataFrame(data=topic_list, columns=['topics'])\n\nsingle_subset_bar(df_=tmp_topic_df, feature_='topics',\n                xaxis_title='Number of Topics', yrange=[0, 100])\n</code></pre> </p> <p>What are the top 10 most popular topics?</p> <p>Solution</p> <p>Kendt &gt; Sport &gt; Begivenhed &gt; Underholdning &gt; Sportsbegivenhed &gt; Kriminalitet &gt; Livsstill &gt; Politik &gt; Fodbold &gt; Erhverv <pre><code># Record the frequency of topics across unique users, readtimes across topics, and scroll percentages across those topics\n\n# Get all unique ids in a list\nunique_user_ids = df['user_id'].values[0:1000]\n\n# Create dictionaries\nunique_users_topics_freq = {}\nunique_topic_scroll_freq = {}\nunique_topic_read_freq = {}\n\n# Iterate through each user id and record the topics viewed!\nfor id in unique_user_ids:\n    # Get the subset of that user id\n    tmp_df = df[df['user_id'] == id]\n    # Now lets go through each topic\n    indices = np.array(tmp_df.index)\n    for i in indices:\n        # Record the topic, scroll percentage and read_time for each index\n        tmp_topics = tmp_df['topics'][i]\n        tmp_scroll = tmp_df['scroll_percentage'][i]\n        tmp_read = tmp_df['read_time'][i]\n        topics = [x for x in tmp_topics]\n        scroll = [tmp_scroll]\n        read = [tmp_read]\n\n    # Find the average scroll percentages across each topic  (Can be related to whether a topic doesnt require too much reading has visualizations)\n    # Look at article_id for whichever topics the article is included in add that scroll percentage\n        tmp_topic_scroll = {k: v for k, v in zip(topics, scroll)}\n        unique_topic_scroll_freq = topics_article_id_scroll_read(\n            tmp_topic_scroll, unique_topic_scroll_freq)\n\n    # Find the average read time across each topic\n    # Look at article_id for whichever topics the article is included in add that readtime\n        tmp_topic_read = {k: v for k, v in zip(topics, read)}\n        unique_topic_read_freq = topics_article_id_scroll_read(\n            tmp_topic_read, unique_topic_read_freq)\n\n    # Unique User Topics\n    # Get rid of duplicate values\n    unique_topics = list(set(topics))\n\n    # Populate our dict\n    populate_dict(unique_topics, unique_users_topics_freq)\n\n\n# Sort the dictionaries\nsorted_topic_freq = dict(\n    sorted(unique_users_topics_freq.items(), key=lambda x: x[1], reverse=True))\n\n# Find the average read times across each topic\nunique_topic_read_avg_freq = {k: round(np.nanmean(v), 2) for k, v in zip(\n    unique_topic_read_freq.keys(), unique_topic_read_freq.values())}\nsorted_unique_topic_read_avg_freq = dict(\n    sorted(unique_topic_read_avg_freq.items(), key=lambda x: x[1], reverse=True))\n\n# Sort the topics for distribution\nsorted_unique_topic_read_freq = dict(sorted(unique_topic_read_freq.items()))\n\n# Find the average scroll percentages across each topic\nunique_topic_scroll_avg_freq = {k: round(np.nanmean(v), 2) for k, v in zip(\n    unique_topic_scroll_freq.keys(), unique_topic_scroll_freq.values())}\nsorted_unique_topic_scroll_avg_freq = dict(\n    sorted(unique_topic_scroll_avg_freq.items(), key=lambda x: x[1], reverse=True))\n\n# Sort the topics scroll pct for distribution\nsorted_unique_topic_scroll_freq = dict(\n    sorted(unique_topic_scroll_freq.items()))\n\n# Distribution of Topics across users!\n## Indices / Values for Plot\nindices = [x for x in sorted_topic_freq.keys()][0:10]\nvalues = [x for x in sorted_topic_freq.values()][0:10]\n\n## Plot\nplot_bar(\n    indices_=indices, values_=values,\n    yrange_=[0, 3], xaxis_title='Topics',\n    yaxis_title='Count', title_='&lt;b&gt; Top 10 Highest Topic Activity&lt;b&gt;')\n</code></pre></p> <p></p> <p>What is the distribution of read time and scroll percentage across each topic?</p> <p>Solution</p> <p>How do daily and hourly activity patterns relate to each topic?</p> <p>Solution</p> <pre><code># Daily and Hourly Activity across each Topic\n\n# Get all the unique topics\ntopic_list = unique_subset_topics(df)\nunique_topics = sorted(topic_list)\n\n# Get the list of each unqiue topic in a specific session\ntopics = df.groupby(by='session_id')['topics'].apply(list)\n\n# Get the list of each unique timestamp for these sessions\ntimestamps = df.groupby(by='session_id')['impression_time'].apply(list)\nunique_dates = []\n\n# Create a list of hours in a str format\nunique_hours = [i for i in range(24)]\nunique_hours = [str(i) + ':00' if i &gt; 9 else str(0) +\n                str(i) + ':00' for i in unique_hours]\n\n# Iterate through each timestamp\nfor i in range(len(timestamps.values)):\n    # Iterate through each idx\n    for j in range(len(timestamps.values[i])):\n        # Assign datetime and date objects\n        tmp_datetime = timestamps.values[i][j]\n        tmp_date = tmp_datetime.date()\n        # if date not in unique dates, append\n        if tmp_date not in unique_dates:\n            unique_dates.append(tmp_date)\n\n# Sort dates\nunique_dates = sorted(unique_dates)\n\n# Instantiate dict objects with unique dates and unique key values set to 0\nunique_topic_daily_activity = {\n    k: {k: 0 for k in unique_dates} for k in unique_topics}\nunique_topic_hourly_activity = {\n    k: {k: 0 for k in unique_hours} for k in unique_topics}\n\n\n# Iterate through each session id\nfor i in zip(range(len(topics.values))):\n    # Iterate through each index of nested list\n    for j, k in zip(range(0, len(topics.values[i][0])), range(0, len(i))):\n        tmp = topics.values[i][0][j]\n        # Assign a datetime and time object\n        tmp_datetime = timestamps.values[i][k]\n        tmp_date = tmp_datetime.date()\n        tmp_time = tmp_datetime.time()\n        tmp_hour = tmp_time.hour\n\n        # Convert hour into a string\n        if tmp_hour &gt; 9:\n            tmp_time = str(tmp_hour) + ':00'\n\n        else:\n            tmp_time = \"0\" + str(tmp_hour) + ':00'\n\n        # Add to dictionary\n        unique_topic_daily_activity[tmp][tmp_date] += 1\n        unique_topic_hourly_activity[tmp][tmp_time] += 1\n</code></pre>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#read-time_2","title":"Read Time","text":"<p><pre><code># Box Plot of Read Time across Topics\n## Indices / Values for Plot\nindices = [x for x in sorted_unique_topic_read_freq.keys()]\nvalues = [x for x in sorted_unique_topic_read_freq.values()]\n## Plot\nplot_box(\n    indices_ = indices, values_ = values,\n    yrange_ = [0, 3.5], xaxis_title = 'Topics',\n    yaxis_title= 'Read Time(s)', title_ = '&lt;b&gt; Distributions of Read Times across each Topic&lt;b&gt;')\n</code></pre> </p>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#scroll-percentage_2","title":"Scroll Percentage","text":"<p><pre><code># Box Plot of Scroll Percentage across Topics\n## Indices / Values for Plot\nindices = [x for x in sorted_unique_topic_scroll_freq.keys()]\nvalues = [x for x in sorted_unique_topic_scroll_freq.values()]\n## Plot\nplot_box(\n    indices_ = indices, values_ = values,\n    yrange_ = [0, 2.1], xaxis_title = 'Topics',\n    yaxis_title= 'Read Time(s)', title_ = '&lt;b&gt; Distributions of Read Times across each Topic&lt;b&gt;')\n</code></pre> </p>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#daily-activity","title":"Daily Activity","text":"<p>Daily activity vary significantly across topics, with some topics experiencing higher levels of activity due to their popularity. <pre><code># Daily Activity of Topics \nactivity_scatter(\n    dict_=unique_topic_daily_activity,  yrange_=[0, 2100],\n    xaxis_title='Dates', yaxis_title='Active Users', title_='&lt;b&gt; Daily Active Users per Topic')\n</code></pre> </p>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#hourly-activity","title":"Hourly Activity","text":"<p><pre><code># Hourly Activity of Topics \nactivity_scatter(\n    dict_=unique_topic_hourly_activity,  yrange_=[0, 1000],\n    xaxis_title='Hourly', yaxis_title='Active Users', title_='&lt;b&gt; Hourly Active Users per Topic')\n</code></pre> </p>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#devices","title":"Devices","text":"<p>What is the distribution of devices?</p> <p>Solution</p> <p>There are 23536 desktop users, 44472 mobile users, and 2413 tablet users.  <pre><code># Distribution of Devices\nmultiple_subset_bar(df_=df, feature_='device_type', yrange=[0, 5], xaxis_title = 'Devices')\n</code></pre> </p> <p>What is the distribution of read time and scroll percentages for devices?</p> <p>Solution</p> <p>What is the topic distribution for devices?</p> <p>Solution</p> <p>The topic distribution is relatively consistent across all devices: Kendt &gt; Sport &gt; Begivenhed, Underholdning &gt; Kriminalitet  <pre><code># Distribution of Topics Per Device\n# Unique Topics\n# Distribution of Topics Per Device\n# Unique Topics\ntopic_list = unique_subset_topics(df)\nunique_topics = sorted(topic_list)\n# Plot\ntopic_feature_bar_distribution(\n    df_=df, feature_='device_type', yrange=[0, 4.5],\n    topic_list_=unique_topics, subplot_titles_=[\n        '&lt;b&gt;Desktop&lt;b&gt;', '&lt;b&gt;Mobile&lt;b&gt;', '&lt;b&gt;Tablet&lt;b&gt;'],\n    xaxis_title='&lt;b&gt;Topics&lt;b&gt;', yaxis_title='&lt;b&gt;Count&lt;b&gt;',\n    title_='&lt;b&gt;Topic Distribution Per Device&lt;b&gt;',\n    height_=750, width_=1000\n)\n</code></pre></p> <p></p> <p>What is the daily and hourly activity for devices?</p> <p>Solution</p> <p>The majority of active users are on mobile devices, outnumbering both desktop and tablet users combined. However, all activity plots exhibit similar patterns with varying magnitudes. <pre><code># Daily and Hourly Activity across Devices\ndaily_hourly_activity_feature_bar_distribution(\n    df_ = df, feature_ = 'device_type', yrange = [0, 4],\n    subplot_titles_ = ['&lt;b&gt;Daily&lt;b&gt;', '&lt;b&gt;Monthly&lt;b&gt;'],\n    title_ = '&lt;b&gt;Daily and Hourly Activity per Device&lt;b&gt;',\n    height_ = 750, width_ = 1000\n    )\n</code></pre> </p>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#read-time_3","title":"Read Time","text":"<pre><code># Read Time across Devices\nmultiple_subset_feature_visualization(\n    df_=df,  feature_1='device_type',\n    feature_2='read_time', feature_1_title='Devices',\n    feature_2_title='Read Time(s)', histogram_xaxis_title='Read Time(s)'\n)\n</code></pre>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#scroll-percentage_3","title":"Scroll Percentage","text":"<pre><code># Scroll Percentage across Devices\nmultiple_subset_feature_visualization(\n    df_=df,  feature_1='device_type',\n    feature_2='scroll_percentage', feature_1_title='Devices',\n    feature_2_title='Scroll Percentages(%)', histogram_xaxis_title='Scroll Percentages(%)'\n)\n</code></pre>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#age","title":"Age","text":"<p>What are the distribution of ages?</p> <p>Solution</p> <p>There's significant variation in the frequency of ages, with a majority of users falling within the 50-79 age range. <pre><code># Distribution of Ages\nmultiple_subset_bar(\n    df_=df, feature_='age', yrange=[0, 3.5],\n    xaxis_title ='Age'\n    )\n</code></pre> </p> <p>What is the distribution of read time and scroll percentages for ages?</p> <p>Solution</p> <p>What is the topic distribution for age?</p> <p>Solution</p> <p>Five topics consistently appear across all age ranges, but their frequency varies within each age group. <pre><code># Distribution of Topics across Ages\n## Get all the unique topics\ntopic_list = unique_subset_topics(df)\nunique_topics = sorted(topic_list)\n## Plot\ntopic_feature_bar_distribution(\n    df_=df, feature_='age', yrange=[0, 2.5],\n    topic_list_=unique_topics,\n    subplot_titles_=[\n        '&lt;b&gt;20-29&lt;b&gt;', '&lt;b&gt;30-39&lt;b&gt;', '&lt;b&gt;40-49&lt;b&gt;',\n        '&lt;b&gt;50-59&lt;b&gt;', '&lt;b&gt;60-69&lt;b&gt;', '&lt;b&gt;70-79&lt;b&gt;',\n        '&lt;b&gt;80-89&lt;b&gt;', '&lt;b&gt;90-99&lt;b&gt;'\n    ],\n    xaxis_title='&lt;b&gt;Topics&lt;b&gt;', yaxis_title='&lt;b&gt;Count&lt;b&gt;',\n    title_='&lt;b&gt;Topic Distribution of Age Groups&lt;b&gt;',\n    height_=1000, width_=1000\n)\n</code></pre> </p> <p>What is the daily and hourly activity for age?</p> <p>Solution</p> <p>The daily and hourly activity trajectories are similar across each age group, except for the 90-99 age group, which appears scattered due to missing data on certain dates. <pre><code># Daily Activity Users / Hourly Activity Users across Age\ndaily_hourly_activity_feature_bar_distribution(\n    df_=df, feature_='age', yrange=[0, 2.5],\n    subplot_titles_=['&lt;b&gt;Daily&lt;b&gt;', '&lt;b&gt;Monthly&lt;b&gt;'],\n    title_='&lt;b&gt;Daily and Hourly Activity of Age Groups&lt;b&gt;',\n    height_=750, width_=1000\n)\n</code></pre> </p>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#read-time_4","title":"Read Time","text":"<p><pre><code># Read Time across Ages\nmultiple_subset_feature_visualization(\n    df_=df,  feature_1='age',\n    feature_2='read_time', feature_1_title='Age',\n    feature_2_title='Read Time(s)', histogram_xaxis_title='Read Time(s)'\n)\n</code></pre> </p>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#scroll-percentage_4","title":"Scroll Percentage","text":"<p><pre><code># Scroll Percentages across Ages\nmultiple_subset_feature_visualization(\n    df_=df,  feature_1='age',\n    feature_2='scroll_percentage', feature_1_title='Age',\n    feature_2_title='Scroll Percent(%)', histogram_xaxis_title='Scroll Percentage(%)'\n)\n</code></pre> </p>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#postcodes","title":"Postcodes","text":"<p>What is the distribution of postcodes?</p> <p>Solution</p> <p>There are 268 Big City users, 572 Metropolitan users, 190 Municipality users, 254 Provincial users, and 352 Rural District users. <pre><code># Distribution of Postcodes. \nmultiple_subset_bar(\n    df_=df, feature_='postcode',\n    yrange=[0, 3], xaxis_title ='Postcode')\n</code></pre> </p> <p>What is the distribution read time and scroll percentage of postcodes?</p> <p>Solution</p> <p>What is the topic distribution for postcodes?</p> <p>Solution</p> <p>Five topics consistently appear across all postcodes, but their frequency varies within each postcode. <pre><code># Distribution of Topics across Postcodes\n## Get all the unique topics\ntopic_list = unique_subset_topics(df)\nunique_topics = sorted(topic_list)\n## Plot\ntopic_feature_bar_distribution(\n    df_=df, feature_='postcode', yrange=[0, 2.5],\n    topic_list_=unique_topics,\n    subplot_titles_=[\n        '&lt;b&gt;Big City&lt;b&gt;', '&lt;b&gt;Metropolitan&lt;b&gt;', '&lt;b&gt;Municiplaity&lt;b&gt;',\n        '&lt;b&gt;Provincial&lt;b&gt;', '&lt;b&gt;Rural District&lt;b&gt;'\n    ],\n    xaxis_title='&lt;b&gt;Topics&lt;b&gt;', yaxis_title='&lt;b&gt;Count&lt;b&gt;',\n    title_='&lt;b&gt;Topic Distribution per Postcodes&lt;b&gt;',\n    height_=850, width_=1000\n)\n</code></pre> </p> <p>What is the daily and hourly activity for postcodes?</p> <p>Solution</p> <p>The daily and hourly activity trajectories are similar across each postcode. <pre><code># Daily Activity Users / Hourly Activity Users across Postcodes\ndaily_hourly_activity_feature_bar_distribution(\n    df_=df, feature_='postcode', yrange=[0, 4],\n    subplot_titles_=['&lt;b&gt;Daily&lt;b&gt;', '&lt;b&gt;Monthly&lt;b&gt;'],\n    title_='&lt;b&gt;Daily and Hourly Activity per Postcode&lt;b&gt;',\n    height_=750, width_=1000\n)\n</code></pre> </p> <p>For more analysis, check out my notebook containing the code!</p> <p>Stay tuned for my next post which will go over the model selection for Recommendation Systems!</p>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#read-time_5","title":"Read Time","text":"<p><pre><code># Read Time across Postcodes\nmultiple_subset_feature_visualization(\n    df_=df,  feature_1='postcode',\n    feature_2='read_time', feature_1_title='Postcodes',\n    feature_2_title='Read Time(s)', histogram_xaxis_title='Read Time(s)'\n)\n</code></pre> </p>"},{"location":"writing/2024/04/25/recsys-challenge-2024-eda/#scroll-percentage_5","title":"Scroll percentage","text":"<p><pre><code># Scroll Percentages across Postcodes\nmultiple_subset_feature_visualization(\n    df_=df,  feature_1='postcode',\n    feature_2='scroll_percentage', feature_1_title='Postcode',\n    feature_2_title='Scroll Percent(%)', histogram_xaxis_title='Scroll Percentage(%)'\n)\n</code></pre> </p>"},{"location":"writing/2024/12/15/recsys-challenge-2024-model-selection/","title":"RecSys Challenge 2024: Model Selection","text":""},{"location":"writing/2024/12/15/recsys-challenge-2024-model-selection/#introduction","title":"Introduction","text":"<p>Purpose</p> <p>This article will cover model selection in the RecSys Challenge 2024. The content will be structured into the following sections:</p> <ul> <li>News RecSys</li> <li>Model Selection</li> </ul> <p>For more in-depth analysis, please check out the notebook!</p> <p>About</p> <p>This year's challenge focuses on online news recommendation, addressing both the technical and normative challenges inherent in designing effective and responsible recommender systems for news publishing. The challenge will delve into the unique aspects of news recommendation, including modeling user preferences based on implicit behavior, accounting for the influence of the news agenda on user interests, and managing the rapid decay of news items. Furthermore, our challenge embraces the normative complexities, involving investigating the effects of recommender systems on the news flow and whether they resonate with editorial values. [1]</p> <p>Challenge Task</p> <p>The Ekstra Bladet RecSys Challenge aims to predict which article a user will click on from a list of articles that were seen during a specific impression. Utilizing the user's click history, session details (like time and device used), and personal metadata (including gender and age), along with a list of candidate news articles listed in an impression log, the challenge's objective is to rank the candidate articles based on the user's personal preferences. This involves developing models that encapsulate both the users and the articles through their content and the users' interests. The models are to estimate the likelihood of a user clicking on each article by evaluating the compatibility between the article's content and the user's preferences. The articles are ranked based on these likelihood scores, and the precision of these rankings is measured against the actual selections made by users. [1]</p> <p>Dataset Information</p> <p>The Ekstra Bladet News Recommendation Dataset (EB-NeRD) was created to support advancements in news recommendation research. It was collected from user behavior logs at Ekstra Bladet. We collected behavior logs from active users during the 6 weeks from April 27 to June 8, 2023. This timeframe was selected to avoid major events, e.g., holidays or elections, that could trigger atypical behavior at Ekstra Bladet. The active users were defined as users who had at least 5 and at most 1,000 news click records in a three-week period from May 18 to June 8, 2023. To protect user privacy, every user was delinked from the production system when securely hashed into an anonymized ID using one-time salt mapping. Alongside, we provide Danish news articles published by Ekstra Bladet. Each article is enriched with textual context features such as title, abstract, body, categories, among others. Furthermore, we provide features that have been generated by proprietary models, including topics, named entity recognition (NER), and article embeddings. [2]</p> <p>For more information on the dataset.</p>"},{"location":"writing/2024/12/15/recsys-challenge-2024-model-selection/#news-recommendation","title":"News Recommendation","text":"<p>Why do we care about news recommendations?</p> <p>With the rise of online news services like Google News, millions now access news conveniently online. However, finding relevant articles can be challenging. Personalized news recommender systems (NRS) address this by tailoring recommendations to individual interests, enhancing user experience and saving time. These systems track recent user interactions and incorporate factors like publication date, recency, and popularity into their algorithms.</p> <p>What are important aspects of personalized news recommenders?</p> <p>Unlike recommendation methods based solely on non-personalized factors like news popularity and freshness, personalized news recommendation (PNR) deeply considers each user's interests. It matches news content, location, and category to user preferences, offering personalized services. Key characteristics of PNR include:</p> <p>1) Diversity: High-quality recommendations require diverse results to enhance user experience and engagement. Most methods focus on accuracy, often neglecting diversity.</p> <p>2) Timeliness: Delivering news quickly is crucial, as the value of news diminishes over time.</p> <p>3) Popularity: Current methods predict click-through rates (CTR) using headlines, summaries, and entities, but few integrate news popularity and user attention to popular news in their predictions.</p> <p>How can we solve this problem?</p> <p>Understanding how predictions are made by standard news recommenders is often difficult due to unclear influencing factors. To address this, identifying the key factors that determine whether a user will click on an article is crucial. This approach offers several benefits:</p> <p>1) It helps system designers understand user behavior and improve recommendations.</p> <p>2) Transparent recommendations can increase user trust and acceptance.</p> <p>3) An explainable system enhances user experience, building overall trust.</p> <p>What are area in which personalzied news recommendations fall under?</p>"},{"location":"writing/2024/12/15/recsys-challenge-2024-model-selection/#time-based-news-recommendation","title":"Time-Based News Recommendation","text":"<p>Time-based news recommendation involves timeliness and real-time updates, crucial features that distinguish news recommendations from others. News loses value over time, so systems must update recommendations based on current user behavior (e.g., pulling and sliding). This ensures quick adaptation to changing interests and enhances the user experience.</p> <p>Liu et al. propose a dual-task deep neural network model that uses an extended time module to refine news embedding and predict user engagement time. This method improves the timeliness of recommendations and promotes the dissemination of the latest news. However, it can slightly diminish the focus on user interests, highlighting the need to balance timeliness and user preferences in news recommendation systems. [1] </p>"},{"location":"writing/2024/12/15/recsys-challenge-2024-model-selection/#location-based-news-recommendation","title":"Location-Based News Recommendation","text":"<p>Mobile users' locations vary, making it essential to consider their current location for more accurate news recommendations. Location-based news recommendations help users find local news and stay updated on nearby events. However, research often focuses on improving location matching accuracy, neglecting location-aware user preferences.</p> <p>Yuan et al. address this by incorporating the location of news events into recommendation models. They propose an algorithm to extract news event locations and use a vector space model to represent news features. Separate user interest models are then constructed for news with and without geographical locations. [2]</p>"},{"location":"writing/2024/12/15/recsys-challenge-2024-model-selection/#social-network-based-news-recommendation","title":"Social Network-Based News Recommendation","text":"<p>Social networks, composed of nodes (individuals) and edges (relationships), enable users to make friends and share information, significantly impacting information dissemination. Social information, reflecting users' and their friends' activities, captures the dynamic and diverse nature of user interests. Experiments show that incorporating social factors dynamically captures changing user interests, improving news recommendation effectiveness. </p> <p>Saravanapriya et al. propose a multilabel convolutional neural network that mines social media to predict users' multilabel interests and recommend popular news articles based on user tags. [3]</p>"},{"location":"writing/2024/12/15/recsys-challenge-2024-model-selection/#sesssion-based-news-recommendation","title":"Sesssion-Based News recommendation","text":"<p>Session-based news recommendation models user preferences over short periods to provide personalized reading suggestions.</p> <p>Moreira et al. propose Chameleon, a session-based deep learning meta-architecture using recurrent neural networks (RNN) to model users' sequential interests. [4]</p> <p>How do traditional recommendaton system methods apply to news?</p> <p>Traditional news recommendation methods fall into three categories: collaborative filtering-based, content-based, and hybrid methods.</p> <p>1) Collaborative filtering: Recommends news based on user similarities or news similarity. However, it often faces data sparsity and cold start issues due to insufficient user ratings, metadata, and textual content for new or less popular news.</p> <p>2) Content-based: Recommends news with similar content to the clicked news, focusing on mining semantic features of news texts.</p> <p>3) Hybrid: Combines collaborative filtering and content-based approaches to overcome limitations of individual methods.</p> <p>Personalized News Recommender Workflow</p> <p>In a personalized news recommender system, when a user visits the platform, a small set of candidate news is recalled from a large pool. The personalized recommender then ranks these based on inferred user interests from profiles. The top K news are displayed, and user behaviors on these are recorded to update user profiles for future recommendations. [5]</p> <p></p> <p>There are three key components in this architecture that are crucial in providing accurate recommendations:</p> <p>1) News Modeling</p> <p>News modeling, crucial for news recommendation, employs two main techniques: feature-based and deep learning-based methods.</p> <p>Feature-based: Relies on handcrafted features to represent news articles. For example, in collaborative filtering (CF), news articles are often represented by their IDs. However, this approach suffers from severe cold-start problems and suboptimal performance.</p> <p>Deep learning-based: Incorporates content features to represent news, often using features extracted from news texts. For instance, methods like SF-IDF use WordNet synonym sets to enhance TF-IDF. Additionally, factors like news popularity and recency are considered. However, manually designing features requires significant effort and domain knowledge, and may not fully capture semantic information encoded in news texts.</p> <p>2) User Modeling</p> <p>User modeling techniques in news recommendation fall into two main categories: feature-based and deep learning-based methods.</p> <p>Feature-based: Often represent users with their IDs but suffer from data sparsity issues. To improve accuracy, other user information such as click behaviors on news is considered. For instance, Garcin et al. use Latent Dirichlet Allocation (LDA) to extract topics from news titles, summaries, and bodies, aggregating topic vectors of clicked news into a user vector.</p> <p>Deep learning-based: Utilize deep learning techniques to model user interests more accurately. These methods can automatically learn complex patterns from user interactions with news articles, potentially alleviating the need for manual feature engineering.</p> <p>3) Personalized Ranking</p> <p>Once news and user interests are modeled, the next step is to rank candidate news in a personalized manner. Methods typically prioritize news based on their relevance to user interests. Accurately measuring this relevance is the core challenge in personalized news ranking.</p> <p>How do we evaluate these methods?</p> <p>Various metrics evaluate news recommender system performance, focusing on ranking relevance. The Area Under Curve (AUC) score is commonly used, especially in methods treating recommendation as a classification problem.</p> \\[ AUC = \\frac{|\\left((i,j)|Rank(p_i) &lt; Rank(n_j)\\right)|}{N_pN_n}\\] <p>\\(N_p\\) and \\(N_n\\) represent the numbers of positive and negative samples, respectively. \\(p_i\\) is the predicted score of the \\(i\\)-th positive sample, and \\(n_j\\) is the score of the \\(j\\)-th negative sample.</p> <p>Due to the large volume of news, users typically focus more on news at the top of the recommendation list. Some methods adjust recommendation results based on the ranking list. Common ranking evaluation metrics include MRR (mean reciprocal rank) and NDCG (normalized discounted cumulative gain).</p> \\[MRR = \\frac{1}{|U|}\\sum_{u \\in U}\\frac{1}{rank_a}\\] <p>Here, \\(u \\in U\\) traverses all users, and \\(rank_u\\) denotes the position of the first TP example in the user recommendation list.</p> \\[ NDCG@K = \\frac{\\sum_{i = 1}^k (2^{r_i} - 1)/log_2(l+i)}{\\sum_{u \\in U} 1/log_2(l+i)}\\] <p>where \\(r_i\\) is the relevance score of the \\(i\\)-th news. If user clicks on the \\(i\\)-th news, the value of \\(r_i\\) is 1.</p>"},{"location":"writing/2024/12/15/recsys-challenge-2024-model-selection/#model-selection","title":"Model Selection","text":"<p>We will explore news recommendation systems, starting from first principles and progressing to state-of-the-art approaches. This will include exploring content-based approaches and hybrid-based approaches.</p>"},{"location":"writing/2024/12/15/recsys-challenge-2024-model-selection/#content-based","title":"Content Based","text":"<p>Our first approach will be utilizing TF-IDF vectorization because of its simplicity.</p>"},{"location":"writing/2024/12/15/recsys-challenge-2024-model-selection/#tf-idf-vectorization","title":"TF-IDF vectorization","text":"<p>It stands for Term Frequency-Inverse Document Frequency and aims to quantify the importance of each word in a document relative to a collection of documents (the corpus). Words that appear frequently in a document but rarely in the rest of the corpus get high scores, making them important features. It has three major components:</p> <p>(1) Term Frequency (TF): Measures how frequently a term occurs in a document.</p> \\[ TF(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d}\\] <p>(2) Inverse Document Frequency (IDF): Reduces the weight of terms that occur in many documents, as they are less informative.</p> \\[ IDF(t) = \\log\\left(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing term } t}\\right)\\] <p>(3) TF-IDF Score: Combines TF and IDF to assign a score to each term in a document.</p> \\[ \\text{TF-IDF}(t, d) = TF(t, d) \\times IDF(t)\\] <p>Next, we need to use a similarity method to compare documents, such as cosine similarity or FAISS.</p>"},{"location":"writing/2024/12/15/recsys-challenge-2024-model-selection/#cosine-similarity","title":"Cosine Similarity","text":"<p>Cosine similarity is a measure used to determine the similarity between two non-zero vectors in a multi-dimensional space. It calculates the cosine of the angle between the vectors, with values ranging from -1 to 1. In the context of NLP and recommender systems, it is commonly used to compare TF-IDF or word embeddings to evaluate the similarity between documents or items. Setting an appropriate threshold value is crucial for determining the significance of similarity. </p> \\[ \\text{cosine_similarity}(\\mathbf{A}, \\mathbf{B}) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|}\\] <p>Implmentation</p> <p><pre><code># Packages\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\nimport pandas as pd\nimport sklearn\n\n# Load in training data\n# Articles\ndf_art = pd.read_parquet(\"Data/Small/articles.parquet\")\n\n# Behaviors\ndf_bev = pd.read_parquet(\"Data/Small/train/behaviors.parquet\")\n\n# History\ndf_his = pd.read_parquet(\"Data/Small/train/history.parquet\")\n\n# Preprocess the data\n# Convert datatype of column first\ndf_bev['article_id'] = df_bev['article_id'].apply(lambda x: x if isinstance(x, str) else int(x) if not np.isnan(x) else x)\n\n# Join bevhaiors to article\ndf = df_bev.join(df_art.set_index(\"article_id\"), on=\"article_id\")\n\n# Join bevhaiors to history\ndf = df.join(df_his.set_index(\"user_id\"), on=\"user_id\")\n\n# Drop all other dataframes from me\ndf.dropna(subset=['article_id'], inplace=True)\n\n# Change article IDs into int\ndf['article_id'] = df['article_id'].apply(lambda x: int(x))\ndf['article_id'] = df['article_id'].astype(np.int64)\n\n# Change age from int to string\ndf['device_type'] = df['device_type'].apply(lambda x: device_(x))\n\n# Change genders from float to string\ndf['gender'] = df['gender'].apply(lambda x: gender_(x))\n\n# Change age to str it's a range\ndf['age'] = df['age'].astype('Int64')\ndf['age'] = df['age'].astype(str)\ndf['age'] = df['age'].apply(\n    lambda x: x if x == '&lt;NA&gt;' else x + ' - ' + x[0] + '9')\n\n\n# Change postcodes from int to str\ndf['postcode'] = df['postcode'].apply(lambda x: postcodes_(x))\n\n# Modeling\n# Rand Seed\nnp.random.seed(42)\n\n# Merge fields into strings for processing\ndf_art['topics_str'] = df_art['topics'].apply(' '.join)\ndf_art['entity_groups_str'] = df_art['entity_groups'].apply(' '.join)\ndf_art['ner_clusters_str'] = df_art['ner_clusters'].apply(' '.join)\n\n# Create a dictionary for quick lookups\narticle_content_dict = {\n    row['article_id']: f\"{row['title']} {row['body']} {row['category_str']} {row['article_type']} \"\n                       f\"{row['ner_clusters_str']} {row['entity_groups_str']} {row['topics_str']}\"\n    for _, row in df_art.iterrows()\n}\n\n# Fit TF-IDF vectorizer\nvectorizer = TfidfVectorizer(norm='l1')\ntfidf_matrix_all = vectorizer.fit_transform(article_content_dict.values())\narticle_ids = list(article_content_dict.keys())\n\n# Map article IDs to indices in the TF-IDF matrix\narticle_id_to_idx = {article_id: idx for idx, article_id in enumerate(article_ids)}\n\n# Initialize predicted impressions list\npredicted_impressions = []\n# Store similarity scores for auc_score\nsimilarity_scores = []\n\n# Process each row of the user behavior data\nfor i in df_bev.index[0:1000]:  \n    # Extract the articles viewed by the current user\n    user_article_history = df_bev.loc[i, 'article_ids_inview']\n\n    # Map article IDs to indices if they exist in the article ID-to-index dictionary\n    indices = [article_id_to_idx[x] for x in user_article_history if x in article_id_to_idx]\n\n    # If no valid indices are found, append None to predictions and skip further processing\n    if not indices:\n        predicted_impressions.append(None)\n        continue\n\n    # Compute the average TF-IDF vector for the user's article history\n    user_profile_vector = tfidf_matrix_all[indices].mean(axis=0).A\n\n    highest_similarity = 0\n    best_imp = None\n\n    # Evaluate each article in the user's history for similarity to the user's profile\n    for imp in user_article_history:\n        if imp in article_id_to_idx:\n            # Retrieve the TF-IDF vector for the article\n            imp_idx = article_id_to_idx[imp]\n            imp_tfidf_vector = tfidf_matrix_all[imp_idx].A\n\n            # Calculate the cosine similarity between the user's profile and the article\n            similarity = cosine_similarity(user_profile_vector, imp_tfidf_vector.reshape(1, -1))[0, 0]\n\n            # Update the best match if the current similarity is higher\n            if similarity &gt; highest_similarity:\n                highest_similarity = similarity\n                best_imp = imp\n\n    # If similarity is low, pick a random article; otherwise, use the best match\n    if highest_similarity &lt;= 0.5:\n        impression = np.random.choice(user_article_history)\n        predicted_impressions.append(impression)\n        # Assign low similarity value for random choice\n        similarity_scores.append(0)  \n        print(f\"User {df_bev.loc[i, 'user_id']}: Low similarity, random impression chosen\")\n    else:\n        predicted_impressions.append(best_imp)\n        similarity_scores.append(highest_similarity)\n        print(f\"User {df_bev.loc[i, 'user_id']}: Best impression {best_imp} with similarity {highest_similarity}\")\n\n# Prepare binary labels for AUC\nactual_impressions = [x[0] for x in df_bev['article_ids_clicked'].values][0:1000]\nbinary_labels = [1 if pred == actual else 0 for pred, actual in zip(predicted_impressions, actual_impressions)]\nauc_score = sklearn.metrics.roc_auc_score(binary_labels, similarity_scores)\n\n# Calculate accuracy\ny_pred = predicted_impressions\ny_true = actual_impressions\nacc = sklearn.metrics.accuracy_score(y_true, y_pred)\n\nprint(\"-------------------------\")\nprint(\"Accuracy:\", acc)\nprint(\"AUC Score:\", auc_score)\n</code></pre> The model's accuracy is 0.12, lower than a random model's 1/6 (\u22480.1667) chance. However, its ROC-AUC score is 0.59, indicating a decent starting point as it outperforms random guessing.</p>"},{"location":"writing/2024/12/15/recsys-challenge-2024-model-selection/#faiss","title":"FAISS","text":"<p>FAISS (Facebook AI Similarity Search) is a library developed by Facebook AI that provides efficient tools for similarity search and clustering of dense vectors. The central task in FAISS is finding vectors in a database that are closest to a query vector, typically using a similarity measure like cosine similarity or Euclidean distance. Setting an appropriate threshold value is crucial for determining the significance of similarity. In some cases, a dimensional reduction techniques such as SVD must be employed to lower the number of memory required for computation.</p> <p>Implmentation</p> <p><pre><code># Packages\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nimport pandas as pd\nimport sklearn\nimport faiss  \n\n# Load in training data\n# Articles\ndf_art = pd.read_parquet(\"Data/Small/articles.parquet\")\n\n# Behaviors\ndf_bev = pd.read_parquet(\"Data/Small/train/behaviors.parquet\")\n\n# History\ndf_his = pd.read_parquet(\"Data/Small/train/history.parquet\")\n\n# Preprocess the data\n# Convert datatype of column first\ndf_bev['article_id'] = df_bev['article_id'].apply(lambda x: x if isinstance(x, str) else int(x) if not np.isnan(x) else x)\n\n# Join bevhaiors to article\ndf = df_bev.join(df_art.set_index(\"article_id\"), on=\"article_id\")\n\n# Join bevhaiors to history\ndf = df.join(df_his.set_index(\"user_id\"), on=\"user_id\")\n\n# Drop all other dataframes from me\ndf.dropna(subset=['article_id'], inplace=True)\n\n# Change article IDs into int\ndf['article_id'] = df['article_id'].apply(lambda x: int(x))\ndf['article_id'] = df['article_id'].astype(np.int64)\n\n# Change age from int to string\ndf['device_type'] = df['device_type'].apply(lambda x: device_(x))\n\n# Change genders from float to string\ndf['gender'] = df['gender'].apply(lambda x: gender_(x))\n\n# Change age to str it's a range\ndf['age'] = df['age'].astype('Int64')\ndf['age'] = df['age'].astype(str)\ndf['age'] = df['age'].apply(\n    lambda x: x if x == '&lt;NA&gt;' else x + ' - ' + x[0] + '9')\n\n\n# Change postcodes from int to str\ndf['postcode'] = df['postcode'].apply(lambda x: postcodes_(x))\n\n# Modeling\n# Rand Seed\nnp.random.seed(42)\n\n# Merge fields into strings for processing\ndf_art['topics_str'] = df_art['topics'].apply(' '.join)\ndf_art['entity_groups_str'] = df_art['entity_groups'].apply(' '.join)\ndf_art['ner_clusters_str'] = df_art['ner_clusters'].apply(' '.join)\n\n# Create a dictionary for quick lookups\narticle_content_dict = {\n    row['article_id']: f\"{row['title']} {row['body']} {row['category_str']} {row['article_type']} \"\n                       f\"{row['ner_clusters_str']} {row['entity_groups_str']} {row['topics_str']}\"\n    for _, row in df_art.iterrows()\n}\n\n# Fit TF-IDF vectorizer\nvectorizer = TfidfVectorizer(norm='l1')\ntfidf_matrix_all = vectorizer.fit_transform(article_content_dict.values())\narticle_ids = list(article_content_dict.keys())\n\n# Dimensionality reduction for the TF-IDF matrix\n# Number of components to keep\nn_components = 100  \nsvd = TruncatedSVD(n_components=n_components)\ntfidf_matrix_reduced = svd.fit_transform(tfidf_matrix_all)\n\n# Create a FAISS index for the reduced TF-IDF matrix\nindex = faiss.IndexFlatL2(tfidf_matrix_reduced.shape[1]) \nindex.add(tfidf_matrix_reduced)  \n\n# Map article IDs to indices in the TF-IDF matrix\narticle_id_to_idx = {article_id: idx for idx, article_id in enumerate(article_ids)}\n\n# Initialize predicted impressions list\npredicted_impressions = []\n# Store similarity scores for auc_score\nsimilarity_scores = []\n\n# Process each user behavior\nfor i in df_bev.index[0:1000]:\n    user_article_history = df_bev.loc[i, 'article_ids_inview']\n    indices = [article_id_to_idx[x] for x in user_article_history if x in article_id_to_idx]\n\n    if not indices:\n        predicted_impressions.append(None)\n        continue\n\n    # Aggregate user history into a single vector\n    user_profile_vector = tfidf_matrix_reduced[indices].mean(axis=0).reshape(1, -1).astype(np.float32)\n\n    # Use FAISS to find nearest neighbors (articles) based on user profile vector\n    # Search for top 10 nearest neighbors (articles)\n    D, I = index.search(user_profile_vector, k=10)\n\n    # Get the most similar article based on the nearest neighbor\n    highest_similarity = D[0][0]\n    best_imp_idx = I[0][0]\n\n    # Handle low similarity by selecting a random article\n    if highest_similarity &lt; 0.00005:\n        impression = np.random.choice(user_article_history)\n        predicted_impressions.append(impression)\n        similarity_scores.append(0)\n        print(f\"User {df_bev.loc[i, 'user_id']}: Low similarity, random impression chosen\")\n    else:\n        # Retrieve the article ID of the best match\n        best_imp = article_ids[best_imp_idx]\n        predicted_impressions.append(best_imp)\n        # Store the similarity score\n        similarity_scores.append(highest_similarity)  \n        print(f\"User {df_bev.loc[i, 'user_id']}: Best impression {best_imp} with similarity {highest_similarity}\")\n\n# Prepare binary labels for AUC\nactual_impressions = [x[0] for x in df_bev['article_ids_clicked'].values][0:1000]\nbinary_labels = [1 if pred == actual else 0 for pred, actual in zip(predicted_impressions, actual_impressions)]\nauc_score = sklearn.metrics.roc_auc_score(binary_labels, similarity_scores)\n\n# Calculate accuracy\ny_pred = predicted_impressions\ny_true = actual_impressions\nacc = sklearn.metrics.accuracy_score(y_true, y_pred)\n\nprint(\"-------------------------\")\nprint(\"Accuracy:\", acc)\nprint(\"AUC Score:\", auc_score)\n</code></pre> The model's accuracy is 0.096, below a random model's 1/6 (\u22480.1667) chance, and its ROC-AUC score of 0.3324 indicates it performs worse than random guessing.</p>"},{"location":"writing/2024/12/15/recsys-challenge-2024-model-selection/#hybrid-approach","title":"Hybrid-Approach","text":"<p>Content-based approaches performed poorly due to the use of TF-IDF vectorization, a bag-of-words model that fails to capture relationships between words. Now, we'll use more state-of-the-art models!</p> <p>What are some hybrid-based CTR approaches that can be applied to news recommendation?</p> <p>While researching novel recommender systems for CTR prediction, I discovered the BARS-CTR paper, which offers an open-source benchmark across diverse datasets and models. Its GitHub repository, FuxiCTR, was pivotal in understanding and applying hybrid approaches like Deep &amp; Cross Network (DCN) and Deep Interest Network (DIN). [6]</p>"},{"location":"writing/2024/12/15/recsys-challenge-2024-model-selection/#deep-interest-network","title":"Deep Interest Network","text":"<p>The Deep Interest Network (DIN) enhances CTR prediction by adaptively capturing users' diverse interests. It uses a local activation unit to model user behaviors in relation to the specific ad, improving on traditional Embedding &amp; MLP methods.</p> <p></p> <p>The local activation unit in DIN is a key component that allows the model to capture the diversity of user interests by dynamically generating a user representation vector based on the relevance of their past behaviors to the specific ad being shown. Instead of a fixed representation, this unit emphasizes behaviors related to the ad being considered. For example, if a user has browsed both clothes and electronics, the local activation unit will give more weight to clothes-related behaviors when predicting their likelihood to click on a T-shirt ad. This weighted sum pooling approach, similar to attention mechanisms but without normalizing the weights, retains information about the intensity of the user's interest, allowing DIN to distinguish between strong and weak interest levels. [7]</p>"},{"location":"writing/2024/12/15/recsys-challenge-2024-model-selection/#data-preparation","title":"Data Preparation","text":"<p>The data preparation script is displayed here.</p> <p>The main goal is to preprocess news and user interaction data across the training, validation, and testing sets.</p> <ul> <li>Inputs: article, behavior, history .parquet files</li> <li>Output: training, validation, testing .csv files</li> </ul> <p>Function</p> <ol> <li> <p>Loading in news articles from the parquet files.</p> </li> <li> <p>Tokenize and map categorical features.</p> </li> <li> <p>Process user interaction history.</p> </li> <li> <p>Create feature mappings for categories, sentiments, and article types.</p> </li> <li> <p>Generate CSV files with processed features.</p> </li> </ol>"},{"location":"writing/2024/12/15/recsys-challenge-2024-model-selection/#model-training","title":"Model training","text":"<p>Train a Deep Interest Network (DIN) model. The model configuration is here. </p> <p>Important parameters</p> <p>Model Architecture</p> <ul> <li>embedding_dim: Dimensionality of feature embeddings (default: 40)</li> <li>dnn_hidden_units: Neural network architecture (default: [500, 500, 500])</li> <li>dnn_activations: Activation function for dense layers (default: relu)</li> </ul> <p>Attention Mechanism</p> <ul> <li>din_target_field: The target item being predicted</li> <li>din_sequence_field: User's historical interaction sequence</li> <li>attention_hidden_units: Structure of attention network</li> <li>attention_hidden_activations: Activation in attention layers (default: \"Dice\")</li> <li>din_use_softmax: Whether to use softmax in attention mechanism</li> </ul> <p>Training Dynamics</p> <ul> <li>learning_rate: Controls model convergence (default: 1e-3)</li> <li>batch_size: Number of samples per training iteration</li> <li>epochs: Total training iterations</li> <li>optimizer: Optimization algorithm (default: adam)</li> </ul> <p>Below is a code snippet of a DIN dataset config that performed the best. <pre><code>dataset_config:\n    ebnerd_large_x2:\n        data_root: ./data/\n        data_format: csv\n        train_data: ./data/Ebnerd_large_x2/train.csv\n        valid_data: ./data/Ebnerd_large_x2/valid.csv\n        test_data: ./data//Ebnerd_large_x2/test.csv\n        min_categr_count: 10\n        data_block_size: 100000\n        streaming: True\n        feature_cols:\n            - {name: impression_id, active: True, dtype: int, type: meta, remap: False}\n            - {name: read_time, active: True, dtype: float, type: numeric, fill_na: 0}\n            - {name: user_id, active: True, dtype: str, type: categorical}\n            - {name: article_id, active: True, dtype: str, type: categorical}\n            - {name: trigger_id, active: True, dtype: str, type: categorical}\n            - {name: device_type, active: True, dtype: str, type: categorical}\n            - {name: is_sso_user, active: True, dtype: str, type: categorical}\n            - {name: gender, active: True, dtype: str, type: categorical}\n            - {name: postcode, active: True, dtype: str, type: categorical}\n            - {name: age, active: True, dtype: str, type: categorical}\n            - {name: is_subscriber, active: True, dtype: str, type: categorical}\n            - {name: premium, active: True, dtype: str, type: categorical}\n            - {name: article_type, active: True, dtype: str, type: categorical}\n            - {name: ner_clusters, active: True, dtype: str, type: sequence, splitter: ^, max_len: 5, padding: pre}\n            - {name: topics, active: True, dtype: str, type: sequence, splitter: ^, max_len: 5, padding: pre}\n            - {name: category, active: True, dtype: str, type: categorical}\n            - {name: subcategory, active: True, dtype: str, type: sequence, splitter: ^, max_len: 5, padding: pre}\n            - {name: total_inviews, active: False, dtype: float, type: numeric, fill_na: 0}\n            - {name: total_pageviews, active: False, dtype: float, type: numeric, fill_na: 0}\n            - {name: total_read_time, active: False, dtype: float, type: numeric, fill_na: 0}\n            - {name: sentiment_score, active: False, dtype: float, type: numeric, fill_na: 0}\n            - {name: sentiment_label, active: True, dtype: str, type: categorical}\n            - {name: subcat1, active: True, dtype: str, type: categorical}\n            - {name: hist_id, active: True, dtype: str, type: sequence, splitter: ^, max_len: 50, padding: pre, share_embedding: article_id}\n            - {name: hist_cat, active: True, dtype: str, type: sequence, splitter: ^, max_len: 50, padding: pre, share_embedding: category}\n            - {name: hist_subcat1, active: True, dtype: str, type: sequence, splitter: ^, max_len: 50, padding: pre, share_embedding: subcat1}\n            - {name: hist_sentiment, active: True, dtype: str, type: sequence, splitter: ^, max_len: 50, padding: pre, share_embedding: sentiment_label}\n            - {name: hist_type, active: True, dtype: str, type: sequence, splitter: ^, max_len: 50, padding: pre, share_embedding: article_type}\n            - {name: publish_days, active: True, dtype: str, type: categorical}\n            - {name: publish_hours, active: True, dtype: str, type: categorical}\n            - {name: impression_hour, active: True, dtype: str, type: categorical}\n            - {name: impression_weekday, active: True, dtype: str, type: categorical}\n            - {name: pulish_3day, active: True, dtype: str, type: categorical}\n            - {name: pulish_7day, active: True, dtype: str, type: categorical}\n            - {name: article_id_img, active: True, dtype: str, type: categorical, freeze_emb: True,\n               preprocess: \"copy_from(article_id)\", pretrain_dim: 64, pretrained_emb: \"./data/Ebnerd_large_x1/image_emb_dim64.npz\", \n               pretrain_usage: \"init\", min_categr_count: 1}\n            - {name: article_id_text, active: True, dtype: str, type: categorical, freeze_emb: True,\n               preprocess: \"copy_from(article_id)\", pretrain_dim: 64, pretrained_emb: \"./data/Ebnerd_large_x1/contrast_emb_dim64.npz\", \n               pretrain_usage: \"init\", min_categr_count: 1}\n            - {name: hist_id_img, active: True, dtype: str, type: sequence, splitter: ^, max_len: 50, padding: pre, freeze_emb: True,\n               preprocess: \"copy_from(hist_id)\", pretrain_dim: 64, pretrained_emb: \"./data/Ebnerd_large_x1/image_emb_dim64.npz\", \n               pretrain_usage: \"init\", min_categr_count: 1, share_embedding: article_id_img}\n            - {name: hist_id_text, active: True, dtype: str, type: sequence, splitter: ^, max_len: 50, padding: pre, freeze_emb: True,\n               preprocess: \"copy_from(hist_id)\", pretrain_dim: 64, pretrained_emb: \"./data/Ebnerd_large_x1/contrast_emb_dim64.npz\", \n               pretrain_usage: \"init\", min_categr_count: 1, share_embedding: article_id_text}     \n        label_col: {name: click, dtype: float}\n</code></pre></p> <p>Below is a code snippet of a DIN model config that performed the best. <pre><code>DIN_doubleq:\n    attention_dropout: 0.2\n    attention_hidden_activations: ReLU\n    attention_hidden_units: [512, 256]\n    attention_output_activation: null\n    batch_norm: true\n    batch_size: 7168\n    dataset_id: TBD\n    debug_mode: false\n    din_sequence_field: click_history\n    din_target_field: item_id\n    din_use_softmax: false\n    dnn_activations: ReLU\n    dnn_hidden_units: [1024, 512, 256]\n    early_stop_patience: 2\n    embedding_dim: 64\n    embedding_regularizer: 0.0001\n    epochs: 10\n    eval_steps: null\n    feature_config: null\n    feature_specs: null\n    group_id: impression_id\n    item_info_fields: 12\n    learning_rate: 0.0005\n    loss: binary_crossentropy\n    metrics: [avgAUC, AUC, logloss]\n    model: DIN\n    model_root: ./checkpoints/\n    monitor: avgAUC\n    monitor_mode: max\n    net_dropout: 0.1\n    net_regularizer: 0\n    num_workers: 3\n    optimizer: adam\n    pickle_feature_encoder: true\n    save_best_only: true\n    seed: 20242025\n    shuffle: true\n    task: binary_classification\n    use_features: null\n    verbose: 1\n</code></pre></p>"},{"location":"writing/2024/12/15/recsys-challenge-2024-model-selection/#prediction","title":"Prediction","text":"<p>The submission script is displayed here.</p> <p>The main goal is to make predictions on the testing set.</p> <p>Function</p> <ol> <li>Create a data loader for test data</li> <li>Load test data from CSV</li> <li>Predicts scores for each sample</li> <li>Ranks predictions for each impression </li> <li>Writes results to a predictions.txt file</li> <li>Zip the predictions </li> </ol>"},{"location":"writing/2024/12/15/recsys-challenge-2024-model-selection/#procedure","title":"Procedure","text":"<ol> <li> <p>Prepare the data by preprocessing news and user interaction data. (go to fuxcitr_dir/data directory)</p> <p><code>python prepare_data_v2.py</code></p> </li> <li> <p>Run the following script in the fuxcitr_dir directory to train the model on train and validation sets.</p> <p><code>python run_param_tuner.py --config config/DIN_ebnerd_large_x2_tuner_config_doubleq_02.yaml --gpu 0</code></p> </li> <li> <p>Run the following script in the fuxcitr_dir directory to make predictions on the test set.</p> <p><code>python submit.py --config config/DIN_ebnerd_large_x2_tuner_config_doubleq_02 --expid DIN_ebnerd_large_x2_001_1860e41e --gpu 1</code></p> </li> </ol>"},{"location":"writing/2024/12/15/recsys-challenge-2024-model-selection/#results","title":"Results","text":"<p>The results of our best-performing run, including detailed metrics, can be found here. It outlines the specifications of our top DIN model, in which we achieved an ROC-AUC score of 0.676359.</p> <pre><code> 20241214-233112,[command] python run_expid.py --config config/DiN_ebnerd_large_x2_tuner_config_doubleq_02 --expid DIN_ebnerd_large_x2_001_3e89b3ec --gpu 0,[exp_id] DIN_ebnerd_large_x2_001_3e89b3ec,[dataset_id] ebnerd_large_x2_7ea969aa,[train] N.A.,[val] avgAUC: 0.676359 - MRR: 0.443784 - NDCG(k=5): 0.504221,[test] \n</code></pre>"},{"location":"writing/2024/12/15/recsys-challenge-2024-model-selection/#deep-cross-network","title":"Deep Cross Network","text":"<p>The Deep Cross Network (DCN) combines a cross network and a deep neural network to capture feature interactions in high-dimensional sparse data. The cross network handles explicit feature crossing, while the deep network models implicit interactions. [8]</p> <p></p> <p>Here's a simplified breakdown of its components:</p> <ol> <li> <p>Embedding and Stacking Layer</p> <ul> <li>Converts categorical features (e.g., \"country=USA\") into numerical representations for the model to process.  </li> <li>Combines these embeddings with other numerical features into a single input vector.  </li> </ul> </li> <li> <p>Cross Network</p> <ul> <li>The core of the DCN, designed to efficiently identify interactions between features.  </li> <li>For example, it can uncover that users in the USA who like a specific brand are highly likely to click on a particular ad.  </li> </ul> </li> <li> <p>Deep Network</p> <ul> <li>A standard neural network component that captures complex, nonlinear patterns in the data.  </li> </ul> </li> <li> <p>Combination Layer</p> <ul> <li>Merges the outputs from the cross and deep networks.  </li> <li>The combined features are used to predict the likelihood of a user clicking on an ad.  </li> </ul> </li> </ol>"},{"location":"writing/2024/12/15/recsys-challenge-2024-model-selection/#data-preparation_1","title":"Data Preparation","text":"<p>The data preparation script is displayed here.</p> <p>The main goal is to preprocess news and user interaction data across the training, validation, and testing sets.</p> <ul> <li>Inputs: article, behavior, history .parquet files</li> <li>Output: training, validation, testing .csv files</li> </ul> <p>Function</p> <ol> <li> <p>Loading in news articles from the parquet files.</p> </li> <li> <p>Tokenize and map categorical features.</p> </li> <li> <p>Process user interaction history.</p> </li> <li> <p>Create feature mappings for categories, sentiments, and article types.</p> </li> <li> <p>Generate CSV files with processed features.</p> </li> </ol>"},{"location":"writing/2024/12/15/recsys-challenge-2024-model-selection/#model-training_1","title":"Model training","text":"<p>Train a Deep Cross Network (DCN) model. The model configuration is here. </p> <p>Important Parameters</p> <p>Model Architecture</p> <ul> <li> <p>embedding_dim: Dimensionality of feature embeddings (default: 10). This controls the size of the embedding vectors used to represent categorical features.</p> </li> <li> <p>dnn_hidden_units: Architecture of the fully connected neural network (default: []). This defines the number of layers and the units in each layer. Leave empty to use only the cross network.</p> </li> <li> <p>dnn_activations: Activation function for the dense layers (default: \"ReLU\"). Determines the non-linear transformation applied to the output of each dense layer.</p> </li> <li> <p>num_cross_layers: Number of cross layers in the CrossNet (default: 3). Controls the depth of the cross feature interactions.</p> </li> <li> <p>net_dropout: Dropout rate applied to the dense layers (default: 0). Helps to prevent overfitting.</p> </li> <li> <p>batch_norm: Whether to apply batch normalization to dense layers (default: False). Useful for stabilizing training.</p> </li> </ul> <p>Regularization</p> <ul> <li> <p>embedding_regularizer: Regularization applied to the embedding layer. Helps to control overfitting in the embeddings.</p> </li> <li> <p>net_regularizer: Regularization applied to the network layers.</p> </li> </ul> <p>Training Dynamics</p> <ul> <li>learning_rate: Controls the step size for model optimization (default: 1e-3). Affects how quickly the model converges during training.</li> <li>optimizer: Optimization algorithm used for training (default: as specified in the model initialization).</li> <li>batch_size: Number of samples per training iteration. Determines the computational efficiency and convergence stability.</li> <li>epochs: Total number of training iterations. Defines how many times the model will see the full dataset.</li> </ul> <p>Below is a code snippet of a DCN dataset config that performed the best. <pre><code>dataset_config:\n    ebnerd_large_x2:\n        data_root: ./data/\n        data_format: csv\n        train_data: ./data/Ebnerd_large_x2/train.csv\n        valid_data: ./data/Ebnerd_large_x2/valid.csv\n        test_data: ./data//Ebnerd_large_x2/test.csv\n        min_categr_count: 10\n        data_block_size: 100000\n        streaming: True\n        feature_cols:\n            - {name: impression_id, active: True, dtype: int, type: meta, remap: False}\n            - {name: read_time, active: True, dtype: float, type: numeric, fill_na: 0}\n            - {name: user_id, active: True, dtype: str, type: categorical}\n            - {name: article_id, active: True, dtype: str, type: categorical}\n            - {name: trigger_id, active: True, dtype: str, type: categorical}\n            - {name: device_type, active: True, dtype: str, type: categorical}\n            - {name: is_sso_user, active: True, dtype: str, type: categorical}\n            - {name: gender, active: True, dtype: str, type: categorical}\n            - {name: postcode, active: True, dtype: str, type: categorical}\n            - {name: age, active: True, dtype: str, type: categorical}\n            - {name: is_subscriber, active: True, dtype: str, type: categorical}\n            - {name: premium, active: True, dtype: str, type: categorical}\n            - {name: article_type, active: True, dtype: str, type: categorical}\n            - {name: ner_clusters, active: True, dtype: str, type: sequence, splitter: ^, max_len: 5, padding: pre}\n            - {name: topics, active: True, dtype: str, type: sequence, splitter: ^, max_len: 5, padding: pre}\n            - {name: category, active: True, dtype: str, type: categorical}\n            - {name: subcategory, active: True, dtype: str, type: sequence, splitter: ^, max_len: 5, padding: pre}\n            - {name: total_inviews, active: False, dtype: float, type: numeric, fill_na: 0}\n            - {name: total_pageviews, active: False, dtype: float, type: numeric, fill_na: 0}\n            - {name: total_read_time, active: False, dtype: float, type: numeric, fill_na: 0}\n            - {name: sentiment_score, active: False, dtype: float, type: numeric, fill_na: 0}\n            - {name: sentiment_label, active: True, dtype: str, type: categorical}\n            - {name: subcat1, active: True, dtype: str, type: categorical}\n            - {name: hist_id, active: True, dtype: str, type: sequence, splitter: ^, max_len: 50, padding: pre, share_embedding: article_id}\n            - {name: hist_cat, active: True, dtype: str, type: sequence, splitter: ^, max_len: 50, padding: pre, share_embedding: category}\n            - {name: hist_subcat1, active: True, dtype: str, type: sequence, splitter: ^, max_len: 50, padding: pre, share_embedding: subcat1}\n            - {name: hist_sentiment, active: True, dtype: str, type: sequence, splitter: ^, max_len: 50, padding: pre, share_embedding: sentiment_label}\n            - {name: hist_type, active: True, dtype: str, type: sequence, splitter: ^, max_len: 50, padding: pre, share_embedding: article_type}\n            - {name: publish_days, active: True, dtype: str, type: categorical}\n            - {name: publish_hours, active: True, dtype: str, type: categorical}\n            - {name: impression_hour, active: True, dtype: str, type: categorical}\n            - {name: impression_weekday, active: True, dtype: str, type: categorical}\n            - {name: pulish_3day, active: True, dtype: str, type: categorical}\n            - {name: pulish_7day, active: True, dtype: str, type: categorical}\n            - {name: article_id_img, active: True, dtype: str, type: categorical, freeze_emb: True,\n               preprocess: \"copy_from(article_id)\", pretrain_dim: 64, pretrained_emb: \"./data/Ebnerd_large_x1/image_emb_dim64.npz\", \n               pretrain_usage: \"init\", min_categr_count: 1}\n            - {name: article_id_text, active: True, dtype: str, type: categorical, freeze_emb: True,\n               preprocess: \"copy_from(article_id)\", pretrain_dim: 64, pretrained_emb: \"./data/Ebnerd_large_x1/contrast_emb_dim64.npz\", \n               pretrain_usage: \"init\", min_categr_count: 1}\n            - {name: hist_id_img, active: True, dtype: str, type: sequence, splitter: ^, max_len: 50, padding: pre, freeze_emb: True,\n               preprocess: \"copy_from(hist_id)\", pretrain_dim: 64, pretrained_emb: \"./data/Ebnerd_large_x1/image_emb_dim64.npz\", \n               pretrain_usage: \"init\", min_categr_count: 1, share_embedding: article_id_img}\n            - {name: hist_id_text, active: True, dtype: str, type: sequence, splitter: ^, max_len: 50, padding: pre, freeze_emb: True,\n               preprocess: \"copy_from(hist_id)\", pretrain_dim: 64, pretrained_emb: \"./data/Ebnerd_large_x1/contrast_emb_dim64.npz\", \n               pretrain_usage: \"init\", min_categr_count: 1, share_embedding: article_id_text}     \n        label_col: {name: click, dtype: float}\n</code></pre></p> <p>Below is a code snippet of a DCN model config that performed the best. <pre><code>DCN_doubleq:\n    batch_norm: true\n    batch_size: 7168\n    dataset_id: Ebnerd_large_data\n    debug_mode: false\n    dnn_activations: ReLU\n    dnn_hidden_units: [1024, 512, 256]\n    early_stop_patience: 2\n    embedding_dim: 64\n    embedding_regularizer: 1.0e-05\n    epochs: 10\n    eval_steps: null\n    feature_config: null\n    feature_specs: null\n    group_id: impression_id\n    item_info_fields: 12\n    learning_rate: 0.0005\n    loss: binary_crossentropy\n    metrics: [avgAUC, AUC, logloss]\n    model: DCN\n    model_root: ./checkpoints/\n    monitor: avgAUC\n    monitor_mode: max\n    net_dropout: 0.1\n    net_regularizer: 0\n    num_cross_layers: 3\n    num_workers: 3\n    optimizer: adam\n    pickle_feature_encoder: true\n    save_best_only: true\n    seed: 20242025\n    shuffle: true\n    task: binary_classification\n    use_features: null\n    verbose: 1\n</code></pre></p>"},{"location":"writing/2024/12/15/recsys-challenge-2024-model-selection/#prediction_1","title":"Prediction","text":"<p>The submission script is displayed here.</p> <p>The main goal is to make predictions on the testing set.</p> <p>Function</p> <ol> <li>Create a data loader for test data</li> <li>Load test data from CSV</li> <li>Predicts scores for each sample</li> <li>Ranks predictions for each impression </li> <li>Writes results to a predictions.txt file</li> <li>Zip the predictions </li> </ol>"},{"location":"writing/2024/12/15/recsys-challenge-2024-model-selection/#procedure_1","title":"Procedure","text":"<ol> <li> <p>Prepare the data by preprocessing news and user interaction data. (go to fuxcitr_dir/data directory)</p> <p><code>python prepare_data_v2.py</code></p> </li> <li> <p>Run the following script in the fuxcitr_dir directory to train the model on train and validation sets.</p> <p><code>python run_param_tuner.py --config config/DCN_ebnerd_large_x2_tuner_config_doubleq_02.yaml --gpu 0</code></p> </li> <li> <p>Run the following script in the fuxcitr_dir directory to make predictions on the test set.</p> <p><code>python submit.py --config config/DCN_ebnerd_large_x2_tuner_config_doubleq_01 --expid DCN_ebnerd_large_x2_001_1860e41e --gpu 1</code></p> </li> </ol>"},{"location":"writing/2024/12/15/recsys-challenge-2024-model-selection/#results_1","title":"Results","text":"<p>The results of our best-performing run, including detailed metrics, can be found here. It outlines the specifications of our best performing model, in which we achieved an ROC-AUC score of 0.6857.</p> <pre><code>20241214-205228 [command] python run_expid.py --config config/DCN_ebnerd_large_x2_tuner_config_doubleq_02 --expid DCN_ebnerd_large_x2_001_c76e9991 --gpu 0  [exp_id] DCN_ebnerd_large_x2_001_c76e9991   [dataset_id] ebnerd_large_x2_7ea969aa   [train] N.A.    [val] avgAUC: 0.685714 - MRR: 0.454175 - NDCG(k=5): 0.516215    [test] \n</code></pre>"},{"location":"writing/2024/12/15/recsys-challenge-2024-model-selection/#references","title":"References","text":"<ol> <li> <p>Hypernews: simultaneous news recommendation and active-time prediction via a double-task deep neural network.</p> </li> <li> <p>Research on news recommendation methods considering geographical location of news</p> </li> <li> <p>Saravanapriya M, Senthilkumar R, Saktheeswaran J (2022) Multi-label convolution neural network for personalized news recommendation based on social media mining.</p> </li> <li> <p>Contextual Hybrid Session-based News Recommendation with Recurrent Neural Networks</p> </li> <li> <p>Personalized News Recommendation: Methods and Challenges</p> </li> <li> <p>BARS-CTR: Open Benchmarking for Click-Through Rate Prediction</p> </li> <li> <p>Deep Interest Network for Click-Through Rate Prediction</p> </li> <li> <p>Deep &amp; Cross Network for Ad Click Predictions</p> </li> </ol>"},{"location":"writing/2024/04/28/RecSys-Overview/","title":"Recommendation Systems: Overview","text":""},{"location":"writing/2024/04/28/RecSys-Overview/#introduction","title":"Introduction","text":"<p>Purpose</p> <p>This article will cover an overview of recommendation systems. The content will be structured to answer the following questions:</p> <ul> <li>What is RecSys?</li> <li>What is the importance of Recommender Systems?</li> <li>What are the main challenges faced in Recommender Systems?</li> <li>What are the common evaluation metrics utilized in Recommender Systems?</li> <li>What are the various methods employed in Recommendation Systems?</li> <li>How has neural network architecture been utilized in Recommender Systems?</li> </ul>"},{"location":"writing/2024/04/28/RecSys-Overview/#what-is-recsys","title":"What is RecSys?","text":"<p>Recommender systems suggest things you might like by analyzing your preferences, item details, and how you've interacted with them.  Personalized recommender systems predict individual preferences based on past behavior, while group recommender systems consider collective preferences to resolve conflicts. Personalized systems focus on tailoring recommendations to individual interests and are the most studied type. [1]</p>"},{"location":"writing/2024/04/28/RecSys-Overview/#what-is-the-importance-of-recommender-systems","title":"What is the importance of Recommender Systems?","text":"<p>In industry, recommender systems are crucial for improving user experience and driving sales on online platforms. For instance, 80% of Netflix movie views and 60% of YouTube video clicks stem from recommendations. [2],[3]. Recommender systems are vital for keeping users engaged by supplying relevant items.engaged on the platform. </p>"},{"location":"writing/2024/04/28/RecSys-Overview/#what-are-the-main-challenges-faced-in-recommender-systems","title":"What are the main challenges faced in Recommender Systems?","text":"<p>In commercial applications, a recommender system's robustness, data bias, and fairness are critical for its success. Robustness ensures accurate recommendations despite changing data. Data bias refers to systematic errors that can lead to unfair recommendations. Fairness means providing unbiased recommendations regardless of user characteristics. [1] </p>"},{"location":"writing/2024/04/28/RecSys-Overview/#robustness","title":"Robustness","text":"<p>Adversarial attacks test recommender system robustness by altering image or text inputs. In natural language processing, attackers exploit embeddings, like user or item profiles, for attacks.</p>"},{"location":"writing/2024/04/28/RecSys-Overview/#data-bias","title":"Data bias","text":"<p>Adversarial attacks test recommender system robustness by altering inputs like pixel values or text. In natural language processing, attackers exploit embeddings, such as user or item profiles, exposure bias, and position bias for attacks. Addressing biases is crucial for reliable recommendations.</p>"},{"location":"writing/2024/04/28/RecSys-Overview/#popularity-deviation","title":"Popularity deviation","text":"<p>Popularity deviation is a challenge similar to the long-tail problem. It occurs when a small number of highly popular items dominate user interactions, while less popular items receive little attention. This skews model training towards popular items, resulting in unfair recommendations favoring popular items over others.</p>"},{"location":"writing/2024/04/28/RecSys-Overview/#selection-bias","title":"Selection bias","text":"<p>Selection bias arises when users tend to rate only products they strongly like or dislike, causing a Missing-Not-At-Random (MNAR) problem and skewing recommendations.</p>"},{"location":"writing/2024/04/28/RecSys-Overview/#exposure-bias","title":"Exposure bias","text":"<p>Exposure bias occurs when users can only see and interact with a portion of items presented by the system due to time constraints. This leads to deviations because users may miss items they would like or dislike.</p>"},{"location":"writing/2024/04/28/RecSys-Overview/#positional-deviation","title":"Positional deviation","text":"<p>Positional deviation refers to users' tendency to interact more with items placed prominently in recommendation lists, regardless of relevance to their needs. This concept impacts click-through rate prediction in recommender systems.</p>"},{"location":"writing/2024/04/28/RecSys-Overview/#fairness","title":"Fairness","text":"<p>Recommender system fairness includes user-based and item-based fairness. User-based fairness ensures no discrimination based on sensitive attributes, while item-based fairness ensures equal recommendation opportunities for all items. The cold-start and long-tail problems are examples of item-based fairness issues, with the latter also linked to exposure bias.</p>"},{"location":"writing/2024/04/28/RecSys-Overview/#user-based-fairness","title":"User-Based Fairness","text":"<p>Researchers are actively exploring methods like meta-learning, adversarial training, and differential privacy to improve user-based fairness in machine learning models. Approaches such as cold-transformer aim to enhance user preference accuracy by incorporating context-based embeddings. These efforts highlight the significance of addressing fairness and privacy concerns to better serve under-served users. [4]</p>"},{"location":"writing/2024/04/28/RecSys-Overview/#item-based-fairness","title":"Item-Based Fairness","text":"<p>Various methods address item-based fairness in recommender systems:</p> <ul> <li>Causal inference frameworks reduce popularity bias and alleviate the long tail problem.  [5]</li> <li>Adversarial training  enhances model accuracy but may amplify popularity deviation in unbalanced data distributions. [6]</li> <li>The FairGAN model tackles fairness by mapping it to the negative preference problem and preserving user utility. [7]</li> </ul>"},{"location":"writing/2024/04/28/RecSys-Overview/#what-are-the-common-evaluation-metrics-utilized-in-recommender-systems","title":"What are the common evaluation metrics utilized in Recommender Systems?","text":"<p>Evaluation metrics for recommender systems can be classified into Rating Based Indicators (RBI) and Item Based Indicators (IBI). RBI assesses recommendations using predicted rating scores, while IBI evaluates recommendations based on a list of predicted items. </p>"},{"location":"writing/2024/04/28/RecSys-Overview/#rating-based-indicator","title":"Rating-Based Indicator","text":"<p>Rating-based indicators assess the quality of predicted rating scores, often by calculating the gap between implicit or explicit labels. Common measurements include Root Mean Squared Error (RMSE), especially when the rating score is an explicit value. </p> <p>RMSE is calculated by</p> \\[RMSE = \\sqrt{\\frac{1}{|U||I|}\\sum_{u \\in U, i \\in I}(\\hat{r}_{ui} - r_ui)^2}\\] <p>Similarly, Mean Absolute Error (MAE) is another common measurement, given by:</p> \\[MAE = \\sqrt{\\frac{1}{|U||I|}\\sum_{u \\in U, i \\in I}|\\hat{r}_{ui} - r_ui|}\\] <p>In both equations, \\(U\\) represents the set of users, \\(I\\) represents the set of items, \\(\\hat{r}\\) denotes the predicted rating, and \\(r\\) denotes the true rating.</p> <p>RMSE and MAE are non-negative metrics, where lower values indicate better performance. However, they are sensitive to outliers because each squared difference \\(((\\hat{r_{ui}} - r_{ui}))^2\\) in RMSE, \\(|\\hat{r_{ui}} - r_{ui}|^2\\) in MAE)  contributes proportionally to the final error. [1]</p>"},{"location":"writing/2024/04/28/RecSys-Overview/#item-based-indicator","title":"Item-Based Indicator","text":"<p>When ranking information is absent in a recommender system, evaluation can utilize a confusion matrix. Here, TP (True Positives) represents used items recommended by the system, FP (False Positives) denotes unused items incorrectly recommended, FN (False Negatives) indicates used items not recommended, and TN (True Negatives) is the count of unused items not recommended. Common metrics such as Precision, Recall, and F-Measure are derived from these values for a comprehensive evaluation.</p> \\[Precision = \\frac{TP}{TP + FP}\\] \\[Recall = \\frac{TP}{TP + FN}\\] \\[F-Measure = \\frac{2*Precision*Recall}{Precision + Recall}\\]"},{"location":"writing/2024/04/28/RecSys-Overview/#ranking-indicator","title":"Ranking Indicator","text":"<p>Precision and Recall metrics are not inherently concerned with the order of recommendations. To address this, Precision@k and Recall@k focus on subsets of ordered lists, considering only the top k items for evaluation.</p> <p>A more comprehensive evaluation metric is the mean average precision (MAP@k) and mean average recall (MAR@k), which average precision and recall across all users for the top k recommendations.</p> \\[ MAP@K = \\frac{\\sum_{i=1}^k(Precision@i * rel(i))}{min(k,number\\; of\\; relevant\\; items  )}\\] \\[ MAR@K = \\frac{\\sum_{i=1}^k(Recall@i * rel(i))}{total\\; number\\; of\\; relevant\\; items  )}\\] <p>MAP@k and MAR@k utilize the indicator function \\(rel(i)\\), where 1 indicates relevance and 0 indicates irrelevance of the \\(i-th\\) item to the target. While useful for evaluating overall system performance, these metrics are not suitable for fine-grained numerical ratings as they require a threshold for conversion to binary correlations, leading to bias and loss of detailed information.</p>"},{"location":"writing/2024/04/28/RecSys-Overview/#what-are-the-various-methods-employed-in-recommendation-systems","title":"What are the various methods employed in Recommendation Systems?","text":"<p>Personalized recommender systems can be categorized into three main approaches:</p> <ul> <li> <p>Collaborative filtering</p> </li> <li> <p>Content-based</p> </li> <li> <p>Hybrid methods</p> </li> </ul>"},{"location":"writing/2024/04/28/RecSys-Overview/#collaborative-filtering","title":"Collaborative filtering","text":"<p>Collaborative filtering (CF) is a popular recommendation technique leveraging the collective knowledge of users to predict preferences. It assumes users with similar behaviors share similar opinions, utilizing user profiles created from historical interactions. CF encompasses memory-based CF and model-based CF approaches.</p>"},{"location":"writing/2024/04/28/RecSys-Overview/#memory-based-cf","title":"Memory-based CF","text":"<p>Memory-based collaborative filtering (CF) calculates similarity between users or items and generates recommendations based on sorted similarity values. It's efficient and popular, leveraging historical data to capture user behavior patterns and preferences for personalized recommendations.</p>"},{"location":"writing/2024/04/28/RecSys-Overview/#challenges","title":"Challenges","text":"<p>The sparsity of the interaction matrix, with limited non-zero values, makes recommendation predictions prone to errors. Additionally, as user and item spaces expand, similarity calculations become more complex and time-consuming, hindering scalability in large-scale applications.</p>"},{"location":"writing/2024/04/28/RecSys-Overview/#how-is-similarity-calculated","title":"How is similarity calculated?","text":"<p>Similarity calculation is essential in recommender systems, especially in collaborative filtering, where it's derived from co-rated items to predict preferences. Methods like Pearson Correlation Coefficient (PCC), Cosine similarity (COS), Mean Squared Difference (MSD), Proximity Impact Popularity (PIP), Jaccard Similarity, and Proximity Significance Singularity (PSS) are commonly used. Each method has distinct strengths and limitations, with selection dependent on system type and data characteristics.</p>"},{"location":"writing/2024/04/28/RecSys-Overview/#model-based-cf","title":"Model-based CF","text":"<p>Model-based collaborative filtering (CF) is another popular recommendation approach. It predicts user preferences based on relationships between users and items. This is commonly known as the Two-Tower paradigm.</p> <p>The two tower model encompasses two main components:</p> <ul> <li>user embeddings</li> <li>item embeddings</li> </ul> <p>User preference history, including liked items and timestamps, past searches, location, preferred languages, and relevant metadata are considered for recommendations. [9]</p> <p></p> <p>Gathers information about recommendable items, including their title, description, metadata like language and publisher, and dynamic data such as views and likes over time.</p> <p></p> <p>These components are combined by computing their dot product. A high dot product value indicates a good match between the user and the item.</p>"},{"location":"writing/2024/04/28/RecSys-Overview/#content-based","title":"Content-based","text":"<p>The content-based recommender system classifies items based on attribute information for personalized recommendations. Unlike collaborative filtering, it treats recommendations as a user-specific classification problem, learning a classifier from item features. It focuses on user preference models and interaction history. Item classification in content-based RS relies on item features obtained through item presentation algorithms. Side information can include metadata or user-generated content, typically item descriptions or full-text indexing. </p>"},{"location":"writing/2024/04/28/RecSys-Overview/#similarity-measures","title":"Similarity measures","text":"<p>Cosine Similarity: Measures the cosine of the angle between two vectors. It's widely used for text-based recommendations.</p> <p>Euclidean Distance: Measures the straight-line distance between two vectors in a multidimensional space.</p> <p>Pearson Correlation Coefficient: Measures the linear correlation between two variables. It's often used when dealing with numerical attributes.</p> <p>Jaccard Similarity: Measures the similarity between two sets by comparing their intersection to their union. It's useful for categorical data or binary features</p>"},{"location":"writing/2024/04/28/RecSys-Overview/#hybrid","title":"Hybrid","text":"<p>Recommender systems have evolved to require integration of multiple data sources for real-time, accurate recommendations. Deep learning facilitates effective fusion of diverse information, like context and content, in hybrid recommender systems, combine user-item interactions and contextual features for recommendation. [8].</p>"},{"location":"writing/2024/04/28/RecSys-Overview/#why-is-a-hybrid-approach-suitable","title":"Why is a hybrid approach suitable?","text":"<p>The cold start problem in collaborative filtering occurs when new users have insufficient data for personalized recommendations. To address this, a hybrid approach can be adopted, starting with content-based recommendations until enough user data is collected. Once sufficient data is available, the system can transition to collaborative filtering for more accurate and personalized recommendations.</p>"},{"location":"writing/2024/04/28/RecSys-Overview/#ow-has-neural-network-architecture-been-utilized-in-recommender-systems","title":"ow has neural network architecture been utilized in Recommender Systems?","text":"<p>In recent years, deep learning has significantly advanced recommendation systems, especially with architectures like the dual encoder. Deep neural networks are beneficial as they integrate multiple neural building blocks into a single, trainable function. This is particularly useful for content-based recommendations involving multi-modal data, such as text (reviews, tweets) and images (social posts, product images). CNNs and RNNs enable joint end-to-end representation learning, outperforming traditional methods that require designing modality-specific features.</p>"},{"location":"writing/2024/04/28/RecSys-Overview/#strengths","title":"Strengths","text":""},{"location":"writing/2024/04/28/RecSys-Overview/#nonlinear-transformations","title":"Nonlinear Transformations","text":"<p>Unlike linear models, deep neural networks can model non-linearity in data using nonlinear activations like ReLU, sigmoid, and tanh. This capability allows them to capture complex and intricate user-item interaction patterns</p>"},{"location":"writing/2024/04/28/RecSys-Overview/#representation-learning","title":"Representation Learning","text":"<p>Deep neural networks effectively learn underlying factors and useful representations from input data. They leverage extensive descriptive information about items and users, enhancing our understanding and improving recommendation accuracy.</p>"},{"location":"writing/2024/04/28/RecSys-Overview/#sequence-modelling","title":"Sequence Modelling","text":"<p>Deep neural networks excel in sequential modeling tasks like machine translation, natural language understanding, and speech recognition. They are well-suited for capturing the temporal dynamics of user behavior and item evolution, making them ideal for applications like next-item prediction and session-based recommendations.</p>"},{"location":"writing/2024/04/28/RecSys-Overview/#flexbility","title":"Flexbility","text":"<p>Deep learning techniques are highly flexible, allowing easy combination of neural structures to create hybrid models or replace modules. This enables the development of powerful composite recommendation models that capture various characteristics and factors.</p>"},{"location":"writing/2024/04/28/RecSys-Overview/#limitations","title":"Limitations","text":""},{"location":"writing/2024/04/28/RecSys-Overview/#interpretability","title":"Interpretability","text":"<p>Despite their success, deep learning models are often criticized for being black boxes, with non-interpretable hidden weights and activations. This limits explainability. However, neural attention models have alleviated this concern by improving the interpretability of deep neural models. [10]</p>"},{"location":"writing/2024/04/28/RecSys-Overview/#data","title":"Data","text":"<p>Another limitation of deep learning is its need for large amounts of data to support its rich parameterization. However, unlike domains such as language or vision where labeled data is scarce, recommender systems can easily gather substantial data. Million/billion scale datasets are common in both industry and academia.</p>"},{"location":"writing/2024/04/28/RecSys-Overview/#hyperparamter-tuning","title":"Hyperparamter Tuning","text":"<p>The need for extensive hyperparameter tuning is not unique to deep learning but applies to machine learning in general. Traditional models like matrix factorization also require tuning of parameters such as regularization factors and learning rates. While deep learning may introduce additional hyperparameters, this challenge is not exclusive to it.</p>"},{"location":"writing/2024/04/28/RecSys-Overview/#models","title":"Models","text":"<p>Recommendation models leveraging neural building blocks are categorized into eight subcategories aligned with deep learning models: MLP, AE, CNNs, RNNs, RBM, NADE, AM, AN, and DRL-based recommender systems. We will describe MLP, AE, CNNs, and RNNs. </p> <p>Deep learning-based recommendation models often integrate multiple deep learning techniques. The flexibility of deep neural networks allows for the combination of several neural building blocks to complement each other, resulting in a more powerful hybrid model.</p> <p></p>"},{"location":"writing/2024/04/28/RecSys-Overview/#multilayer-perceptron-based-recommendations","title":"Multilayer Perceptron based Recommendations","text":"<p>The Multilayer Perceptron (MLP) is renowned for its ability to accurately approximate any measurable function, making it a powerful and versatile network. Serving as the cornerstone for many advanced techniques, MLP is extensively utilized across diverse domains. [11]</p>"},{"location":"writing/2024/04/28/RecSys-Overview/#neural-collaborative-filtering","title":"Neural Collaborative Filtering","text":"<p>Recommendation often involves a two-way interaction between user preferences and item features. For instance, matrix factorization decomposes the rating matrix into low-dimensional user and item latent factors. Constructing a dual neural network allows for modeling this interaction between users and items.</p> <p></p> <p>The scoring function is defined as:</p> \\[\\hat{r}_{ui} = f(U^T * s_{u}^{user}, V^T * s_i^{item} | U,V, \\theta) \\] <p>where function \\(f(.)\\) represents the multilayer perceptron, and \\(\u03b8\\) is the parameters of this network. Traditional MF can be viewed as a special case of NCF. Therefore, it is convenient to fuse the neural interpretation of matrix factorization with MLP to formulate a more general model which makes use of both linearity of MF and non-linearity of MLP to enhance recommendation quality.</p> <p>The entire network can be trained with weighted square loss for explicit feedback or binary cross-entropy loss for implicit feedback, defined as:</p> \\[L = - \\sum_{(u,i) \\in OUO}r_{ui}log\\hat{r_ui}+(1-r_{ui}log(1-\\hat{r_{ui}})) \\]"},{"location":"writing/2024/04/28/RecSys-Overview/#deep-factorization-machine","title":"Deep Factorization Machine","text":"<p>DeepFM is an end-to-end model combining factorization machines (FM) and multilayer perceptrons (MLP). It captures low-order feature interactions through FM and high-order interactions via MLP's deep structure. Inspired by the wide &amp; deep network, DeepFM replaces the wide component with a neural FM, reducing the need for extensive feature engineering. The input includes user-item pairs and their features.</p> <p></p> <p>The prediction score is computed by combining the outputs of FM and MLP, represented as \\(y_{FM}\\) and \\(y_{MLP}\\), respectively. [12]</p> \\[\\hat{r}_{ui} = \\sigma(y_{FM}(x) + y_{MLP}(x))\\]"},{"location":"writing/2024/04/28/RecSys-Overview/#feature-representation-with-mlp","title":"Feature Representation with MLP","text":"<p>Originally designed for Google Play app recommendations, the model comprises two main components: a wide learning unit and a deep learning unit. The wide unit memorizes direct features from past data, while the deep unit generates abstract representations for generalization. This combination enhances the model's accuracy and diversity in recommendations.[13]</p> <p></p> <p>Formally, the learning process is represented by the equation \\(y = W_{wide}^T{x, \\phi(X)} + b\\), where \\(W_{wide}^T\\) and \\(b\\) denote the model parameters.</p> <p>The input \\({x, \\phi(x)}\\) represents the concatenated feature set, comprising the raw input feature \\(x\\) and the transformed feature \\(\\phi(x)\\). Each layer of the deep neural component follows the formula \\(a^{(l+1)}= f(W_{deep}^{(l)}+b^{(l)})\\), where \\(l\\) denotes the layer index, \\(f(.)\\) represents the activation function, and \\(W_{deep}^{(i)}\\) and \\(b^{(l)}\\) are the weight and bias terms, respectively. The wide and deep learning model is achieved by combining these two models.</p> \\[P(\\hat{r}_{ui} = 1|x = \\sigma(W_{wide}^T,{x, \\phi(x) + W_{deep}^Ta^{(l_f)} + bias} )) \\] <p>In this equation, \\(\\sigma(.)\\) represents the sigmoid function, \\(\\hat{r}_{ui}\\) denotes the binary rating label, and \\(a^{(l_f)}\\) signifies the final activation. The joint model is optimized using stochastic back-propagation, following the regularized leader algorithm. Finally, the recommendation list is generated based on the predicted scores.</p>"},{"location":"writing/2024/04/28/RecSys-Overview/#recommendation-with-deep-structured-semantic-model","title":"Recommendation with Deep Structured Semantic Model","text":"<p>The Deep Structured Semantic Model (DSSM) is a deep neural network designed to learn semantic representations of entities and measure their semantic similarities. It's commonly applied in information retrieval and particularly well-suited for top-n recommendation tasks. DSSM maps entities into a shared low-dimensional space and computes their similarities using the cosine function. [14]</p> <p>Deep Semantic Similarity based Personalized Recommendation (DSPR) is a personalized recommender system that utilizes tag annotations to represent users and items. Both users and items are mapped into a common tag space, and cosine similarity is used to measure the relevance between users and items. The loss function of DSPR is defined as:</p> \\[ L = - \\sum_{u, i}[log(e^{sim(u,i*)} -  log \\sum_{u,i} e^{sim(u,i)})] \\] <p>Negative samples \\((u, i)\\) are randomly chosen pairs of users and items that are not positively associated, serving as contrastive examples in training.</p> <p>MV-DNN is tailored for cross-domain recommendation, where users are considered as the central view and each domain (assuming there are Z domains) as auxiliary views. Each user-domain pair has a corresponding similarity score. The loss function of MV-DNN is then defined based on these similarity scores.</p> \\[ L = argmin_\\theta \\sum_{j=1} \\frac{exp(\\gamma * cosine(Y_u, Y_{u,j}))}{\\sum{exp(\\gamma*cosine(Y_u, f_a(X')))}{}}\\] <p>In this equation, \\(\\theta\\) represents the model parameters, \\(y\\) is the smoothing factor, \\(Y_u\\) denotes the output of the user view, and \\(a\\) is the index of the active view. \\(R^{da}\\) represents the input domain of view \\(a\\). MV-DNN is able to handle multiple domains effectively.</p>"},{"location":"writing/2024/04/28/RecSys-Overview/#autoencoder-based-recommendation","title":"Autoencoder based Recommendation","text":"<p>Autoencoders can enhance recommender systems in two primary ways:</p> <p>1) Feature Learning: Autoencoders can be used to learn lower-dimensional representations of user-item interactions at the bottleneck layer, effectively compressing the data into a more manageable form.</p> <p>2) Matrix Completion: Autoencoders can directly fill in the missing entries of the interaction matrix in the reconstruction layer, predicting user preferences for unseen items.</p> <p>Various types of autoencoders, including denoising, variational, contractive, and marginalized autoencoders, can be adapted for these recommendation tasks.</p>"},{"location":"writing/2024/04/28/RecSys-Overview/#autoencoder-based-collaborative-filtering","title":"Autoencoder based Collaborative Filtering","text":"<p>AutoRec uses either user partial vectors r(u)r(u) or item partial vectors r(i)r(i) as inputs to reconstruct them in the output layer. There are two versions of AutoRec:</p> <ul> <li>Item-based AutoRec</li> <li>User-based AutoRec</li> </ul> <p>The objective function for item-based AutoRec (I-AutoRec) is designed to minimize the reconstruction error between the original and predicted item vectors.</p> \\[ argmin_\\theta \\sum_{i = 1}^N || r^{(i)} - h(r^{(i)} ; \\theta) ||^2 + \\lambda * reg \\] <p>Before deploying AutoRec, consider these key points:</p> <ul> <li>Performance: I-AutoRec generally outperforms U-AutoRec, likely due to higher variance in user vectors.</li> <li>Activation Functions: The choice of activation functions \\(f(.)\\) and \\(g(.)\\) significantly impacts performance.</li> <li>Hidden Units: Moderately increasing the number of hidden units can improve results by enhancing the model's capacity.</li> <li>Depth: Adding more layers to create a deep network can lead to slight performance improvements.</li> </ul> <p></p> <p>Collaborative Denoising Autoencoder (CDAE) is used for ranking prediction based on user implicit feedback \\(r^{(u)}_{pref}\\)\u200b, where entries are 1 if the user likes an item and 0 otherwise. The input is corrupted by Gaussian noise, resulting in a corrupted version \\(p(\\hat{r}^{(u)}_{pref}| r^{(u)}_{pref})\\)\u200b drawn from a conditional Gaussian distribution. The reconstruction is represented as:</p> \\[ h(\\hat{r}^{(u)}_{pref}) = f(W_2 * g(W_1 * \\hat{r}^{(u)}_{pref} + V_u + b_1) + b_2)\\] <p>CDAE parameters are optimized by minimizing the reconstruction error between the original and reconstructed inputs.</p> \\[argmin_{W_1, W_2, V, b_1, b_2} \\frac{1}{M} \\sum_{u = 1}^M E_{p(\\hat{r}^{(u)}_{pref}|r^{(u)}_{pref})}[l(\\hat{r}^{(u)}_{pref}, h(\\hat{r}^{(u)}_{pref})] + \\lambda * reg\\] <p>CDAE initially updates its parameters using SGD over all feedback. However, considering all ratings is impractical in real-world applications. To address this, the authors proposed a negative sampling technique, which selects a small subset from the negative set (items the user hasn't interacted with). This approach significantly reduces time complexity without compromising ranking quality.</p> <p></p>"},{"location":"writing/2024/04/28/RecSys-Overview/#feature-representation-learning-with-autoencoder","title":"Feature Representation Learning with Autoencoder","text":"<p>Autoencoders are powerful for learning feature representations and can be used in recommender systems to extract features from user or item content.</p> <p>Collaborative Deep Learning (CDL) is a hierarchical Bayesian model that combines stacked denoising autoencoders (SDAE) with probabilistic matrix factorization (PMF). The authors introduced a general Bayesian deep learning framework with two interconnected components:</p> <ul> <li>Perception Component: Provides a probabilistic interpretation of ordinal SDAE.</li> <li>Task Component: Utilizes PMF for recommendation.</li> </ul> <p>This combination in CDL allows balancing side information and interaction history. The generative process of CDL is:</p> <p>1) For each layer \\(l\\) of the SDAE</p> <ul> <li>For each column \\(n\\) of weight matrix \\(W_l\\) draw \\(W_{l,*n} ~ N(0, \\lambda_w^{-1}I_{D1})\\) </li> <li>Draw the bias vector \\(b_1 ~ N(0, \\lambda_W^{-1}I_D)\\) </li> <li>For each row \\(i\\) of \\(X_i\\), draw \\(X_{i,i*} ~ N(\\sigma(X_{l-1, *}W_l + b_l), \\lambda_x^{-1}I_{D_l})\\) </li> </ul> <p>2) For each item \\(i\\) </p> <ul> <li>Draw a clean input \\(X_{c,i*} ~ N(X_L,i* \\lambda_n^{-1}I_{l_1})\\) </li> <li>Draw a latent offset vector \\(\\epsilon_1 ~ N(0, \\lambda_v^{-1}I_D)\\) and set the latent item vector: \\(V_i = \\epsilon_i + X_{L}^T\\) </li> <li>Draw a latent user vector for each user \\(u\\), \\(U_u ~ N(0, \\lambda_u^{-1}I_D)\\) </li> <li>Draw a rating \\(r_{ut}\\) for each user-item pair \\((u,i), r_{ui} ~ N(U^T_uV_i, C^{-1}_{ui})\\) </li> </ul> <p>Here, \\(W_l\\)\u200b and \\(b_l\\)\u200b are the weight matrix and bias vector for layer \\(l\\), \\(X_l\\)\u200b represents layer \\(l\\), and \\(\\lambda_w\\)\u200b, \\(\\lambda_x\\)\u200b, \\(\\lambda_n\\)\u200b, \\(\\lambda_v\\)\u200b, \\(\\lambda_u\\)\u200b are hyperparameters. \\(C_{ui}\\)\u200b is a confidence parameter for the observations.</p> <p></p> <p>In Collaborative Deep Ranking (CDR), tailored for pairwise top-n recommendation, the generative process is as follows:</p> <p>1) For each layer \\(l\\) of the SDAE:</p> <ul> <li>(Same as CDL)</li> </ul> <p>2) For each item \\(i\\):</p> <ul> <li>(Same as CDL)</li> </ul> <p>3) For each user \\(u\\):</p> <ul> <li>Draw a latent user vector for \\(u, U_u ~N(0, \\lambda_u^{-1}I_D)\\) </li> <li>For each pair-wise preference \\((i,j) \\in P_i\\) where \\(P_i = {(i,j):r_{ui} - r_{uj} &gt; 0}\\) , draw the estimator, \\(\\delta_{uij} ~ N(U^T_uV_i - U^T_uV_j, C^{-1}_{uij})\\) </li> </ul> <p>\\(\\delta_{uij} = r_{ui} - r_{uj}\\) represents the pairwise relationship of a user's preference between item \\(i\\) and item \\(j\\), where \\(r_{ui}\\)\u200b and \\(r_{uj}\\)\u200b are the ratings of items \\(i\\) and \\(j\\) by user \\(u\\), respectively. \\(C_{uij}^{\u22121}\\)\u200b is a confidence value indicating how much user \\(u\\) prefers item \\(i\\) over item \\(j\\).</p> <p></p>"},{"location":"writing/2024/04/28/RecSys-Overview/#convolutional-neural-networks-based-recommendation","title":"Convolutional Neural Networks based Recommendation","text":"<p>Convolutional Neural Networks (CNNs) excel at processing unstructured multimedia data by leveraging convolution and pooling operations. Many recommendation models based on CNNs utilize these capabilities for feature extraction from multimedia inputs.</p> <p></p>"},{"location":"writing/2024/04/28/RecSys-Overview/#feature-representation-learning-with-cnns","title":"Feature Representation Learning with CNNs","text":"<p>Convolutional Neural Networks (CNNs) excel at feature representation learning from images, text, audio, and video. In Point-of-Interest (POI) recommendation, CNNs are particularly effective for image feature extraction. For example, Visual POI (VPOI) uses CNNs to extract features from images. This data is then integrated into a recommendation model based on Probabilistic Matrix Factorization (PMF), which examines the interactions between visual content and both latent user factors and latent location factors.</p> <p>In CNNs used for recommendation systems, convolutional and max-pooling layers extract visual features from image patches. These features, along with user information, contribute to personalized recommendations. The network typically comprises two CNNs for image representation learning and a Multilayer Perceptron (MLP) for modeling user preferences.</p> <p>During training, the network compares pairs of images: one positive image (liked by the user) and one negative image (disliked by the user) against the user's preferences. Training data consists of triplets \\(t\\) where the positive image \\(I_t^+\\)\u200b should be closer to the user UtUt\u200b than the negative image \\(I_t^\u2212\\)\u200b according to a distance metric \\(D(\u03c0(U_t),\u03d5(I_t^\u2212))D(\u03c0(U_t\u200b)\\), where \\(D(.)\\) represents the distance metric, such as Euclidean distance.</p>"},{"location":"writing/2024/04/28/RecSys-Overview/#graph-cnns-for-recommendation","title":"Graph CNNs for Recommendation","text":"<p>Graph Convolutional Networks (GCNs) excel at handling non-Euclidean data like social networks and knowledge graphs. In recommendations, GCNs can model interactions as bipartite graphs, integrating user and item information, such as social networks and item relationships, into the recommendation process.</p> <p>Their model generates item embeddings from both graph structure and item feature information using random walk and GCNs. This approach is well-suited for large-scale web recommender systems. The proposed model has been successfully deployed in Pinterest to address various real-world recommendation tasks.</p> <p></p>"},{"location":"writing/2024/04/28/RecSys-Overview/#recurrent-neural-networks-based-recommendations","title":"Recurrent Neural Networks based Recommendations","text":"<p>RNNs are perfect for processing sequential data, making them ideal for capturing the temporal dynamics of user interactions and behaviors, as well as handling sequential signals like text and audio.</p> <p></p>"},{"location":"writing/2024/04/28/RecSys-Overview/#session-based-recommendation-without-user-identifier","title":"Session-based Recommendation without User Identifier","text":"<p>In various real-world applications or websites, users may not always log in, depriving the system of access to their identifiers and long-term consumption habits or interests. However, mechanisms like sessions or cookies enable these systems to capture users' short-term preferences.</p> <p>GRU4Rec is a session-based recommendation model using 1-of-N encoding to represent active session items. It predicts the likelihood of the next item in the session. Efficient training is achieved with a session-parallel mini-batches algorithm and an output sampling method. The model employs a ranking loss called Top!, which focuses on the relative ranking of items.</p> \\[ L_s = \\frac{1}{S} \\sum_{j=1}^S \\sigma(\\hat{r_{si}} - \\hat{r_si}) + \\sigma(\\hat{r}^2_{sj})\\] <p>S is the sample size, \\(\\hat{r}_{sj}\\) and \\(\\hat{r}_{sj}\\) are the scores on negative item \\(i\\) and positive item \\(j\\) at session \\(s\\), \\(\\sigma\\) is the logistic sigmoid function. The last term is used as a regularization.</p> <p></p>"},{"location":"writing/2024/04/28/RecSys-Overview/#sequential-recommendation-with-user-identifier","title":"Sequential Recommendation with User Identifier","text":"<p>In contrast to session-based recommenders, which typically lack user identifiers, the following studies focus on sequential recommendation tasks where user identifications are known.</p> <p>Recurrent Recommender Network (RRN) is a non-parametric recommendation model built on RNNs. It models seasonal item evolution and user preference changes over time. It employs two LSTM networks to capture dynamic user \\((u_{ut})\\) and item \\((v_{it})\\) states. Alongside, it incorporates fixed properties like user long-term interests and item static features, integrating stationary latent attributes for users \\(\u200b(u_u)\\) and items \\((v_i)\\). The predicted rating of item \\(j\\) by user \\(i\\) at time \\(t\\) is defined as:</p> \\[ r_{ui|t} = f(u_{ut}, v_{it}, u_u, v_i)\\] <p>In this model, \\(u_{ut}\\)\u200b and \\(v_{it}\\)\u200b are learned through LSTM, while \\(u_u\\)\u200b and \\(v_i\\)\u200b are learned via standard matrix factorization. The optimization objective is to minimize the squared error between predicted and actual ratings.</p> <p></p>"},{"location":"writing/2024/04/28/RecSys-Overview/#feature-representation-learning-with-rnns","title":"Feature Representation Learning with RNNs","text":"<p>For side information with sequential patterns, RNNs are ideal for representation learning. User-item interactions impact changes in preferences and item status. RNNs can model these historical interactions, learning representations of influences like drift, evolution, and co-evolution of user and item features.</p> <p>Bansal et al. proposed a hybrid model employing GRUs to encode text sequences, addressing both warm-start and cold-start challenges in recommendation systems. They introduced a multi-task regularizer to counter overfitting and alleviate data sparsity. The primary task focuses on rating prediction, while an auxiliary task predicts item metadata such as tags and genres.[16]</p>"},{"location":"writing/2024/04/28/RecSys-Overview/#references","title":"References","text":"<ol> <li> <p>Recent Developments in Recommender Systems: A Survey</p> </li> <li> <p>The Netflix Recommender System: Algorithms, Business Value, and Innovation</p> </li> <li> <p>The Youtube Video Recommendation System. In Recsys</p> </li> <li> <p>Transform cold-start users into warm via fused behaviors in large-scale recommendation</p> </li> <li> <p>\"Causer: Causal session-based recommendations for handling popularity bias\"</p> </li> <li> <p>The Idiosyncratic Effects of Adversarial Training on Bias in Personalized Recommendation Learning</p> </li> <li> <p>FairGAN: GANs-based Fairness-aware Learning for Recommendations with Implicit Feedback</p> </li> <li> <p>Together is better: Hybrid recommendations combining graph embeddings and contextualized word representation</p> </li> <li> <p>Two tower model for retrieval</p> </li> <li> <p>Interpretable convolutional neural networks with dual local and global attention for review rating prediction</p> </li> <li> <p>Camo: A collaborative ranking method for content based recommendation</p> </li> <li> <p>Asymmetrical Context-aware Modulation for Collaborative Filtering Recommendation</p> </li> <li> <p>Joint Deep Modeling of Users and Items Using Reviews for Recommendation</p> </li> <li> <p>Hypercomplex Graph Collaborative Filtering</p> </li> <li> <p>Recurrent Recommender Networks</p> </li> <li> <p>Ask the gru: Multi-task learning for deep text recommendations</p> </li> </ol>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/","title":"From Reddit to Insights: Building an AI-Powered Data Pipeline with Gemini (Cloud)","text":""},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#introduction","title":"Introduction","text":"<p>Purpose</p> <p>In this blog post, I document the process of building an AI-driven, cloud data pipeline to automate this task. Using Google\u2019s Gemini AI, the pipeline collects, processes, and synthesizes discussions from AI-related subreddits into structured daily reports. The system is designed to filter out irrelevant or harmful content, ensuring the extracted insights are both meaningful and actionable.</p> <p>Check out the project GitHub repository for the full code and detailed documentation and Web Application.</p> <p>Problem Statement</p> <p>The field of artificial intelligence and machine learning evolves at an unprecedented pace, with new breakthroughs, trends, and discussions emerging daily. Platforms like Reddit host vibrant AI-focused communities that are rich with valuable insights. However, manually monitoring multiple subreddits to extract meaningful information is both time-consuming and inefficient.</p>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#what-youll-learn","title":"What You\u2019ll Learn","text":"<ul> <li>Leveraging Google Cloud Storage (GCS) for scalable data lake: How to use GCS for storing raw data, ML artifacts, and daily results.</li> <li>Optimizing data warehousing with BigQuery and dbt: Best practices for structuring, managing, and transforming data efficiently in BigQuery.</li> <li>Applying AI for sentiment analysis and summarization: Techniques to extract concise insights from unstructured Reddit discussions using BART and RoBERTa models.</li> <li>Utilizing Google\u2019s Gemini AI for content analysis: Insights into leveraging advanced AI models to categorize and interpret data.</li> <li>Orchestrating data workflows with Airflow on a single VM: How to manage complex data pipelines using Airflow in a cloud environment.</li> <li>Implementing cost-effective cloud solutions: Strategies for managing costs through scheduled VM start/stop, preemptible instances, and resource optimization.</li> </ul>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#system-architecture-modular-and-scalable-design","title":"System Architecture: Modular and Scalable Design","text":"<p>Our pipeline is designed with modularity and scalability in mind, comprising six main layers. Below is a high-level overview of how the components interact:</p> <p></p> <p>The diagram above illustrates the flow of data through our system, from collection to presentation. Each layer has specific responsibilities and communicates with adjacent layers through well-defined interfaces.</p>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#1-data-collection-layer","title":"1. Data Collection Layer","text":"<ul> <li>Reddit API Integration:<ul> <li>Fetch posts and comments from AI-focused subreddits</li> <li>Store raw data in GCS buckets</li> </ul> </li> <li>Text Preprocessing:<ul> <li>Clean and standardize text data</li> <li>Prepare for ML processing</li> </ul> </li> <li>BigQuery Integration: <ul> <li>Load data into staging tables</li> <li>Manage processing state</li> <li>Incremental processing</li> </ul> </li> </ul>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#2-storage-layer","title":"2. Storage Layer","text":"<ul> <li>Google Cloud Storage (GCS):<ul> <li>Raw data storage</li> <li>ML model artifacts</li> <li>Daily results</li> </ul> </li> <li>BigQuery:<ul> <li>Structured data storage</li> <li>Efficient query processing</li> <li>Cost-optimized table partitioning</li> </ul> </li> <li>DBT Transformations:<ul> <li>Stage posts and comments for further processing.</li> <li>Clean and standardize data structures.</li> <li>Manage processing state for each pipeline run.</li> <li>Automate cleanup upon pipeline completion.</li> </ul> </li> </ul>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#3-processing-layer","title":"3. Processing Layer","text":"<ul> <li>Text Summarization: BART for concise summaries</li> <li>Sentiment Analysis: RoBERTa model</li> <li>Insight Generation: Google's Gemini AI</li> <li>MLflow Integration: Track experiments and models</li> </ul>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#4-orchestration-layer","title":"4. Orchestration Layer","text":"<ul> <li>Single VM Deployment:<ul> <li>e2-standard-2 instance</li> <li>Docker container orchestration</li> <li>Automated start/stop</li> </ul> </li> <li>Airflow DAGs:<ul> <li>Main pipeline workflow</li> <li>GitHub sync automation</li> <li>Metric collection</li> </ul> </li> </ul>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#5-observability-layer","title":"5. Observability Layer","text":"<ul> <li>Cloud Monitoring:<ul> <li>VM health checks</li> <li>Cost monitoring</li> <li>Alert policies</li> </ul> </li> <li>Local Monitoring:<ul> <li>Prometheus metrics</li> <li>Grafana dashboards</li> <li>StatsD integration</li> </ul> </li> </ul>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#6-cost-management-layer","title":"6. Cost Management Layer","text":"<ul> <li>VM Lifecycle:<ul> <li>Scheduled start/stop</li> <li>Preemptible instances</li> <li>Resource optimization</li> </ul> </li> </ul> <p>This modular design ensures adaptability, maintainability, and scalability, enabling seamless interaction between components and the efficient transformation of Reddit data into actionable insights.</p>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#deep-dive-key-components","title":"Deep Dive: Key Components","text":""},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#resource-management","title":"Resource Management","text":"<p>The Reddit AI Pulse infrastructure on Google Cloud is a comprehensive data processing platform, designed with Terraform for scalability and security. At its heart, a custom network provides a secure foundation for a Debian VM running Airflow, which orchestrates the entire data pipeline. Data is efficiently managed using BigQuery for both raw feeds and analytics, along with a Cloud Storage bucket for logs and other artifacts. This automated, cost-effective setup ensures reliable analysis and processing of large-scale Reddit data.</p>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#resource-creation","title":"Resource Creation","text":"<p><code>build_res.sh</code> is a shell script that builds the resources using Terraform. <pre><code># =============================================================================\n# build_res.sh\n# -----------------------------------------------------------------------------\n# Main deployment script for the Reddit AI Pulse Cloud infrastructure.\n# This script sets up and configures all necessary GCP resources for the project.\n#\n# The script performs the following operations:\n# 1. Loads environment configuration\n# 2. Generates terraform.tfvars from environment variables\n# 3. Sets up Docker and Artifact Registry\n# 4. Builds and pushes Cloud Run images\n# 5. Initializes and applies Terraform configuration\n# 6. Updates Cloud Run URLs in .env file\n# 7. Manages service account credentials\n# 8. Uploads secrets to GitHub\n# 9. Configures Git repository settings\n#\n# The script includes error handling, logging, and automatic backup\n# of sensitive files. It can optionally commit changes to Git.\n#\n# Usage:\n#   ./build_res.sh\n#\n# Requirements:\n#   - .env file with required configuration\n#   - Authenticated gcloud CLI\n#   - Docker installed and configured\n#   - GitHub CLI installed and authenticated\n#   - Terraform installed\n# =============================================================================\n</code></pre> Once the resources are created, secrets are securely uploaded to Google Cloud Storage (GCS) using GitHub Actions via the <code>upload-secrets.yml</code> workflow.</p> <pre><code>name: Upload Secrets to GCS\n\non:\n  workflow_dispatch:  # Manual trigger\n  repository_dispatch:\n    types: [secrets_updated]  # Custom event type\n  push:\n    paths:\n      - '.github/workflows/upload-secrets.yml'\n\njobs:\n  upload-secrets:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n      id-token: write\n\n    steps:\n    - uses: actions/checkout@v4\n\n    - id: 'auth'\n      name: 'Authenticate to Google Cloud'\n      uses: 'google-github-actions/auth@v2'\n      with:\n        credentials_json: '${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}'\n\n    - name: 'Set up Cloud SDK'\n      uses: 'google-github-actions/setup-gcloud@v2'\n\n    - name: 'Create secrets directory'\n      run: mkdir -p secrets\n\n    - name: 'Create .env file'\n      run: |\n        cat &lt;&lt; EOF &gt; secrets/.env\n        # Docker Registry\n        DOCKER_REGISTRY=${{ secrets.DOCKER_REGISTRY }}\n\n        # GCP Project Configuration\n        GCP_PROJECT_ID=${{ secrets.GCP_PROJECT_ID }}\n        GCS_BUCKET_NAME=${{ secrets.GCS_BUCKET_NAME }}\n        GCP_REGION=${{ secrets.GCP_REGION }}\n        GCP_ZONE=${{ secrets.GCP_ZONE }}\n\n        # Service Account &amp; Authentication\n        GOOGLE_APPLICATION_CREDENTIALS=/opt/airflow/credentials/service-account.json\n        SA_EMAIL=${{ secrets.SA_EMAIL }}\n\n        # VM Configuration\n        VM_INSTANCE_NAME=${{ secrets.VM_INSTANCE_NAME }}\n        VM_MACHINE_TYPE=${{ secrets.VM_MACHINE_TYPE }}\n\n        # Network Configuration\n        NETWORK_NAME=${{ secrets.NETWORK_NAME }}\n        SUBNET_NAME=${{ secrets.SUBNET_NAME }}\n        SUBNET_CIDR=${{ secrets.SUBNET_CIDR }}\n\n        # Email Configuration\n        ALERT_EMAIL=${{ secrets.ALERT_EMAIL }}\n        ALERT_EMAIL_PASSWORD=${{ secrets.ALERT_EMAIL_PASSWORD }}\n\n        # BigQuery Configuration\n        BIGQUERY_DATASET_RAW=${{ secrets.BIGQUERY_DATASET_RAW }}\n        BIGQUERY_DATASET_PROCESSED=${{ secrets.BIGQUERY_DATASET_PROCESSED }}\n\n        # Reddit API Configuration\n        REDDIT_CLIENT_ID=${{ secrets.REDDIT_CLIENT_ID }}\n        REDDIT_CLIENT_SECRET=${{ secrets.REDDIT_CLIENT_SECRET }}\n        REDDIT_USERNAME=${{ secrets.REDDIT_USERNAME }}\n        REDDIT_PASSWORD=${{ secrets.REDDIT_PASSWORD }}\n        REDDIT_USER_AGENT=${{ secrets.REDDIT_USER_AGENT }}\n\n        # Gemini API Key\n        GOOGLE_GEMINI_API_KEY=${{ secrets.GOOGLE_GEMINI_API_KEY }}\n\n        # GitHub Configuration\n        GH_PAT=${{ secrets.GH_PAT }}\n        GH_OWNER=${{ secrets.GH_OWNER }}\n        GH_REPO=${{ secrets.GH_REPO }}\n        GH_WEBSITE_REPO=${{ secrets.GH_WEBSITE_REPO }}\n        AUTO_COMMIT=${{ secrets.AUTO_COMMIT }}\n\n        # Grafana credentials\n        GF_SECURITY_ADMIN_USER=${{ secrets.GF_SECURITY_ADMIN_USER }}\n        GF_SECURITY_ADMIN_PASSWORD=${{ secrets.GF_SECURITY_ADMIN_PASSWORD }}\n\n        # Airflow PostgreSQL credentials\n        AIRFLOW_DB_USER=${{ secrets.AIRFLOW_DB_USER }}\n        AIRFLOW_DB_PASSWORD=${{ secrets.AIRFLOW_DB_PASSWORD }}\n        AIRFLOW_DB_NAME=${{ secrets.AIRFLOW_DB_NAME }}\n\n        # VM Function URLs\n        STOP_VM_FUNCTION_URL=${{ secrets.STOP_VM_FUNCTION_URL }}\n        START_VM_FUNCTION_URL=${{ secrets.START_VM_FUNCTION_URL }}\n\n        # Airflow configuration\n        AIRFLOW_UID=50000\n        AIRFLOW_GID=0\n        EOF\n\n    - name: 'Create service account key file'\n      run: |\n        echo '${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}' &gt; secrets/service-account.json\n\n    - name: 'Upload secrets to GCS'\n      run: |\n        gsutil cp secrets/.env gs://${{ secrets.GCS_BUCKET_NAME }}/secrets/.env\n        gsutil cp secrets/service-account.json gs://${{ secrets.GCS_BUCKET_NAME }}/secrets/service-account.json \n</code></pre>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#resource-deletion","title":"Resource Deletion","text":"<p><code>cleanup.sh</code> is a shell script that deletes the resources using Terraform. <pre><code># =============================================================================\n# cleanup.sh\n# -----------------------------------------------------------------------------\n# A comprehensive cleanup script for the Reddit AI Pulse Cloud infrastructure.\n# This script systematically removes all GCP resources created by the project.\n#\n# The script performs cleanup in the following order:\n# 1. IAM (roles and bindings)\n# 2. Cloud Scheduler jobs\n# 3. Cloud Functions\n# 4. GCS buckets and contents\n# 5. BigQuery datasets and tables\n# 6. Artifact Registry repositories\n# 7. Cloud Run services\n# 8. Service accounts\n# 9. Compute Engine resources\n# 10. VPC, subnets, and firewall rules\n# 11. Monitoring resources (alerts, dashboards)\n# 12. Terraform state\n# 13. Log buckets\n#\n# The script includes error handling, retries for dependent resources,\n# and creates detailed logs of the cleanup process in the resource_info directory.\n#\n# Usage:\n#   ./cleanup.sh\n#\n# Requirements:\n#   - GCP project configuration in .env file\n#   - Authenticated gcloud CLI\n#   - Required permissions to delete resources\n# =============================================================================\n</code></pre></p>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#reddit-data-collection-and-preprocessing","title":"Reddit Data Collection and Preprocessing","text":"<p>The foundation of our pipeline is reliable data collection and preprocessing. We utilize Python's Reddit API wrapper (PRAW) to fetch posts and comments from specified subreddits, with immediate text preprocessing for clean data storage.</p>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#1-data-collection-with-praw","title":"1. Data Collection with PRAW","text":"<p>The Reddit client implementation focuses on efficient and reliable data collection. Here's how we handle both posts and their comments:</p> <pre><code>def fetch_and_save_posts(reddit, subreddit_name, db_utils, conn):\n    \"\"\"Fetches new posts from a subreddit since the last processed timestamp.\"\"\"\n    last_processed_utc = get_last_processed_timestamp(conn, subreddit_name)\n    current_max_utc = 0\n\n    subreddit = reddit.subreddit(subreddit_name)\n    for submission in subreddit.hot(limit=20):\n        # Skip already processed posts\n        if submission.created_utc &lt;= last_processed_utc:\n            continue\n\n        # Process post data\n        post_data = {\n            \"subreddit\": subreddit_name,\n            \"post_id\": submission.id,\n            \"title\": str(submission.title),\n            \"author\": str(submission.author),\n            \"url\": str(submission.url),\n            \"score\": int(submission.score),\n            \"created_utc\": submission.created_utc,\n            \"comments\": []\n        }\n\n        # Handle comments if they exist\n        if submission.num_comments &gt; 0:\n            try:\n                submission.comments.replace_more(limit=0)  # Expand comment tree\n                post_data[\"comments\"] = [\n                    {\n                        \"comment_id\": comment.id,\n                        \"author\": str(comment.author),\n                        \"body\": str(comment.body),\n                        \"created_utc\": comment.created_utc,\n                    }\n                    for comment in submission.comments.list()[:10]  # Top 10 comments\n                    if comment.created_utc &gt; last_processed_utc\n                ]\n            except Exception as e:\n                logging.error(f\"Error fetching comments for post {submission.id}: {e}\")\n\n        # Save to database\n        db_utils.insert_raw_post_data(conn, post_data)\n        current_max_utc = max(current_max_utc, submission.created_utc)\n</code></pre> <p>The raw data is stored in GCS as a JSON file.</p> <pre><code>def save_to_gcs(storage_client: storage.Client, bucket_name: str, \n                subreddit_name: str, posts: List[Dict]) -&gt; None:\n    \"\"\"Saves posts data to Google Cloud Storage.\"\"\"\n    try:\n        # Get bucket\n        bucket = storage_client.bucket(bucket_name)\n\n        # Generate blob name with current timestamp\n        batch_timestamp = datetime.utcnow()\n        blob_name = get_gcs_blob_name(subreddit_name, batch_timestamp)\n        blob = bucket.blob(blob_name)\n\n        # Convert to newline-delimited JSON\n        ndjson_data = '\\n'.join(json.dumps(post) for post in posts)\n        blob.upload_from_string(ndjson_data)\n\n        logger.info(f\"Saved {len(posts)} posts to GCS: {blob_name}\")\n    except Exception as e:\n        logger.error(f\"Error saving to GCS: {e}\")\n        raise\n</code></pre> <p>Our system is designed for efficient and reliable data processing. We use incremental data collection, fetching only new data based on timestamps. To manage the volume of Reddit data, we focus on the top 10 comments per post, allowing us to process approximately 1,000 comments daily in about 50 minutes. This can be easily scaled by adding more compute resources. We store comments in JSONB format, which provides flexibility for handling semi-structured data.</p> <p>We've implemented robust error handling with retry mechanisms and transaction management to ensure data consistency. Batch processing is used to improve scalability and efficiency when dealing with large datasets. While stream processing could further enhance scalability, we opted for batch processing for this use case.</p> <p>JSON was chosen for its ability to handle semi-structured data like Reddit comments, which often vary in format and content. By storing comments as JSON, the system accommodates diverse data structures without rigid schemas, while still allowing efficient querying and indexing for analytics.</p>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#2-data-preprocessing-with-pyspark","title":"2. Data Preprocessing with PySpark","text":"<p>To ensure the quality of our analysis, we implement a series of preprocessing steps for Reddit posts using PySpark. The following code snippet demonstrates how we filter and clean post titles:</p>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#posts-processing","title":"Posts Processing","text":"<p>The following code snippet demonstrates how we parse, filter, and clean posts: <pre><code>def preprocess_posts(df):\n    \"\"\"Preprocesses Reddit posts with content filtering and cleaning.\"\"\"\n    # Content filtering patterns\n    profanity_pattern = r'(?i)\\b(wordword2|word3|word4|word5)\\b|' + \\\n                       r'(?i)\\b(w\\*\\*d1|w\\*rd2|w\\*rd3|w\\*\\*d4)\\b|' + \\\n                       r'(?i)w[^\\w]?r[^\\w]?d1|' + \\\n                       r'(?i)w[^\\w]?r[^\\w]?d2'\n\n    # Filter and clean posts\n    return df.filter(\n        # Basic validation\n        (F.col(\"title\").isNotNull()) &amp; \n        (F.col(\"title\") != \"\") &amp; \n        (F.col(\"title\") != \"[deleted]\") &amp;\n        (F.col(\"author\").isNotNull()) &amp; \n        (F.col(\"author\") != \"[deleted]\") &amp;\n        # Content filtering\n        ~F.col(\"title\").rlike(profanity_pattern)\n    ).withColumn(\n        # Clean and normalize text\n        \"title\", F.regexp_replace(\"title\", r'[\\n\\r\\t]', ' ')\n    ).withColumn(\n        # Remove multiple spaces\n        \"title\", F.regexp_replace(\"title\", r'\\s+', ' ')\n    ).dropDuplicates([\"post_id\"])\n</code></pre> Our post preprocessing involves several key steps: we filter out offensive content using regular expressions, validate the <code>title</code> and <code>author</code> fields, clean and normalize the text, and remove duplicate posts. This ensures our analysis is based on clean and relevant data.</p>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#comments-processing","title":"Comments Processing","text":"<p>Similar to posts, we also preprocess Reddit comments to ensure data quality. The following code snippet demonstrates how we parse, filter, and clean comments stored in JSON format:</p> <p><pre><code>def preprocess_comments(df):\n    \"\"\"Preprocesses Reddit comments with content filtering and cleaning.\"\"\"\n    # Parse and explode comments from JSONB\n    comments_schema = ArrayType(StructType([\n        StructField(\"body\", StringType(), True),\n        StructField(\"author\", StringType(), True),\n        StructField(\"comment_id\", StringType(), True),\n        StructField(\"created_utc\", DoubleType(), True)\n    ]))\n\n    return df.withColumn(\n        \"comments_parsed\",\n        F.from_json(F.col(\"comments\"), comments_schema)\n    ).withColumn(\n        \"comment\", F.explode(\"comments_parsed\")\n    ).select(\n        \"post_id\",\n        F.col(\"comment.body\").alias(\"body\"),\n        F.col(\"comment.author\").alias(\"author\"),\n        F.col(\"comment.comment_id\").alias(\"comment_id\"),\n        F.to_timestamp(F.col(\"comment.created_utc\")).alias(\"created_utc\")\n    ).filter(\n        # Remove deleted/empty comments\n        (F.col(\"body\").isNotNull()) &amp;\n        (F.col(\"body\") != \"\") &amp;\n        (F.col(\"body\") != \"[deleted]\") &amp;\n        (F.col(\"author\") != \"[deleted]\")\n    ).withColumn(\n        # Clean and normalize text\n        \"body\", F.regexp_replace(\"body\", r'[\\n\\r\\t]', ' ')\n    ).withColumn(\n        # Remove multiple spaces\n        \"body\", F.regexp_replace(\"body\", r'\\s+', ' ')\n    ).dropDuplicates([\"comment_id\"])\n</code></pre> Our comment preprocessing involves defining a schema for the JSON data, parsing and exploding the comments, selecting and aliasing relevant fields (including converting the timestamp), filtering out invalid comments, cleaning and normalizing the text, and removing duplicates.</p>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#processed-data-schema","title":"Processed Data Schema","text":"<p>The cleaned data is stored in separate tables for posts and comments:</p> <pre><code># Dynamic processed posts tables for each subreddit\nresource \"google_bigquery_table\" \"processed_posts_tables\" {\n  for_each = toset(var.subreddits)\n\n  dataset_id = google_bigquery_dataset.processed_data.dataset_id\n  table_id   = \"posts_${lower(each.value)}\"\n  project    = var.project\n\n  deletion_protection = false\n\n  schema = jsonencode([\n    {\n      name = \"post_id\",\n      type = \"STRING\",\n      mode = \"REQUIRED\"\n    },\n    {\n      name = \"subreddit\",\n      type = \"STRING\",\n      mode = \"REQUIRED\"\n    },\n    {\n      name = \"title\",\n      type = \"STRING\",\n      mode = \"NULLABLE\"\n    },\n    {\n      name = \"author\",\n      type = \"STRING\",\n      mode = \"NULLABLE\"\n    },\n    {\n      name = \"url\",\n      type = \"STRING\",\n      mode = \"NULLABLE\"\n    },\n    {\n      name = \"score\",\n      type = \"INTEGER\",\n      mode = \"NULLABLE\"\n    },\n    {\n      name = \"created_utc\",\n      type = \"TIMESTAMP\",\n      mode = \"NULLABLE\"\n    },\n    {\n      name = \"ingestion_timestamp\",\n      type = \"TIMESTAMP\",\n      mode = \"NULLABLE\"\n    }\n  ])\n}\n\n# Dynamic processed comments tables for each subreddit\nresource \"google_bigquery_table\" \"processed_comments_tables\" {\n  for_each = toset(var.subreddits)\n\n  dataset_id = google_bigquery_dataset.processed_data.dataset_id\n  table_id   = \"comments_${lower(each.value)}\"\n  project    = var.project\n\n  deletion_protection = false\n\n  schema = jsonencode([\n    {\n      name = \"post_id\",\n      type = \"STRING\",\n      mode = \"REQUIRED\"\n    },\n    {\n      name = \"comment_id\",\n      type = \"STRING\",\n      mode = \"REQUIRED\"\n    },\n    {\n      name = \"author\",\n      type = \"STRING\",\n      mode = \"NULLABLE\"\n    },\n    {\n      name = \"body\",\n      type = \"STRING\",\n      mode = \"NULLABLE\"\n    },\n    {\n      name = \"created_utc\",\n      type = \"TIMESTAMP\",\n      mode = \"NULLABLE\"\n    }\n  ])\n}\n</code></pre>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#daily-summary-processing","title":"Daily Summary Processing","text":"<p>After preprocessing, we generate daily summaries of the Reddit data using PySpark. The following code snippet demonstrates how we load, filter, and join posts and comments to create these summaries:</p> <p><pre><code>def generate_daily_summaries(spark: SparkSession, project_id: str, dataset_id: str, subreddits: list, bucket_name: str):\n    \"\"\"\n    Generates daily summaries of the processed data using PySpark.\n    Implements incremental loading by only processing data since the last processed timestamp.\n    \"\"\"\n    current_timestamp = datetime.now()\n    last_processed = get_last_processed_timestamp(spark, project_id, dataset_id)\n    total_summaries_added = 0\n\n    if last_processed:\n        logger.info(f\"Processing summaries from {last_processed} to {current_timestamp}\")\n    else:\n        logger.info(\"No previous summaries found. Processing all available data.\")\n\n    for subreddit in subreddits:\n        logger.info(f\"Processing daily summary for subreddit: {subreddit}\")\n\n        # Load posts and comments tables from BigQuery\n        posts_df = spark.read.format('bigquery') \\\n            .option('table', f\"{project_id}.{dataset_id}.posts_{subreddit.lower()}\") \\\n            .load()\n\n        posts_count = posts_df.count()\n        logger.info(f\"Found {posts_count} posts for {subreddit}\")\n\n        comments_df = spark.read.format('bigquery') \\\n            .option('table', f\"{project_id}.{dataset_id}.comments_{subreddit.lower()}\") \\\n            .load()\n\n        comments_count = comments_df.count()\n        logger.info(f\"Found {comments_count} comments for {subreddit}\")\n\n        # Filter posts and comments based on created_utc timestamp\n        if last_processed:\n            daily_posts_df = posts_df.filter(\n                (F.col(\"created_utc\") &gt; last_processed) &amp;\n                (F.col(\"created_utc\") &lt;= current_timestamp)\n            )\n\n            daily_comments_df = comments_df.filter(\n                (F.col(\"created_utc\") &gt; last_processed) &amp;\n                (F.col(\"created_utc\") &lt;= current_timestamp)\n            )\n        else:\n            daily_posts_df = posts_df.filter(F.col(\"created_utc\") &lt;= current_timestamp)\n            daily_comments_df = comments_df.filter(F.col(\"created_utc\") &lt;= current_timestamp)\n\n        filtered_posts_count = daily_posts_df.count()\n        logger.info(f\"Filtered to {filtered_posts_count} posts for processing\")\n\n        if filtered_posts_count == 0:\n            logger.info(f\"No new posts to summarize for {subreddit}\")\n            continue\n\n        # Join posts and comments on post_id\n        joined_df = daily_posts_df.alias(\"posts\").join(\n            daily_comments_df.alias(\"comments\"), \n            \"post_id\", \n            \"right\"\n        )\n\n        # Prepare the daily summary\n        daily_summary_df = joined_df.select(\n            F.monotonically_increasing_id().alias(\"id\"),\n            F.col(\"subreddit\"),\n            F.col(\"posts.post_id\").alias(\"post_id\"),\n            F.col(\"posts.score\").alias(\"post_score\"),\n            F.col(\"posts.url\").alias(\"post_url\"),\n            F.col(\"comments.comment_id\").alias(\"comment_id\"),\n            F.to_date(F.col(\"comments.created_utc\")).alias(\"summary_date\"),\n            F.current_timestamp().alias(\"processed_date\"),\n            F.lit(True).alias(\"needs_processing\"),\n            F.col(\"posts.title\").alias(\"post_content\"),\n            F.col(\"comments.body\").alias(\"comment_body\")\n        )\n\n        # Filter out rows where required fields are null\n        daily_summary_df = daily_summary_df.filter(\n            (F.col(\"comment_body\").isNotNull()) &amp; \n            (F.col(\"comment_id\").isNotNull()) &amp;\n            (F.col(\"post_id\").isNotNull())\n        )\n\n        filtered_summaries_count = daily_summary_df.count()\n        logger.info(f\"Generated {filtered_summaries_count} summaries before deduplication\")\n\n        # Deduplication Logic\n        existing_summary_df = spark.read.format('bigquery') \\\n            .option('table', f\"{project_id}.{dataset_id}.daily_summary_data\") \\\n            .load()\n\n        # Perform a left anti-join to get only new records\n        new_daily_summary_df = daily_summary_df.join(\n            existing_summary_df,\n            [\"post_id\", \"comment_id\"],  # Using composite key for deduplication\n            \"left_anti\"\n        )\n\n        new_summaries_count = new_daily_summary_df.count()\n\n        if new_summaries_count &gt; 0:\n            # Write the new daily summaries to BigQuery\n            new_daily_summary_df.write.format('bigquery') \\\n                .option('table', f\"{project_id}.{dataset_id}.daily_summary_data\") \\\n                .option('temporaryGcsBucket', bucket_name) \\\n                .mode('append') \\\n                .save()\n\n            total_summaries_added += new_summaries_count\n            logger.info(f\"Successfully added {new_summaries_count} new summaries for {subreddit}\")\n        else:\n            logger.info(f\"No new unique summaries to add for {subreddit}\")\n</code></pre> To create daily summaries, we load new posts and comments for each subreddit. We then join these datasets on post_id, select the necessary fields, and filter out any rows with null values. This function combines post and comment data, preparing it for further analysis.</p> <p>The daily summary data is stored with the following schema:</p> <pre><code>resource \"google_bigquery_table\" \"daily_summary\" {\n  dataset_id = google_bigquery_dataset.processed_data.dataset_id\n  table_id   = \"daily_summary_data\"\n  project    = var.project\n\n  deletion_protection = false\n\n  schema = jsonencode([\n    {\n      name = \"id\",\n      type = \"INTEGER\",\n      mode = \"REQUIRED\"\n    },\n    {\n      name = \"subreddit\",\n      type = \"STRING\",\n      mode = \"REQUIRED\"\n    },\n    {\n      name = \"post_id\",\n      type = \"STRING\",\n      mode = \"REQUIRED\"\n    },\n    {\n      name = \"post_score\",\n      type = \"INTEGER\",\n      mode = \"NULLABLE\"\n    },\n    {\n      name = \"post_url\",\n      type = \"STRING\",\n      mode = \"NULLABLE\"\n    },\n    {\n      name = \"comment_id\",\n      type = \"STRING\",\n      mode = \"NULLABLE\"\n    },\n    {\n      name = \"summary_date\",\n      type = \"DATE\",\n      mode = \"REQUIRED\"\n    },\n    {\n      name = \"processed_date\",\n      type = \"TIMESTAMP\",\n      mode = \"REQUIRED\"\n    },\n    {\n      name = \"needs_processing\",\n      type = \"BOOLEAN\",\n      mode = \"REQUIRED\"\n    },\n    {\n      name = \"post_content\",\n      type = \"STRING\",\n      mode = \"NULLABLE\"\n    },\n    {\n      name = \"comment_body\",\n      type = \"STRING\",\n      mode = \"NULLABLE\"\n    }\n  ])\n}\n</code></pre>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#dbt","title":"dbt","text":""},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#dbt-data-transformations","title":"DBT Data Transformations","text":"<p>Our data transformations are managed using dbt, which allows us to structure our project with specific models for each stage of processing. This approach provides a clear, modular, and maintainable data pipeline.</p> <p><pre><code># dbt_project.yml configuration\nname: 'dbt_reddit_summary_cloud'\nversion: '1.0.0'\n\nmodels:\n  dbt_reddit_summary_local:\n    +materialized: table  # Default materialization\n    current_summary_staging:\n      +materialized: view  # Staging view\n    joined_summary_analysis:\n      +materialized: table  # Final analysis\n    update_processing_status:\n      +materialized: incremental\n\nvars:\n  subreddits: ['dataengineering', 'datascience', 'machinelearning', 'claudeai',\n   'singularity', 'localllama', 'openai', 'stablediffusion']\n</code></pre> This dbt project configuration includes a default table materialization, a staging view for intermediate data, a final analysis table, and an incremental model for updating processing status. Additionally, we define a list of subreddits that are processed by our pipeline.</p> <p>The DBT workflow includes three main transformation models:</p> <ol> <li> <p>Staging Model (<code>current_summary_staging</code>): <pre><code>{{\n    config(\n        materialized='view'\n    )\n}}\n\nSELECT\n    id,\n    subreddit,\n    post_id,\n    post_score,\n    post_url,\n    comment_id,\n    summary_date,\n    processed_date,\n    post_content,\n    comment_body\nFROM {{ source('summary_analytics', 'daily_summary_data') }} ds\nWHERE comment_body IS NOT NULL\n    AND needs_processing = TRUE\n</code></pre> The current_summary_staging model is a view that selects key fields from the daily summary data, filtering out rows with null comment bodies and those that do not need processing. This view is materialized as a view for real-time filtering.</p> </li> <li> <p>Analysis Model (<code>joined_summary_analysis</code>): <pre><code>{{ config(\n    materialized='table',\n    unique_key='comment_id',\n) }}\n\nWITH validation_cte AS (\n    SELECT \n        cs.*,\n        ts.comment_summary,\n        sa.sentiment_score,\n        sa.sentiment_label,\n        CASE \n            WHEN LENGTH(cs.comment_body) &lt; 2 THEN 'Too short'\n            WHEN LENGTH(cs.comment_body) &gt; 10000 THEN 'Too long'\n            ELSE 'Valid'\n        END as comment_quality,\n        CASE \n            WHEN ts.comment_summary IS NULL OR TRIM(ts.comment_summary) = '' THEN 'Missing'\n            WHEN LENGTH(ts.comment_summary) &gt; LENGTH(cs.comment_body) THEN 'Invalid'\n            ELSE 'Valid'\n        END as summary_quality\n    FROM {{ source('summary_analytics', 'current_summary_staging') }} cs\n    LEFT JOIN {{ source('summary_analytics', 'text_summary_results') }} ts \n        ON cs.comment_id = ts.comment_id\n    LEFT JOIN {{ source('summary_analytics', 'sentiment_analysis_results') }} sa \n        ON cs.comment_id = sa.comment_id\n)\n\nSELECT \n    *,\n    CURRENT_TIMESTAMP as processed_at,\n    'dbt' as processed_by\nFROM validation_cte\nWHERE comment_quality = 'Valid' \n  AND summary_quality = 'Valid'\n</code></pre> The joined_summary_analysis model joins the staging data with text summary and sentiment analysis results. It then validates the quality of comments and summaries, selecting only valid rows for further analysis. This model also adds processing metadata.</p> </li> <li> <p>Processing Status Model (<code>update_processing_status</code>): <pre><code>{{ config(\n    materialized='incremental',\n    unique_key=['comment_id', 'post_id'],\n    post_hook=[\n        \"\"\"\n        UPDATE {{ source('summary_analytics', 'daily_summary_data') }}\n        SET needs_processing = FALSE\n        WHERE comment_id IN (\n            SELECT comment_id \n            FROM {{ this }}\n        );\n        \"\"\"\n    ]\n) }}\n\n-- Select all records that were attempted to be processed, including failed validations\nWITH validation_cte AS (\n    SELECT \n        comment_id,\n        comment_body,\n        post_id\n    FROM {{ source('summary_analytics', 'current_summary_staging') }}\n),\nall_processed_records AS (\n    SELECT \n        cs.comment_id,\n        cs.post_id,\n        v.comment_quality\n    FROM {{ source('summary_analytics', 'current_summary_staging') }} cs\n    LEFT JOIN (\n        SELECT \n            comment_id,\n            CASE \n                WHEN LENGTH(comment_body) &lt; 2 THEN 'Too short'\n                WHEN LENGTH(comment_body) &gt; 10000 THEN 'Too long'\n                ELSE 'Valid'\n            END as comment_quality\n        FROM validation_cte\n    ) v ON cs.comment_id = v.comment_id\n)\n\nSELECT \n    comment_id,\n    post_id\nFROM all_processed_records\n</code></pre> The update_processing_status model is an incremental model that updates the processing status of comments. It selects all records that were attempted to be processed, including those that failed validation, and uses a post-hook to mark them as processed in the source table. This ensures that comments are not reprocessed.</p> </li> </ol>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#dbt-data-quality-tests","title":"DBT Data Quality Tests","text":"<p>Our project implements comprehensive data quality tests using DBT's testing framework. These tests are defined in our <code>schema.yml</code>:</p> <pre><code>version: 2\n\nsources:\n  - name: raw_data\n    schema: raw_data\n    tables:\n      - name: raw_dataengineering\n        columns: &amp;raw_columns\n          - name: post_id\n            tests:\n              - not_null\n              - unique\n              - is_text\n          - name: subreddit\n            tests:\n              - not_null\n              - is_text\n              - accepted_values:\n                  values: [\"dataengineering\", \"datascience\", \"machinelearning\", \n                          \"claudeai\", \"singularity\", \"localllama\", \"openai\", \n                          \"stablediffusion\"]\n          - name: score\n            tests:\n              - not_null\n              - is_int\n              - dbt_utils.expression_is_true:\n                  expression: \"{{ column_name }} &gt;= 0\"\n          - name: comments\n            tests:\n              - not_null\n              - is_json\n</code></pre> <p>Our testing strategy includes generic tests for data presence and type validation, custom tests for specific data structures and rules, relationship tests for data integrity, and data quality metrics to monitor data health. These tests are integrated into our CI/CD pipeline for early detection of issues.</p>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#text-processing-pipeline","title":"Text Processing Pipeline","text":"<p>Our cleaned data is analyzed using transformer models for text summarization and sentiment analysis, with MLflow for experiment tracking.</p>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#1-text-summarization","title":"1. Text Summarization","text":"<p>We use a fine-tuned BART model for generating concise summaries of Reddit comments:</p> <pre><code>def create_summarizer():\n    \"\"\"Initialize the summarization model.\"\"\"\n    model_name = \"philschmid/bart-large-cnn-samsum\"\n    local_model_path = \"/models/models--philschmid--bart-large-cnn-samsum/snapshots/e49b3d60d923f12db22bdd363356f1a4c68532ad\"\n\n    tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n    model = AutoModelForSeq2SeqLM.from_pretrained(\n        local_model_path,\n        torch_dtype=torch.float32,\n        device_map=\"auto\",\n        low_cpu_mem_usage=True\n    )\n\n    return pipeline(\n        \"summarization\",\n        model=model,\n        tokenizer=tokenizer,\n        framework=\"pt\"\n    )\n\ndef generate_summary(comment_body, summarizer):\n    \"\"\"Generate summary for a given comment.\"\"\"\n    if len(comment_body.split()) &lt;= 59:\n        return comment_body \n\n    try:\n        summary = summarizer(\n            comment_body, \n            max_length=60, \n            min_length=15, \n            do_sample=False, \n            truncation=True\n        )\n        if summary and isinstance(summary, list) and len(summary) &gt; 0:\n            return summary[0]['summary_text']\n    except Exception as e:\n        logging.error(f\"Summarization failed: {e}\")\n\n    return \"\"\n</code></pre> <p>We use a fine-tuned BART-large model for dialogue summarization, which outperforms the standard model on Reddit comments. It's optimized for local machine use with efficient memory management.</p>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#2-sentiment-analysis","title":"2. Sentiment Analysis","text":"<p>We employ a RoBERTa-based model fine-tuned for emotion detection:</p> <pre><code>def initialize_emotion_analyzer():\n    \"\"\"Initialize the emotion analysis model.\"\"\"\n    model_name = \"SamLowe/roberta-base-go_emotions\"\n    local_model_path = \"/models/models--SamLowe--roberta-base-go_emotions/snapshots/58b6c5b44a7a12093f782442969019c7e2982299\"\n\n    model = AutoModelForSequenceClassification.from_pretrained(local_model_path)\n    tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n\n    return pipeline(\n        \"text-classification\",\n        model=model,\n        tokenizer=tokenizer\n    )\n\ndef perform_sentiment_analysis(text, emotion_analyzer):\n    \"\"\"Perform sentiment analysis on the given text.\"\"\"\n    try:\n        if not text or text.strip() == \"\":\n            return \"Neutral\", 0.0, \"No content\"\n\n        truncated_text = truncate_text(text, max_length=500)\n        emotion_output = emotion_analyzer(truncated_text)\n\n        if not emotion_output:\n            return \"Neutral\", 0.0, \"Analysis failed\"\n\n        emotion_label = emotion_output[0]['label']\n        emotion_score = emotion_output[0]['score']\n\n        # Map emotions to sentiment categories\n        sentiment_mapping = {\n            'joy': 'Positive',\n            'love': 'Positive',\n            'admiration': 'Positive',\n            'approval': 'Positive',\n            'excitement': 'Positive',\n            'gratitude': 'Positive',\n            'optimism': 'Positive',\n            'anger': 'Negative',\n            'disappointment': 'Negative',\n            'disgust': 'Negative',\n            'fear': 'Negative',\n            'sadness': 'Negative',\n            'confusion': 'Neutral',\n            'curiosity': 'Neutral',\n            'surprise': 'Neutral'\n        }\n\n        sentiment_label = sentiment_mapping.get(emotion_label.lower(), 'Neutral')\n        return sentiment_label, float(emotion_score), emotion_label\n\n    except Exception as e:\n        logging.error(f\"Error during sentiment analysis: {str(e)}\")\n        return \"Neutral\", 0.0, \"Error in analysis\"\n</code></pre> <p>Our sentiment analysis component uses a RoBERTa model fine-tuned for emotion detection, which we found to be more effective than standard sentiment analysis for Reddit comments. Key features include smart text truncation, mapping to sentiment categories (Positive, Negative, or Neutral), and comprehensive error handling.</p>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#3-pipeline-integration","title":"3. Pipeline Integration","text":"<p>Both components are integrated into the data pipeline with MLflow, which we use for experiment tracking, allowing us to monitor model performance and manage different model versions. We also use MLflow for model serving, enabling us to deploy our models.</p> <pre><code># MLflow configuration\nmlflow.set_tracking_uri(\"http://mlflow:5000\")\nmlflow.set_experiment(\"reddit_sentiment_analysis_experiments\")\n\nwith mlflow.start_run() as run:\n    # Log parameters\n    mlflow.log_param(\"model_name\", \"SamLowe/roberta-base-go_emotions\")\n</code></pre>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#gemini-ai-analysis","title":"Gemini AI Analysis","text":"<p>We use Google's Gemini AI for advanced analysis, generating structured insights from Reddit discussions with a focus on content safety and consistent formatting.</p>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#1-analysis-configuration","title":"1. Analysis Configuration","text":"<p>The Gemini analysis is guided by a prompt template that instructs the model to filter content for safety, rank threads by score, summarize content, analyze emotions, extract top viewpoints, and embed links. The output is structured with a title, date, description, tags, overall ranking, and detailed thread analysis.</p> <p><pre><code>def create_prompt_template():\n    \"\"\"Return the standard prompt template for Gemini analysis.\"\"\"\n    current_date = datetime.now().strftime('%Y-%m-%d')\n\n    return f\"\"\"\n    Analyze the provided text files, which contain Reddit posts and comments.\n    **Instructions:**\n    1.  **Content Filtering:** Check for harassment, hate speech, or explicit material\n    2.  **Ranking:** Rank threads by total \"Score\"\n    3.  **Summarization:** Utilize \"Summary\" fields for thread overviews\n    4.  **Emotional Analysis:** Analyze \"Emotion Label\" and \"Emotion Score\"\n    5.  **Point of View Extraction:** Extract top 3 viewpoints per thread\n    6.  **Links:** Embed URLs in thread titles using Markdown\n\n    **Output Format:**\n    ---\n    title: \"{{subreddit_name}} subreddit\"\n    date: \"{current_date}\"\n    description: \"Analysis of top discussions and trends\"\n    tags: [\"tag1\", \"tag2\", \"tag3\"]\n    ---\n\n    # Overall Ranking and Top Discussions\n    [Ranked list of threads with scores and summaries]\n\n    # Detailed Analysis by Thread \n    [Thread-by-thread analysis with summaries, emotions, and viewpoints]\n    \"\"\"\n</code></pre> The Gemini analysis is guided by a prompt template that instructs the model to filter content for safety, rank threads by score, summarize content, analyze emotions, extract top viewpoints, and embed links. The output is structured with a title, date, description, tags, overall ranking, and detailed thread analysis.</p>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#2-data-processing","title":"2. Data Processing","text":"<p>The system processes data subreddit by subreddit, formatting each post and its comments:</p> <pre><code>def process_subreddit(model, cur, subreddit, output_dir):\n    \"\"\"Process a single subreddit's data.\"\"\"\n    # Fetch processed data\n    cur.execute(\"\"\"\n        SELECT post_id, subreddit, post_score, post_url, comment_id,\n               summary_date, post_content, comment_body, comment_summary,\n               sentiment_score, sentiment_label\n        FROM processed_data.joined_summary_analysis\n        WHERE subreddit = %s\n        ORDER BY post_score DESC\n    \"\"\", (subreddit,))\n\n    # Format data for analysis\n    text_files = \"\".join(\n        format_text_file(row) for row in cur.fetchall()\n    )\n\n    # Generate insights using Gemini\n    response = model.generate_content(final_prompt + text_files)\n\n    # Save formatted output\n    output_file_path = os.path.join(output_dir, f\"llm_{subreddit}.md\")\n    with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(response.text)\n</code></pre>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#3-output-formatting","title":"3. Output Formatting","text":"<p>The system ensures consistent formatting across all analyses:</p> <pre><code>def get_formatted_subreddit_name(subreddit: str) -&gt; str:\n    \"\"\"Returns a properly formatted subreddit name for display.\"\"\"\n    subreddit_formats = {\n        \"claudeai\": \"ClaudeAI\",\n        \"dataengineering\": \"Data Engineering\",\n        \"datascience\": \"Data Science\",\n        \"localllama\": \"LocalLLaMA\",\n        \"machinelearning\": \"Machine Learning\",\n        \"openai\": \"OpenAI\",\n        \"singularity\": \"Singularity\",\n        \"stablediffusion\": \"Stable Diffusion\"\n    }\n    return f\"{subreddit_formats.get(subreddit.lower(), subreddit)} Subreddit\"\n</code></pre>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#4-pipeline-integration","title":"4. Pipeline Integration","text":"<p>The analysis is integrated into the main pipeline with organized output storage:</p> <pre><code>def analyze_data():\n    \"\"\"\n    Main function to analyze Reddit data using Google's Gemini model.\n\n    Pipeline steps:\n    1. Set up logging and output directory with current date\n    2. Initialize Gemini model\n    3. Connect to BigQuery and GCS\n    4. Process each subreddit\n    5. Clean generated markdown files\n    6. Upload results to GCS\n    \"\"\"\n    try:\n        # 1. Set up output directory with year/month/day structure\n        current_date = datetime.now()\n        year = current_date.strftime('%Y')\n        month = current_date.strftime('%m')\n        day = current_date.strftime('%d')\n\n        output_dir = os.path.join(CLOUD_DIR, 'results', year, month, day)\n        os.makedirs(output_dir, exist_ok=True)\n        logger.info(f\"Output directory set to {output_dir}\")\n\n        # 2. Initialize model\n        genai.configure(api_key=GEMINI_CONFIG['GOOGLE_GEMINI_API_KEY'])\n        model = genai.GenerativeModel('gemini-2.0-flash-exp')\n        logger.info(\"Model loaded\")\n\n        # 3. Connect to BigQuery and GCS\n        bq_client = bigquery.Client()\n        storage_client = storage.Client()\n        logger.info(\"BigQuery and GCS clients initialized\")\n\n        # 4. Process subreddits\n        for subreddit in SUBREDDITS:\n            process_subreddit(model, bq_client, subreddit, output_dir)\n\n        # 5. Clean markdown files and upload to GCS\n        for filename in os.listdir(output_dir):\n            if filename.endswith('.md'):\n                local_file_path = os.path.join(output_dir, filename)\n\n                # Clean the markdown file\n                clean_markdown_file(local_file_path)\n\n                # Upload to GCS\n                gcs_blob_path = get_gcs_path(year, month, day, filename)\n                upload_to_gcs(local_file_path, gcs_blob_path, storage_client)\n\n        logger.info(\"Analysis complete - all files processed and uploaded to GCS\")\n\n    except Exception as e:\n        logger.error(f\"Pipeline failed: {e}\")\n        raise\n</code></pre>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#web-application","title":"Web Application","text":"<p>Our web application, built with Next.js 15, React 18, TypeScript, and MDX, provides a modern and user-friendly way to explore our insights. The application automatically syncs with our on-premise analysis results and deploys to Vercel using GitHub Actions, with build optimizations for performance.</p>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#automated-deployment","title":"Automated Deployment","text":"<p>The deployment process leverages GitHub Actions:</p> <pre><code># .github/workflows/deploy.yml\nname: Deploy to Vercel\n\non:\n  push:\n    branches:\n      - main\n  pull_request:\n    branches:\n      - main\n\nenv:\n  VERCEL_ORG_ID: ${{ secrets.VERCEL_ORG_ID }}\n  VERCEL_PROJECT_ID: ${{ secrets.VERCEL_PROJECT_ID }}\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n          cache: 'npm'\n      - name: Install dependencies\n        run: npm ci\n      - name: Install Vercel CLI\n        run: npm install --global vercel@latest\n      - name: Pull Vercel Environment Information\n        run: vercel pull --yes --environment=production --token=${{ secrets.VERCEL_TOKEN }}\n      - name: Build Project Artifacts\n        run: vercel build --prod --token=${{ secrets.VERCEL_TOKEN }}\n      - name: Deploy Project Artifacts to Vercel\n        run: vercel deploy --prebuilt --prod --token=${{ secrets.VERCEL_TOKEN }}\n</code></pre>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#orchestrator-stack","title":"Orchestrator Stack","text":"<p>Our pipeline is orchestrated using Apache Airflow, configured with a robust setup that ensures reliability, scalability, and observability. The orchestration layer manages the entire data pipeline from ingestion to analysis.</p>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#1-airflow-configuration","title":"1. Airflow Configuration","text":"<p>The Airflow setup is containerized using Docker Compose with several key components:</p> <pre><code>x-common-env: &amp;common-env\n  AIRFLOW__CORE__EXECUTOR: CeleryExecutor\n  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@postgres/${AIRFLOW_DB_NAME}\n  AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@postgres/${AIRFLOW_DB_NAME}\n  AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0\n  AIRFLOW__CORE__LOAD_EXAMPLES: false\n  AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/airflow_project/dags\n  AIRFLOW__LOGGING__BASE_LOG_FOLDER: /opt/airflow/airflow_project/logs\n  AIRFLOW__CORE__PLUGINS_FOLDER: /opt/airflow/airflow_project/plugins\n  AIRFLOW__LOGGING__LOGGING_LEVEL: INFO\n  AIRFLOW__LOGGING__FAB_LOGGING_LEVEL: WARNING\n  AIRFLOW__LOGGING__LOGGING_CONFIG_CLASS: airflow.config_templates.airflow_local_settings.DEFAULT_LOGGING_CONFIG\n  AIRFLOW__LOGGING__DELETE_LOCAL_LOGS: \"true\"\n  AIRFLOW__LOGGING__MAX_LOG_AGE_IN_DAYS: 7\n  PYTHONPATH: /opt/airflow\n  AIRFLOW__METRICS__STATSD_ON: \"true\"\n  AIRFLOW__METRICS__STATSD_HOST: \"statsd-exporter\"\n  AIRFLOW__METRICS__STATSD_PORT: \"9125\"\n  AIRFLOW__METRICS__STATSD_PREFIX: \"airflow\"\n  AIRFLOW__METRICS__STATSD_ALLOW_LIST: \"*\"\n  AIRFLOW__METRICS__METRICS_ALLOW_LIST: \"*\"\n  AIRFLOW__WEBSERVER__EXPOSE_METRICS: 'true'\n  AIRFLOW__METRICS__STATSD_INTERVAL: \"30\"\n  GOOGLE_APPLICATION_CREDENTIALS: ${GOOGLE_APPLICATION_CREDENTIALS}\n  GH_PAT: ${GH_PAT}\n  GH_OWNER: ${GH_OWNER}\n  GH_REPO: ${GH_REPO}\n  GH_WEBSITE_REPO: ${GH_WEBSITE_REPO}\n  GCP_PROJECT_ID: ${GCP_PROJECT_ID}\n  GCS_BUCKET_NAME: ${GCS_BUCKET_NAME}\n  STOP_VM_FUNCTION_URL: ${STOP_VM_FUNCTION_URL}\n  ALERT_EMAIL: ${ALERT_EMAIL}\n  AIRFLOW__SMTP__SMTP_HOST: smtp.gmail.com\n  AIRFLOW__SMTP__SMTP_USER: ${ALERT_EMAIL}\n  AIRFLOW__SMTP__SMTP_PASSWORD: ${ALERT_EMAIL_PASSWORD}\n  AIRFLOW__SMTP__SMTP_PORT: 587\n  AIRFLOW__SMTP__SMTP_MAIL_FROM: ${ALERT_EMAIL}\n  AIRFLOW__SMTP__SMTP_STARTTLS: \"true\"\n  AIRFLOW__SMTP__SMTP_SSL: \"false\"\n\nx-common-volumes: &amp;common-volumes\n  - ..:/opt/airflow\n  - ../credentials:/opt/airflow/credentials\n  - ../mlflow:/mlflow\n  - ../results:/opt/airflow/results\n  - ../dbt_reddit_summary_cloud:/opt/airflow/dbt_reddit_summary_cloud\n\nx-airflow-common: &amp;airflow-common\n  volumes: *common-volumes\n  environment:\n    &lt;&lt;: *common-env\n  user: \"${AIRFLOW_UID:-50000}:0\"\n</code></pre> <p>The Airflow services are deployed as separate containers, each with specific responsibilities:</p> <ol> <li> <p>Airflow Webserver <pre><code>airflow-webserver:\n   image: ${DOCKER_REGISTRY}/reddit-airflow-webserver:latest\n   &lt;&lt;: *airflow-common\n   depends_on:\n     airflow-init:\n       condition: service_completed_successfully\n   ports:\n     - \"8080:8080\"\n   command: airflow webserver\n   healthcheck:\n     test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8080/health\"]\n     interval: 30s\n     timeout: 30s\n     retries: 10\n     start_period: 60s\n   restart: always\n</code></pre> Our Airflow webserver provides the web UI for DAG management and monitoring, handles user authentication and authorization, exposes REST API endpoints, and includes health checks to ensure UI availability.</p> </li> <li> <p>Airflow Scheduler <pre><code>airflow-scheduler:\n   image: ${DOCKER_REGISTRY}/reddit-airflow-scheduler:latest\n   &lt;&lt;: *airflow-common\n   depends_on:\n     airflow-webserver:\n       condition: service_healthy\n     postgres:\n       condition: service_healthy\n     redis:\n       condition: service_healthy\n   command: airflow scheduler\n   healthcheck:\n     test: [\"CMD-SHELL\", 'airflow jobs check --job-type SchedulerJob --hostname \"$${HOSTNAME}\"']\n     interval: 30s\n     timeout: 30s\n     retries: 10\n     start_period: 60s\n   restart: always\n</code></pre> The Airflow scheduler monitors and triggers task execution, manages DAG parsing and scheduling, handles task dependencies and queuing, and ensures proper task distribution to workers. This component is crucial for orchestrating our data pipeline.</p> </li> <li> <p>Airflow Worker <pre><code>airflow-worker:\n   image: ${DOCKER_REGISTRY}/reddit-airflow-worker:latest\n   &lt;&lt;: *airflow-common\n   depends_on:\n     airflow-webserver:\n       condition: service_healthy\n   command: airflow celery worker\n   healthcheck:\n     test: [\"CMD-SHELL\", 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d \"celery@$${HOSTNAME}\"']\n     interval: 30s\n     timeout: 30s\n     retries: 5\n   restart: always\n</code></pre> Our Airflow worker executes the actual tasks, handles ML model inference, manages resource allocation, supports parallel task execution, and is configured for ML workloads with PyTorch and Transformers.</p> </li> <li> <p>Airflow Init <pre><code>airflow-init:\n   build:\n     context: .\n     dockerfile: Dockerfile.webserver\n   env_file:\n     - ../.env\n   &lt;&lt;: *airflow-common\n   depends_on:\n     postgres:\n       condition: service_healthy\n     redis:\n       condition: service_healthy\n   environment:\n     &lt;&lt;: *common-env\n     GIT_PYTHON_REFRESH: quiet\n   command: &gt;\n     bash -c \"\n     airflow db init &amp;&amp;\n     airflow db upgrade &amp;&amp;\n     airflow users create -r Admin -u admin -p admin -e admin@example.com -f Anonymous -l Admin\n     \"\n</code></pre> The Airflow init service initializes the Airflow database, creates an admin user, performs database migrations, and runs only during the initial setup. This component is essential for setting up the Airflow environment.</p> </li> </ol>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#2-pipeline-structure","title":"2. Pipeline Structure","text":"<p>Our DAG (<code>reddit_pipeline</code>) is organized into 27 stages but can be categorized into 6 main sections, each with specific responsibilities and metrics collection:</p> <ol> <li>Data Collection and Preprocessing <pre><code>ingest_task = PythonOperator(\n    task_id='ingest_and_preprocess',\n    python_callable=ingest_preprocess_process,\n    provide_context=True\n)\n</code></pre> Data is ingested and preprocessed.</li> <li>DBT Transformations <pre><code>dbt_staging_task = BashOperator(\n    task_id='run_dbt_staging',\n    bash_command='cd /opt/airflow/dags/dbt_reddit_summary_cloud &amp;&amp; dbt run --select current_summary_staging',\n    dag=dag\n)\n</code></pre> DBT transformations are run to prepare the data for analysis.</li> <li> <p>Model Processing <pre><code>    summarize_metrics_task = PythonOperator(\n        task_id='parse_summarize_metrics',\n        python_callable=parse_summarize_metrics,\n        provide_context=True\n    )\n\n    sentiment_task = PythonOperator(\n        task_id='run_sentiment_analysis',\n        python_callable=sentiment_analysis_process,\n        provide_context=True\n    )\n\n\n    gemini_task = PythonOperator(\n        task_id='run_gemini',\n        python_callable=gemini_analysis_process,\n        dag=dag\n    )\n</code></pre> Our analysis pipeline includes text summarization using BART, sentiment analysis with RoBERTa, and advanced analysis with Gemini AI.</p> </li> <li> <p>Quality Checks <pre><code>dbt_test_raw_sources = BashOperator(\n    task_id='test_raw_sources',\n    bash_command=DBT_TEST_CMD.format(selector='source:raw_data source:processed_data'),\n    dag=dag\n)\n</code></pre> DBT tests are run to ensure the data is valid.</p> </li> <li> <p>Metrics Collection</p> <ul> <li>We monitor our pipeline with dedicated metrics tasks, using a StatsD exporter to send real-time data to Prometheus, and MLflow tracking for model performance.</li> </ul> </li> <li> <p>Shutdown VM using Cloud Function <pre><code>    shutdown_vm = PythonOperator(\n        task_id='shutdown_vm',\n        python_callable=trigger_vm_shutdown,\n        provide_context=True,\n        trigger_rule='all_done',  # Run even if upstream tasks fail\n        retries=0,  # Don't retry on failure since VM will be shutting down\n        dag=dag,\n        email_on_failure=False,\n    )\n</code></pre> We use a Cloud Function to shutdown the VM after the pipeline is complete.</p> </li> </ol>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#3-task-dependencies","title":"3. Task Dependencies","text":"<p>The pipeline follows a clear dependency chain: <pre><code>ingest_task &gt;&gt; ingest_metrics_task &gt;&gt; \\\ndbt_test_raw_sources &gt;&gt; dbt_test_raw_metrics_task &gt;&gt; \\\ndbt_staging_task &gt;&gt; dbt_test_staging_models &gt;&gt; \\\nsummarize_task &gt;&gt; sentiment_task &gt;&gt; \\\ndbt_join_summary_analysis_task &gt;&gt; \\\ngemini_task &gt;&gt; push_gemini_results_task &gt;&gt; shutdown_vm\n</code></pre> Our pipeline starts with data ingestion and preprocessing, followed by DBT testing and staging, then text summarization and sentiment analysis, and finally Gemini AI analysis, result pushing and VM shutdown.</p>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#4-integration-points","title":"4. Integration Points","text":"<p>The orchestrator connects with BigQuery for pipeline data storage, local model deployment for model serving, StatsD and Prometheus for monitoring, and GitHub for version control of results. These connections are essential for the functionality of our pipeline.</p>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#5-cloud-function","title":"5. Cloud Function","text":""},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#51-start-vm","title":"5.1. Start VM","text":"<p>The Cloud Function to start the VM is deployed using Cloud Run. It ensures seamless startup of virtual machines on-demand, optimizing resource utilization.</p> <pre><code># Create Flask app\napp = Flask(__name__)\n\ndef get_gcloud_path():\n    \"\"\"Get the full path to gcloud executable based on OS\"\"\"\n    if platform.system() == \"Windows\":\n        # Common installation paths for gcloud on Windows\n        possible_paths = [\n            r\"C:\\Program Files (x86)\\Google\\Cloud SDK\\google-cloud-sdk\\bin\\gcloud.cmd\",\n            r\"C:\\Program Files\\Google\\Cloud SDK\\google-cloud-sdk\\bin\\gcloud.cmd\",\n            os.path.expanduser(\"~\") + r\"\\AppData\\Local\\Google\\Cloud SDK\\google-cloud-sdk\\bin\\gcloud.cmd\"\n        ]\n        for path in possible_paths:\n            if os.path.exists(path):\n                return path\n        raise Exception(\"gcloud not found. Please ensure Google Cloud SDK is installed and in PATH\")\n    return \"gcloud\"  # For non-Windows systems, assume it's in PATH\n\n@app.route(\"/\", methods=[\"POST\"])\ndef start_vm_http():\n    \"\"\"\n    HTTP endpoint for Cloud Run to start a Compute Engine instance.\n    Accepts POST requests with optional JSON body:\n    {'trigger_source': 'scheduler'|'manual'|'dag'}\n    \"\"\"\n    return start_vm(request)\n\ndef start_vm(request):\n    \"\"\"\n    Function to start a Compute Engine instance and execute startup script.\n    \"\"\"\n    project = os.environ.get('PROJECT_ID')\n    zone = os.environ.get('ZONE')\n    instance = os.environ.get('INSTANCE')\n    gh_repo = os.environ.get('GH_REPO')\n\n    # Setup logging\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n    logger = logging.getLogger(__name__)\n    logger.info(f\"Environment: PROJECT_ID={project}, ZONE={zone}, INSTANCE={instance}, GH_REPO={gh_repo}\")\n\n    # Parse request data\n    trigger_source = \"scheduler\"\n    if request and hasattr(request, 'is_json') and request.is_json:\n        data = request.get_json()\n        if data and 'trigger_source' in data:\n            trigger_source = data.get('trigger_source')\n\n    logger.info(f\"Start triggered by: {trigger_source}\")\n\n    try:\n        # Get gcloud path\n        gcloud_path = get_gcloud_path()\n        logger.info(f\"Using gcloud path: {gcloud_path}\")\n\n        # Check if instance is running\n        status_cmd = [gcloud_path, \"compute\", \"instances\", \"describe\", instance,\n                     f\"--zone={zone}\", f\"--project={project}\",\n                     \"--format=get(status)\"]\n        status = subprocess.run(status_cmd, text=True, capture_output=True)\n\n        is_running = status.stdout.strip().upper() == \"RUNNING\"\n\n        if not is_running:\n            # Start the instance\n            start_cmd = [gcloud_path, \"compute\", \"instances\", \"start\", instance,\n                        f\"--zone={zone}\", f\"--project={project}\", \"--quiet\"]\n            logger.info(f\"Starting instance with command: {' '.join(start_cmd)}\")\n            process = subprocess.run(start_cmd, text=True, capture_output=True)\n\n            if process.returncode != 0:\n                error_msg = f\"Failed to start instance: {process.stderr}\"\n                logger.error(error_msg)\n                return jsonify({'status': 'error', 'message': error_msg}), 500\n\n            # Wait for VM to be fully started\n            logger.info(\"Waiting 90 seconds for VM to be fully ready...\")\n            time.sleep(90)\n\n        # Execute startup script\n        logger.info(\"Executing startup script...\")\n        startup_cmd = [gcloud_path, \"compute\", \"ssh\", \"--quiet\", f\"airflow@{instance}\",\n                      f\"--zone={zone}\", f\"--project={project}\", \n                      \"--command\", f\"bash /opt/airflow/{gh_repo}/Cloud/infrastructure/terraform/vm_scripts/start_dag.sh\"]\n\n        process = subprocess.run(startup_cmd, text=True, capture_output=True)\n\n        if process.returncode == 0:\n            logger.info(\"Startup script executed successfully\")\n            logger.info(f\"Startup stdout: {process.stdout}\")\n            msg = f'Successfully started instance and executed startup script'\n            return jsonify({'status': 'success', 'message': msg})\n        else:\n            error_msg = f\"Startup script failed: {process.stderr}\"\n            logger.error(error_msg)\n            return jsonify({'status': 'error', 'message': error_msg}), 500\n\n    except Exception as e:\n        error_msg = f'Error during startup process: {str(e)}'\n        logger.error(error_msg)\n        logger.error(f\"Full stack trace: {traceback.format_exc()}\")\n        return jsonify({'status': 'error', 'message': error_msg}), 500\n</code></pre>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#52-stop-vm","title":"5.2. Stop VM","text":"<p>The Cloud Function to stop the VM is deployed using Cloud Run. This helps automate shutdown processes, reducing costs by powering down unused resources.</p> <pre><code>import os\nimport json\nimport logging\nimport subprocess\nfrom flask import Flask, request, jsonify\nimport platform\nimport traceback\n\n# Create Flask app\napp = Flask(__name__)\n\ndef get_gcloud_path():\n    \"\"\"Get the full path to gcloud executable based on OS\"\"\"\n    if platform.system() == \"Windows\":\n        # Common installation paths for gcloud on Windows\n        possible_paths = [\n            r\"C:\\Program Files (x86)\\Google\\Cloud SDK\\google-cloud-sdk\\bin\\gcloud.cmd\",\n            r\"C:\\Program Files\\Google\\Cloud SDK\\google-cloud-sdk\\bin\\gcloud.cmd\",\n            os.path.expanduser(\"~\") + r\"\\AppData\\Local\\Google\\Cloud SDK\\google-cloud-sdk\\bin\\gcloud.cmd\"\n        ]\n        for path in possible_paths:\n            if os.path.exists(path):\n                return path\n        raise Exception(\"gcloud not found. Please ensure Google Cloud SDK is installed and in PATH\")\n    return \"gcloud\"  # For non-Windows systems, assume it's in PATH\n\n@app.route(\"/\", methods=[\"POST\"])\ndef stop_vm_http():\n    \"\"\"\n    HTTP endpoint for Cloud Run to stop a Compute Engine instance.\n    Accepts POST requests with optional JSON body:\n    {'trigger_source': 'scheduler'|'manual'|'dag'}\n    \"\"\"\n    return stop_vm(request)\n\ndef stop_vm(request):\n    \"\"\"\n    Function to stop a Compute Engine instance.\n    Only runs shutdown script if VM is running.\n\n    Args:\n        request (flask.Request): The request object\n        - If called from manual trigger: expects JSON with {'trigger_source': 'manual'}\n        - If called from Scheduler: no specific payload needed\n    Returns:\n        JSON response with status message\n    \"\"\"\n    project = os.environ.get('PROJECT_ID')\n    zone = os.environ.get('ZONE')\n    instance = os.environ.get('INSTANCE')\n    gh_repo = os.environ.get('GH_REPO')\n\n    # Setup detailed logging\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)s - %(levelname)s - %(message)s'\n    )\n    logger = logging.getLogger(__name__)\n\n    # Log environment variables (excluding any sensitive data)\n    logger.info(f\"Environment: PROJECT_ID={project}, ZONE={zone}, INSTANCE={instance}, GH_REPO={gh_repo}\")\n\n    # Parse request data if present\n    trigger_source = \"scheduler\"\n    if request and hasattr(request, 'is_json') and request.is_json:\n        data = request.get_json()\n        if data and 'trigger_source' in data:\n            trigger_source = data.get('trigger_source')\n\n    logger.info(f\"Shutdown triggered by: {trigger_source}\")\n\n    try:\n        # Get gcloud path\n        gcloud_path = get_gcloud_path()\n        logger.info(f\"Using gcloud path: {gcloud_path}\")\n\n        # Check if instance is running\n        status_cmd = [gcloud_path, \"compute\", \"instances\", \"describe\", instance, \n                     f\"--zone={zone}\", f\"--project={project}\", \n                     \"--format=get(status)\"]\n        status = subprocess.run(status_cmd, text=True, capture_output=True)\n\n        is_running = status.stdout.strip().upper() == \"RUNNING\"\n\n        if not is_running:\n            msg = f'Instance {instance} is already stopped (Triggered by: {trigger_source})'\n            logger.info(msg)\n            return jsonify({'status': 'success', 'message': msg})\n\n        # Execute shutdown script via SSH since VM is running\n        logger.info(\"Executing shutdown script via SSH...\")\n        shutdown_cmd = f\"bash /opt/airflow/{gh_repo}/Cloud/infrastructure/terraform/vm_scripts/shutdown.sh\"\n        ssh_cmd = [gcloud_path, \"compute\", \"ssh\", \"--quiet\", f\"airflow@{instance}\", \n                  f\"--zone={zone}\", f\"--project={project}\", \"--command\", shutdown_cmd]\n\n        logger.info(f\"Executing command: {' '.join(ssh_cmd)}\")\n        process = subprocess.run(ssh_cmd, text=True, capture_output=True)\n\n        if process.returncode != 0:\n            logger.error(f\"SSH command failed with return code {process.returncode}\")\n            logger.error(f\"SSH stderr: {process.stderr}\")\n            logger.error(f\"SSH stdout: {process.stdout}\")\n            raise Exception(f\"Failed to execute shutdown script: {process.stderr}\")\n\n        logger.info(\"Shutdown script executed successfully. Stopping instance...\")\n        logger.info(f\"SSH stdout: {process.stdout}\")\n\n        # Stop the instance\n        stop_cmd = [gcloud_path, \"compute\", \"instances\", \"stop\", instance, \n                   f\"--zone={zone}\", f\"--project={project}\", \"--quiet\"]\n        logger.info(f\"Executing stop command: {' '.join(stop_cmd)}\")\n        process = subprocess.run(stop_cmd, text=True, capture_output=True)\n\n        if process.returncode != 0:\n            logger.error(f\"Stop command failed with return code {process.returncode}\")\n            logger.error(f\"Stop stderr: {process.stderr}\")\n            logger.error(f\"Stop stdout: {process.stdout}\")\n            raise Exception(f\"Failed to stop instance: {process.stderr}\")\n\n        logger.info(f\"Stop command stdout: {process.stdout}\")\n        msg = f'Successfully executed shutdown script and stopped instance {instance} (Triggered by: {trigger_source})'\n        logger.info(msg)\n        return jsonify({'status': 'success', 'message': msg})\n\n    except Exception as e:\n        error_msg = f'Error during shutdown process: {str(e)}'\n        logger.error(error_msg)\n        logger.error(f\"Full stack trace: {traceback.format_exc()}\")\n        return jsonify({'status': 'error', 'message': error_msg}), 500\n\nif __name__ == \"__main__\":\n    # For local testing with Flask\n    app.run(host=\"0.0.0.0\", port=int(os.environ.get(\"PORT\", 9099)), debug=False) \n</code></pre>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#53-cloud-scheduler","title":"5.3. Cloud Scheduler","text":"<p>Cloud Scheduler is used to trigger the Cloud Function to start and stop the VM daily at 4:00 PM EST and 6:00 PM EST respectively.</p> <pre><code># Service account for Cloud Scheduler\nresource \"google_service_account\" \"scheduler_sa\" {\n  account_id   = \"vm-scheduler-sa\"\n  display_name = \"Service Account for VM Scheduler\"\n\n  lifecycle {\n    ignore_changes = [\n      account_id,\n      display_name\n    ]\n  }\n}\n\n# Grant necessary permissions\nresource \"google_project_iam_member\" \"scheduler_sa_compute_admin\" {\n  project = var.project\n  role    = \"roles/compute.instanceAdmin.v1\"\n  member  = \"serviceAccount:${google_service_account.scheduler_sa.email}\"\n}\n\n# Cloud Scheduler job for starting VM (4:00 PM EST)\nresource \"google_cloud_scheduler_job\" \"start_vm_schedule\" {\n  name             = \"start-vm-daily\"\n  description      = \"Starts the Airflow VM daily at 4:00 PM EST\"\n  schedule         = \"0 16 * * *\"  # 16:00 = 4:00 PM\n  time_zone        = \"America/New_York\"\n  attempt_deadline = \"900s\"  # 15 minutes\n\n  http_target {\n    http_method = \"POST\"\n    uri         = google_cloud_run_service.start_vm.status[0].url\n\n    body = base64encode(jsonencode({\n      trigger_source = \"scheduler\"\n    }))\n\n    headers = {\n      \"Content-Type\" = \"application/json\"\n    }\n  }\n}\n\n# Cloud Scheduler job for stopping VM (6:00 PM EST)\nresource \"google_cloud_scheduler_job\" \"stop_vm_schedule\" {\n  name             = \"stop-vm-daily\"\n  description      = \"Stops the Airflow VM daily at 6:00 PM EST (backup)\"\n  schedule         = \"0 18 * * *\"  # 18:00 = 6:00 PM\n  time_zone        = \"America/New_York\"\n  attempt_deadline = \"900s\"  # 15 minutes\n\n  http_target {\n    http_method = \"POST\"\n    uri         = google_cloud_run_service.stop_vm.status[0].url\n\n    body = base64encode(jsonencode({\n      trigger_source = \"scheduler\"\n    }))\n\n    headers = {\n      \"Content-Type\" = \"application/json\"\n    }\n  }\n} \n</code></pre>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#observability-stack","title":"Observability Stack","text":"<p>We use Grafana, Prometheus, and MLflow for comprehensive pipeline observability, with each component containerized and integrated through a centralized metrics system.</p>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#1-metrics-collection-architecture","title":"1. Metrics Collection Architecture","text":"<p>The metrics collection is structured in layers:</p> <ol> <li> <p>Source Metrics (via StatsD)    <pre><code>AIRFLOW__METRICS__STATSD_ON: \"true\"\nAIRFLOW__METRICS__STATSD_HOST: \"statsd-exporter\"\nAIRFLOW__METRICS__STATSD_PORT: \"9125\"\nAIRFLOW__METRICS__STATSD_PREFIX: \"airflow\"\n</code></pre></p> </li> <li> <p>Prometheus Targets <pre><code>scrape_configs:\n  - job_name: 'airflow'\n    targets: ['statsd-exporter:9102']\n  - job_name: 'redis'\n    targets: ['redis-exporter:9121']\n  - job_name: 'postgres'\n    targets: ['postgres-exporter:9187']\n  - job_name: 'mlflow'\n    targets: ['mlflow:5000']\n</code></pre></p> </li> </ol>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#2-task-specific-metrics","title":"2. Task-Specific Metrics","text":"<p>Each pipeline component has dedicated metrics tracking:</p> <ol> <li> <p>Gemini Analysis Metrics <pre><code># From log_gemini_metrics.py\nmetrics = {\n    'outputs_generated': 0,      # Number of successful analyses\n    'subreddits_processed': 0,   # Subreddits completed\n    'duration_seconds': 0,       # Processing time\n    'attempt': current_attempt   # Retry tracking\n}\n</code></pre></p> </li> <li> <p>Sentiment Analysis Performance <pre><code># From log_sentiment_analysis_metrics.py\nmetrics = {\n    'sentiments_processed': 0,   # Comments analyzed\n    'duration_seconds': 0,       # Processing duration\n    'attempt': current_attempt   # Execution attempts\n}\n</code></pre></p> </li> <li> <p>Pipeline Processing Volumes <pre><code>{\n  \"targets\": [\n    {\"expr\": \"reddit_ingest_preprocess_posts_total\"},\n    {\"expr\": \"reddit_summarize_summaries_added\"},\n    {\"expr\": \"reddit_sentiment_processed_total\"},\n    {\"expr\": \"reddit_joined_table_rows\"}\n  ]\n}\n</code></pre></p> </li> </ol>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#3-performance-dashboards","title":"3. Performance Dashboards","text":"<p>Our Grafana dashboard is designed to visualize processing efficiency (task durations, volumes, success rates, resource utilization), data quality (DBT test results, validation metrics, processing status, error rates), and provide real-time monitoring.</p>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#4-mlflow-integration","title":"4. MLflow Integration","text":"<p>MLflow's model versioning, experiment tracking, model serving, and artifact management features aid in model selection by allowing us to identify the best-performing models.</p>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#5-alert-configuration","title":"5. Alert Configuration","text":"<p>The system monitors critical thresholds for performance, including task duration, error rate spikes, resource exhaustion, and pipeline stalls. It also monitors quality thresholds, including data validation failures, model performance degradation, processing anomalies, and system health issues</p>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#6-cloud-monitoring","title":"6. Cloud Monitoring","text":"<p>We use Cloud Monitoring to monitor the health of our VM and Cloud Run service, and to set up alerts for critical issues.</p> <pre><code># Cloud Monitoring Dashboard\nresource \"google_monitoring_dashboard\" \"airflow_dashboard\" {\n  dashboard_json = jsonencode({\n    displayName = \"Reddit Pipeline Dashboard\"\n    gridLayout = {\n      columns = \"2\"\n      widgets = [\n        # VM Metrics\n        {\n          title = \"VM CPU Utilization\"\n          xyChart = {\n            dataSets = [{\n              timeSeriesQuery = {\n                timeSeriesFilter = {\n                  filter = \"metric.type=\\\"compute.googleapis.com/instance/cpu/utilization\\\" resource.type=\\\"gce_instance\\\"\"\n                }\n              }\n            }]\n          }\n        },\n        {\n          title = \"VM Memory Usage\"\n          xyChart = {\n            dataSets = [{\n              timeSeriesQuery = {\n                timeSeriesFilter = {\n                  filter = \"metric.type=\\\"compute.googleapis.com/instance/memory/usage\\\" resource.type=\\\"gce_instance\\\"\"\n                }\n              }\n            }]\n          }\n        },\n        # Airflow Task Metrics\n        {\n          title = \"DAG Success Rate\"\n          xyChart = {\n            dataSets = [{\n              timeSeriesQuery = {\n                timeSeriesFilter = {\n                  filter = \"metric.type=\\\"custom.googleapis.com/airflow/dag_success_rate\\\"\"\n                }\n              }\n            }]\n          }\n        },\n        {\n          title = \"Task Duration\"\n          xyChart = {\n            dataSets = [{\n              timeSeriesQuery = {\n                timeSeriesFilter = {\n                  filter = \"metric.type=\\\"custom.googleapis.com/airflow/task_duration\\\"\"\n                }\n              }\n            }]\n          }\n        },\n        # Pipeline Metrics\n        {\n          title = \"Records Processed\"\n          xyChart = {\n            dataSets = [{\n              timeSeriesQuery = {\n                timeSeriesFilter = {\n                  filter = \"metric.type=\\\"custom.googleapis.com/reddit/records_processed\\\"\"\n                }\n              }\n            }]\n          }\n        },\n        {\n          title = \"Processing Errors\"\n          xyChart = {\n            dataSets = [{\n              timeSeriesQuery = {\n                timeSeriesFilter = {\n                  filter = \"metric.type=\\\"custom.googleapis.com/reddit/processing_errors\\\"\"\n                }\n              }\n            }]\n          }\n        }\n      ]\n    }\n  })\n}\n\n# Alert Policies\nresource \"google_monitoring_alert_policy\" \"vm_cpu_utilization\" {\n  display_name = \"High CPU Utilization Alert\"\n  combiner     = \"OR\"\n  conditions {\n    display_name = \"CPU Usage &gt; 80%\"\n    condition_threshold {\n      filter          = \"metric.type=\\\"compute.googleapis.com/instance/cpu/utilization\\\" resource.type=\\\"gce_instance\\\"\"\n      duration        = \"300s\"\n      comparison      = \"COMPARISON_GT\"\n      threshold_value = 0.8\n      trigger {\n        count = 1\n      }\n    }\n  }\n  notification_channels = [google_monitoring_notification_channel.email.id]\n}\n\nresource \"google_monitoring_alert_policy\" \"airflow_task_failures\" {\n  display_name = \"Airflow Task Failures Alert\"\n  combiner     = \"OR\"\n  conditions {\n    display_name = \"Task Failure Rate &gt; 20%\"\n    condition_threshold {\n      filter          = \"metric.type=\\\"${google_monitoring_metric_descriptor.airflow_task_failure_rate.type}\\\" AND resource.type=\\\"gce_instance\\\"\"\n      duration        = \"300s\"\n      comparison      = \"COMPARISON_GT\"\n      threshold_value = 0.2\n      trigger {\n        count = 1\n      }\n    }\n  }\n  notification_channels = [google_monitoring_notification_channel.email.id]\n  enabled = true\n  depends_on = [google_monitoring_metric_descriptor.airflow_task_failure_rate]\n}\n\nresource \"google_monitoring_alert_policy\" \"pipeline_errors\" {\n  display_name = \"Pipeline Processing Errors Alert\"\n  combiner     = \"OR\"\n  conditions {\n    display_name = \"Processing Error Rate\"\n    condition_threshold {\n      filter          = \"metric.type=\\\"${google_monitoring_metric_descriptor.processing_errors.type}\\\" AND resource.type=\\\"gce_instance\\\"\"\n      duration        = \"300s\"\n      comparison      = \"COMPARISON_GT\"\n      threshold_value = 10\n      trigger {\n        count = 1\n      }\n    }\n  }\n  notification_channels = [google_monitoring_notification_channel.email.id]\n  enabled = true\n  depends_on = [google_monitoring_metric_descriptor.processing_errors]\n}\n\nresource \"google_monitoring_alert_policy\" \"vm_automation\" {\n  display_name = \"VM Automation Alert\"\n  combiner     = \"OR\"\n  conditions {\n    display_name = \"VM Start/Stop Failures\"\n    condition_threshold {\n      filter          = \"metric.type=\\\"${google_monitoring_metric_descriptor.vm_automation_failures.type}\\\" AND resource.type=\\\"gce_instance\\\"\"\n      duration        = \"0s\"\n      comparison      = \"COMPARISON_GT\"\n      threshold_value = 0\n      trigger {\n        count = 1\n      }\n    }\n  }\n  notification_channels = [google_monitoring_notification_channel.email.id]\n  enabled = true\n  depends_on = [google_monitoring_metric_descriptor.vm_automation_failures]\n}\n\n# Notification Channels\nresource \"google_monitoring_notification_channel\" \"email\" {\n  display_name = \"Email Notification Channel\"\n  type         = \"email\"\n  labels = {\n    email_address = var.alert_email_address\n  }\n}\n\n# Optional: Slack notification channel\nresource \"google_monitoring_notification_channel\" \"slack\" {\n  count = var.slack_webhook_url != \"\" ? 1 : 0\n\n  display_name = \"Slack Notification Channel\"\n  type         = \"slack\"\n  labels = {\n    channel_name = \"#airflow-alerts\"\n  }\n  sensitive_labels {\n    auth_token = var.slack_webhook_url\n  }\n}\n\n# First, define the custom metric descriptors\nresource \"google_monitoring_metric_descriptor\" \"airflow_task_failure_rate\" {\n  description = \"Rate of Airflow task failures\"\n  display_name = \"Airflow Task Failure Rate\"\n  type = \"custom.googleapis.com/airflow/task_failure_rate\"\n  metric_kind = \"GAUGE\"\n  value_type = \"DOUBLE\"\n  unit = \"1\"\n  labels {\n    key = \"task_id\"\n    value_type = \"STRING\"\n    description = \"The ID of the Airflow task\"\n  }\n}\n\nresource \"google_monitoring_metric_descriptor\" \"processing_errors\" {\n  description = \"Number of processing errors in the pipeline\"\n  display_name = \"Pipeline Processing Errors\"\n  type = \"custom.googleapis.com/reddit/processing_errors\"\n  metric_kind = \"GAUGE\"\n  value_type = \"INT64\"\n  unit = \"1\"\n}\n\nresource \"google_monitoring_metric_descriptor\" \"vm_automation_failures\" {\n  description = \"Number of VM automation failures\"\n  display_name = \"VM Automation Failures\"\n  type = \"custom.googleapis.com/airflow/vm_automation_failures\"\n  metric_kind = \"GAUGE\"\n  value_type = \"INT64\"\n  unit = \"1\"\n} \n</code></pre>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#results-and-impact","title":"Results and Impact","text":"<p>The pipeline successfully processes a thousand of Reddit posts daily, generates concise and meaningful summaries, identifies key trends and discussions, maintains high accuracy in content filtering, provides real-time insights through a web interface. </p>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#future-improvements","title":"Future Improvements","text":"<ol> <li>Enhanced Analysis<ul> <li>Integration with additional AI models</li> <li>Advanced visualization features</li> <li>Real-time analysis capabilities</li> </ul> </li> <li>System Scalability<ul> <li>Distributed processing implementation</li> <li>Enhanced caching mechanisms</li> <li>API endpoint for analysis access</li> </ul> </li> <li>User Experience<ul> <li>Interactive dashboard development</li> <li>Customizable analysis parameters</li> <li>Trend prediction features</li> </ul> </li> </ol>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#conclusion","title":"Conclusion","text":"<p>This project showcases the power of automation in extracting valuable insights from social media using AI and data engineering, while handling real-world challenges. The open-source code is on GitHub, and welcome community contributions.</p>"},{"location":"writing/2025/01/23/Reddit%20AI%20Pulse%20%28Cloud%29/#resources-and-references","title":"Resources and References","text":"<ul> <li>Project GitHub Repository</li> <li>Web Application</li> </ul>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/","title":"From Reddit to Insights: Building an AI-Powered Data Pipeline with Gemini (On-Prem)","text":""},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#introduction","title":"Introduction","text":"<p>Purpose</p> <p>In this blog post, I document the process of building an AI-driven, on-premises data pipeline to automate this task. Using Google\u2019s Gemini AI, the pipeline collects, processes, and synthesizes discussions from AI-related subreddits into structured daily reports. The system is designed to filter out irrelevant or harmful content, ensuring the extracted insights are both meaningful and actionable.</p> <p>Check out the project GitHub repository for the full code and detailed documentation and Web Application.</p> <p>Problem Statement</p> <p>The field of artificial intelligence and machine learning evolves at an unprecedented pace, with new breakthroughs, trends, and discussions emerging daily. Platforms like Reddit host vibrant AI-focused communities that are rich with valuable insights. However, manually monitoring multiple subreddits to extract meaningful information is both time-consuming and inefficient.</p>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#what-youll-learn","title":"What You\u2019ll Learn","text":"<ul> <li>Designing a robust data pipeline orchestrated with Airflow: How to automate and orchestrate complex data workflows.  </li> <li>Optimizing data flow with PostgreSQL and dbt: Best practices for structuring and managing collected data efficiently.  </li> <li>Applying AI for sentiment analysis and summarization: Techniques to extract concise insights from unstructured Reddit discussions.    </li> <li>Utilizing Google\u2019s Gemini AI for content analysis: Insights into leveraging advanced AI models to categorize and interpret data.  </li> </ul>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#system-architecture-modular-and-scalable-design","title":"System Architecture: Modular and Scalable Design","text":"<p>Our pipeline is designed with modularity and scalability in mind, comprising six main layers. Below is a high-level overview of how the components interact:</p> <p></p> <p>The diagram above illustrates the flow of data through our system, from collection to presentation. Each layer has specific responsibilities and communicates with adjacent layers through well-defined interfaces.</p>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#1-data-collection-layer","title":"1. Data Collection Layer","text":"<ul> <li>Reddit API Integration:<ul> <li>Fetch posts and comments from AI-focused subreddits.</li> </ul> </li> <li>Text Preprocessing and Cleaning:<ul> <li>Separate processing pipelines for posts and comments.</li> <li>Remove special characters and formatting.</li> </ul> </li> <li>Content Validation and Filtering: <ul> <li>Ensure only relevant and high-quality data is processed.</li> </ul> </li> <li>Rate Limiting and Error Handling: <ul> <li>Manage API limits.</li> </ul> </li> <li>DBT Transformations:<ul> <li>Stage posts and comments for further processing.</li> <li>Clean and standardize data structures.</li> <li>Manage processing state for each pipeline run.</li> <li>Automate cleanup upon pipeline completion.</li> </ul> </li> </ul>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#2-storage-layer","title":"2. Storage Layer","text":"<ul> <li>PostgreSQL Database: <ul> <li>Structured data storage with the following tables:<ul> <li>Raw data tables for posts and comments.</li> <li>DBT staging tables for processing queues.</li> <li>Processed results tables for analysis outputs.</li> </ul> </li> </ul> </li> <li>Organized File System:<ul> <li>Store analysis outputs in a structured and accessible manner.</li> </ul> </li> <li>Data Versioning and Backup:<ul> <li>Implement robust strategies for data integrity and recovery.</li> </ul> </li> </ul>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#3-processing-layer","title":"3. Processing Layer","text":"<ul> <li>Text Summarization: Generate concise summaries of discussions.</li> <li>Sentiment Analysis: Extract emotional context from posts and comments.</li> <li>Gemini AI Integration: Perform comprehensive content analysis using LLMs.</li> </ul>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#4-orchestration-layer","title":"4. Orchestration Layer","text":"<ul> <li>Airflow DAGs: Manage pipeline workflows efficiently.</li> <li>Scheduled Task Execution: Automate task triggers based on predefined schedules.</li> <li>Monitoring and Error Handling: Ensure reliability through automated checks.</li> </ul>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#5-observability-layer","title":"5. Observability Layer","text":"<ul> <li>Grafana Dashboards: Provide real-time monitoring of pipeline health and performance.</li> <li>Prometheus Metrics Collection: Gather and analyze system metrics.</li> <li>MLflow: Track machine learning experiments and maintain a model registry.</li> </ul>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#6-presentation-layer","title":"6. Presentation Layer","text":"<ul> <li>React-Based Web Application: Present insights through an interactive user interface.</li> <li>Automated Deployment: Streamline updates using GitHub Actions.</li> <li>Real-Time Data Synchronization: Provides updates in batches (daily updates) to ensure users receive timely insights.</li> <li>Interactive Visualizations: Enable dynamic exploration of AI-related discussions.</li> </ul> <p>This modular design ensures adaptability, maintainability, and scalability, enabling seamless interaction between components and the efficient transformation of Reddit data into actionable insights.</p>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#deep-dive-key-components","title":"Deep Dive: Key Components","text":""},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#reddit-data-collection-and-preprocessing","title":"Reddit Data Collection and Preprocessing","text":"<p>The foundation of our pipeline is reliable data collection and preprocessing. We utilize Python's Reddit API wrapper (PRAW) to fetch posts and comments from specified subreddits, with immediate text preprocessing for clean data storage.</p>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#1-data-collection-with-praw","title":"1. Data Collection with PRAW","text":"<p>The Reddit client implementation focuses on efficient and reliable data collection. Here's how we handle both posts and their comments:</p> <pre><code>def fetch_and_save_posts(reddit, subreddit_name, db_utils, conn):\n    \"\"\"Fetches new posts from a subreddit since the last processed timestamp.\"\"\"\n    last_processed_utc = get_last_processed_timestamp(conn, subreddit_name)\n    current_max_utc = 0\n\n    subreddit = reddit.subreddit(subreddit_name)\n    for submission in subreddit.hot(limit=20):\n        # Skip already processed posts\n        if submission.created_utc &lt;= last_processed_utc:\n            continue\n\n        # Process post data\n        post_data = {\n            \"subreddit\": subreddit_name,\n            \"post_id\": submission.id,\n            \"title\": str(submission.title),\n            \"author\": str(submission.author),\n            \"url\": str(submission.url),\n            \"score\": int(submission.score),\n            \"created_utc\": submission.created_utc,\n            \"comments\": []\n        }\n\n        # Handle comments if they exist\n        if submission.num_comments &gt; 0:\n            try:\n                submission.comments.replace_more(limit=0)  # Expand comment tree\n                post_data[\"comments\"] = [\n                    {\n                        \"comment_id\": comment.id,\n                        \"author\": str(comment.author),\n                        \"body\": str(comment.body),\n                        \"created_utc\": comment.created_utc,\n                    }\n                    for comment in submission.comments.list()[:10]  # Top 10 comments\n                    if comment.created_utc &gt; last_processed_utc\n                ]\n            except Exception as e:\n                logging.error(f\"Error fetching comments for post {submission.id}: {e}\")\n\n        # Save to database\n        db_utils.insert_raw_post_data(conn, post_data)\n        current_max_utc = max(current_max_utc, submission.created_utc)\n</code></pre> <p>The raw data is stored in PostgreSQL with the following schema:</p> <pre><code>-- Raw posts table schema\nCREATE TABLE raw_data.raw_posts (\n    id SERIAL PRIMARY KEY,\n    subreddit VARCHAR(50) NOT NULL,\n    post_id VARCHAR(20) UNIQUE NOT NULL,\n    title TEXT NOT NULL,\n    author VARCHAR(100) NOT NULL,\n    url TEXT,\n    score INTEGER,\n    created_utc TIMESTAMP WITH TIME ZONE,\n    comments JSONB,  -- Stores comment data as JSON\n    processed BOOLEAN DEFAULT FALSE,\n    inserted_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n);\n</code></pre> <p>Our system is designed for efficient and reliable data processing. We use incremental data collection, fetching only new data based on timestamps. To manage the volume of Reddit data, we focus on the top 10 comments per post, allowing us to process approximately 1,000 comments daily in about 50 minutes. This can be easily scaled by adding more compute resources. We store comments in JSONB format, which provides flexibility for handling semi-structured data.</p> <p>We've implemented robust error handling with retry mechanisms and transaction management to ensure data consistency. Batch processing is used to improve scalability and efficiency when dealing with large datasets. While stream processing could further enhance scalability, we opted for batch processing for this use case.</p> <p>JSONB storage was chosen for its ability to handle semi-structured data like Reddit comments, which often vary in format and content. By storing comments as JSONB, the system accommodates diverse data structures without rigid schemas, while still allowing efficient querying and indexing for analytics.</p>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#2-data-preprocessing-with-pyspark","title":"2. Data Preprocessing with PySpark","text":"<p>To ensure the quality of our analysis, we implement a series of preprocessing steps for Reddit posts using PySpark. The following code snippet demonstrates how we filter and clean post titles:</p>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#posts-processing","title":"Posts Processing","text":"<p>The following code snippet demonstrates how we parse, filter, and clean posts: <pre><code>def preprocess_posts(df):\n    \"\"\"Preprocesses Reddit posts with content filtering and cleaning.\"\"\"\n    # Content filtering patterns\n    profanity_pattern = r'(?i)\\b(wordword2|word3|word4|word5)\\b|' + \\\n                       r'(?i)\\b(w\\*\\*d1|w\\*rd2|w\\*rd3|w\\*\\*d4)\\b|' + \\\n                       r'(?i)w[^\\w]?r[^\\w]?d1|' + \\\n                       r'(?i)w[^\\w]?r[^\\w]?d2'\n\n    # Filter and clean posts\n    return df.filter(\n        # Basic validation\n        (F.col(\"title\").isNotNull()) &amp; \n        (F.col(\"title\") != \"\") &amp; \n        (F.col(\"title\") != \"[deleted]\") &amp;\n        (F.col(\"author\").isNotNull()) &amp; \n        (F.col(\"author\") != \"[deleted]\") &amp;\n        # Content filtering\n        ~F.col(\"title\").rlike(profanity_pattern)\n    ).withColumn(\n        # Clean and normalize text\n        \"title\", F.regexp_replace(\"title\", r'[\\n\\r\\t]', ' ')\n    ).withColumn(\n        # Remove multiple spaces\n        \"title\", F.regexp_replace(\"title\", r'\\s+', ' ')\n    ).dropDuplicates([\"post_id\"])\n</code></pre> Our post preprocessing involves several key steps: we filter out offensive content using regular expressions, validate the <code>title</code> and <code>author</code> fields, clean and normalize the text, and remove duplicate posts. This ensures our analysis is based on clean and relevant data.</p>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#comments-processing","title":"Comments Processing","text":"<p>Similar to posts, we also preprocess Reddit comments to ensure data quality. The following code snippet demonstrates how we parse, filter, and clean comments stored in JSONB format:</p> <p><pre><code>def preprocess_comments(df):\n    \"\"\"Preprocesses Reddit comments with content filtering and cleaning.\"\"\"\n    # Parse and explode comments from JSONB\n    comments_schema = ArrayType(StructType([\n        StructField(\"body\", StringType(), True),\n        StructField(\"author\", StringType(), True),\n        StructField(\"comment_id\", StringType(), True),\n        StructField(\"created_utc\", DoubleType(), True)\n    ]))\n\n    return df.withColumn(\n        \"comments_parsed\",\n        F.from_json(F.col(\"comments\"), comments_schema)\n    ).withColumn(\n        \"comment\", F.explode(\"comments_parsed\")\n    ).select(\n        \"post_id\",\n        F.col(\"comment.body\").alias(\"body\"),\n        F.col(\"comment.author\").alias(\"author\"),\n        F.col(\"comment.comment_id\").alias(\"comment_id\"),\n        F.to_timestamp(F.col(\"comment.created_utc\")).alias(\"created_utc\")\n    ).filter(\n        # Remove deleted/empty comments\n        (F.col(\"body\").isNotNull()) &amp;\n        (F.col(\"body\") != \"\") &amp;\n        (F.col(\"body\") != \"[deleted]\") &amp;\n        (F.col(\"author\") != \"[deleted]\")\n    ).withColumn(\n        # Clean and normalize text\n        \"body\", F.regexp_replace(\"body\", r'[\\n\\r\\t]', ' ')\n    ).withColumn(\n        # Remove multiple spaces\n        \"body\", F.regexp_replace(\"body\", r'\\s+', ' ')\n    ).dropDuplicates([\"comment_id\"])\n</code></pre> Our comment preprocessing involves defining a schema for the JSONB data, parsing and exploding the comments, selecting and aliasing relevant fields (including converting the timestamp), filtering out invalid comments, cleaning and normalizing the text, and removing duplicates.</p>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#processed-data-schema","title":"Processed Data Schema","text":"<p>The cleaned data is stored in separate tables for posts and comments:</p> <pre><code>-- Processed posts schema\nCREATE TABLE processed_data.posts (\n    id SERIAL PRIMARY KEY,\n    subreddit VARCHAR(50) NOT NULL,\n    post_id VARCHAR(20) UNIQUE NOT NULL,\n    title TEXT NOT NULL,\n    author VARCHAR(100) NOT NULL,\n    url TEXT,\n    score INTEGER,\n    created_utc TIMESTAMP WITH TIME ZONE,\n    processed_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Processed comments schema\nCREATE TABLE processed_data.comments (\n    id SERIAL PRIMARY KEY,\n    post_id VARCHAR(20) REFERENCES processed_data.posts(post_id),\n    comment_id VARCHAR(20) UNIQUE NOT NULL,\n    body TEXT NOT NULL,\n    author VARCHAR(100) NOT NULL,\n    created_utc TIMESTAMP WITH TIME ZONE,\n    processed_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n);\n</code></pre>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#daily-summary-processing","title":"Daily Summary Processing","text":"<p>After preprocessing, we generate daily summaries of the Reddit data using PySpark. The following code snippet demonstrates how we load, filter, and join posts and comments to create these summaries:</p> <p><pre><code>def generate_daily_summaries(spark):\n    \"\"\"Generates daily summaries of the processed data using PySpark.\"\"\"\n    current_timestamp = datetime.now()\n    last_processed = get_last_processed_timestamp(spark)\n    total_summaries_added = 0\n\n    for subreddit in SUBREDDITS:\n        # Load posts and comments tables\n        posts_df = spark.read.format(\"jdbc\") \\\n            .option(\"url\", jdbc_url) \\\n            .option(\"dbtable\", f\"processed_data.posts_{subreddit.lower()}\") \\\n            .load()\n\n        comments_df = spark.read.format(\"jdbc\") \\\n            .option(\"url\", jdbc_url) \\\n            .option(\"dbtable\", f\"processed_data.comments_{subreddit.lower()}\") \\\n            .load()\n\n        # Filter for new data since last processing\n        if last_processed:\n            daily_posts_df = posts_df.filter(\n                (F.col(\"created_utc\") &gt; last_processed) &amp;\n                (F.col(\"created_utc\") &lt;= current_timestamp)\n            daily_comments_df = comments_df.filter(\n                (F.col(\"created_utc\") &gt; last_processed) &amp;\n                (F.col(\"created_utc\") &lt;= current_timestamp)\n            )\n        else:\n            daily_posts_df = posts_df.filter(F.col(\"created_utc\") &lt;= current_timestamp)\n            daily_comments_df = comments_df.filter(F.col(\"created_utc\") &lt;= current_timestamp)\n\n        # Join posts and comments\n        daily_summary_df = daily_posts_df.alias(\"posts\").join(\n            daily_comments_df.alias(\"comments\"), \n            \"post_id\", \n            \"right\"\n        ).select(\n            F.col(\"subreddit\"),\n            F.col(\"posts.post_id\").alias(\"post_id\"),\n            F.col(\"posts.score\").alias(\"post_score\"),\n            F.col(\"posts.url\").alias(\"post_url\"),\n            F.col(\"comments.comment_id\").alias(\"comment_id\"),\n            F.to_date(F.col(\"comments.created_utc\")).alias(\"summary_date\"),\n            F.current_timestamp().alias(\"processed_date\"),\n            F.col(\"posts.title\").alias(\"post_content\"),\n            F.col(\"comments.body\").alias(\"comment_body\")\n        )\n\n        # Deduplication and quality filtering\n        daily_summary_df = daily_summary_df.filter(\n            (F.col(\"comment_body\").isNotNull()) &amp; \n            (F.col(\"comment_id\").isNotNull()) &amp;\n            (F.col(\"post_id\").isNotNull())\n        )\n</code></pre> To create daily summaries, we load new posts and comments for each subreddit. We then join these datasets on post_id, select the necessary fields, and filter out any rows with null values. This function combines post and comment data, preparing it for further analysis.</p> <p>The daily summary data is stored with the following schema:</p> <pre><code>CREATE TABLE processed_data.daily_summary_data (\n    id SERIAL,\n    subreddit TEXT,\n    post_id TEXT,\n    post_score INTEGER,\n    post_url TEXT,\n    comment_id TEXT,\n    summary_date DATE,\n    processed_date TIMESTAMPTZ,\n    needs_processing BOOLEAN DEFAULT TRUE,\n    post_content TEXT,\n    comment_body TEXT,\n    PRIMARY KEY (post_id, comment_id)\n);\n</code></pre>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#dbt","title":"dbt","text":""},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#dbt-data-transformations","title":"DBT Data Transformations","text":"<p>Our data transformations are managed using dbt, which allows us to structure our project with specific models for each stage of processing. This approach provides a clear, modular, and maintainable data pipeline.</p> <p><pre><code># dbt_project.yml configuration\nname: 'dbt_reddit_summary_local'\nversion: '1.0.0'\n\nmodels:\n  dbt_reddit_summary_local:\n    +materialized: table  # Default materialization\n    current_summary_staging:\n      +materialized: view  # Staging view\n    joined_summary_analysis:\n      +materialized: table  # Final analysis\n    update_processing_status:\n      +materialized: incremental\n\nvars:\n  subreddits: ['dataengineering', 'datascience', 'machinelearning', 'claudeai',\n   'singularity', 'localllama', 'openai', 'stablediffusion']\n</code></pre> This dbt project configuration includes a default table materialization, a staging view for intermediate data, a final analysis table, and an incremental model for updating processing status. Additionally, we define a list of subreddits that are processed by our pipeline.</p> <p>The DBT workflow includes three main transformation models:</p> <ol> <li> <p>Staging Model (<code>current_summary_staging</code>): <pre><code>-- Materialized as a view for real-time filtering\nSELECT\n    id, subreddit, post_id, post_score, post_url,\n    comment_id, summary_date, processed_date,\n    post_content, comment_body\nFROM {{ source('summary_analytics', 'daily_summary_data') }}\nWHERE comment_body IS NOT NULL\n    AND needs_processing = TRUE\n</code></pre> The current_summary_staging model is a view that selects key fields from the daily summary data, filtering out rows with null comment bodies and those that do not need processing. This view is materialized as a view for real-time filtering.</p> </li> <li> <p>Analysis Model (<code>joined_summary_analysis</code>): <pre><code>WITH validation_cte AS (\n    SELECT \n        cs.*,\n        ts.comment_summary,\n        sa.sentiment_score,\n        sa.sentiment_label,\n        CASE \n            WHEN LENGTH(cs.comment_body) &lt; 2 THEN 'Too short'\n            WHEN LENGTH(cs.comment_body) &gt; 10000 THEN 'Too long'\n            ELSE 'Valid'\n        END as comment_quality,\n        CASE \n            WHEN ts.comment_summary IS NULL THEN 'Missing'\n            WHEN LENGTH(ts.comment_summary) &gt; LENGTH(cs.comment_body) THEN 'Invalid'\n            ELSE 'Valid'\n        END as summary_quality\n    FROM {{ source('summary_analytics', 'current_summary_staging') }} cs\n    LEFT JOIN {{ source('summary_analytics', 'text_summary_results') }} ts \n        ON cs.comment_id = ts.comment_id\n    LEFT JOIN {{ source('summary_analytics', 'sentiment_analysis_results') }} sa \n        ON cs.comment_id = sa.comment_id\n)\n\nSELECT \n    *,\n    CURRENT_TIMESTAMP as processed_at,\n    'dbt' as processed_by\nFROM validation_cte\nWHERE comment_quality = 'Valid' \n  AND summary_quality = 'Valid'\n</code></pre> The joined_summary_analysis model joins the staging data with text summary and sentiment analysis results. It then validates the quality of comments and summaries, selecting only valid rows for further analysis. This model also adds processing metadata.</p> </li> <li> <p>Processing Status Model (<code>update_processing_status</code>): <pre><code>{{ config(\n    materialized='incremental',\n    unique_key=['comment_id', 'post_id'],\n    post_hook=[\n        \"\"\"\n        UPDATE {{ source('summary_analytics', 'daily_summary_data') }}\n        SET needs_processing = FALSE\n        WHERE comment_id IN (\n            SELECT comment_id \n            FROM {{ this }}\n        );\n        \"\"\"\n    ]\n) }}\n\n-- Select all records that were attempted to be processed, including failed validations\nWITH validation_cte AS (\n    SELECT \n        comment_id,\n        comment_body,\n        post_id\n    FROM {{ source('summary_analytics', 'current_summary_staging') }}\n),\nall_processed_records AS (\n    SELECT \n        cs.comment_id,\n        cs.post_id,\n        v.comment_quality\n    FROM {{ source('summary_analytics', 'current_summary_staging') }} cs\n    LEFT JOIN (\n        SELECT \n            comment_id,\n            CASE \n                WHEN LENGTH(comment_body) &lt; 2 THEN 'Too short'\n                WHEN LENGTH(comment_body) &gt; 10000 THEN 'Too long'\n                ELSE 'Valid'\n            END as comment_quality\n        FROM validation_cte\n    ) v ON cs.comment_id = v.comment_id\n)\n\nSELECT \n    comment_id,\n    post_id\nFROM all_processed_records\n</code></pre> The update_processing_status model is an incremental model that updates the processing status of comments. It selects all records that were attempted to be processed, including those that failed validation, and uses a post-hook to mark them as processed in the source table. This ensures that comments are not reprocessed.</p> </li> </ol>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#dbt-data-quality-tests","title":"DBT Data Quality Tests","text":"<p>Our project implements comprehensive data quality tests using DBT's testing framework. These tests are defined in our <code>schema.yml</code>:</p> <pre><code>version: 2\n\nsources:\n  - name: raw_data\n    schema: raw_data\n    tables:\n      - name: raw_dataengineering\n        columns: &amp;raw_columns\n          - name: post_id\n            tests:\n              - not_null\n              - unique\n              - is_text\n          - name: subreddit\n            tests:\n              - not_null\n              - is_text\n              - accepted_values:\n                  values: [\"dataengineering\", \"datascience\", \"machinelearning\", \n                          \"claudeai\", \"singularity\", \"localllama\", \"openai\", \n                          \"stablediffusion\"]\n          - name: score\n            tests:\n              - not_null\n              - is_int\n              - dbt_utils.expression_is_true:\n                  expression: \"{{ column_name }} &gt;= 0\"\n          - name: comments\n            tests:\n              - not_null\n              - is_json\n</code></pre> <p>Our testing strategy includes generic tests for data presence and type validation, custom tests for specific data structures and rules, relationship tests for data integrity, and data quality metrics to monitor data health. These tests are integrated into our CI/CD pipeline for early detection of issues.</p>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#text-processing-pipeline","title":"Text Processing Pipeline","text":"<p>Our cleaned data is analyzed using transformer models for text summarization and sentiment analysis, with MLflow for experiment tracking.</p>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#1-text-summarization","title":"1. Text Summarization","text":"<p>We use a fine-tuned BART model for generating concise summaries of Reddit comments:</p> <pre><code>def create_summarizer():\n    \"\"\"Initialize the summarization model.\"\"\"\n    model_name = \"philschmid/bart-large-cnn-samsum\"\n    local_model_path = \"/models/models--philschmid--bart-large-cnn-samsum/snapshots/e49b3d60d923f12db22bdd363356f1a4c68532ad\"\n\n    tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n    model = AutoModelForSeq2SeqLM.from_pretrained(\n        local_model_path,\n        torch_dtype=torch.float32,\n        device_map=\"auto\",\n        low_cpu_mem_usage=True\n    )\n\n    return pipeline(\n        \"summarization\",\n        model=model,\n        tokenizer=tokenizer,\n        framework=\"pt\"\n    )\n\ndef generate_summary(comment_body, summarizer):\n    \"\"\"Generate summary for a given comment.\"\"\"\n    if len(comment_body.split()) &lt;= 59:\n        return comment_body \n\n    try:\n        summary = summarizer(\n            comment_body, \n            max_length=60, \n            min_length=15, \n            do_sample=False, \n            truncation=True\n        )\n        if summary and isinstance(summary, list) and len(summary) &gt; 0:\n            return summary[0]['summary_text']\n    except Exception as e:\n        logging.error(f\"Summarization failed: {e}\")\n\n    return \"\"\n</code></pre> <p>We use a fine-tuned BART-large model for dialogue summarization, which outperforms the standard model on Reddit comments. It's optimized for local machine use with efficient memory management.</p>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#2-sentiment-analysis","title":"2. Sentiment Analysis","text":"<p>We employ a RoBERTa-based model fine-tuned for emotion detection:</p> <pre><code>def initialize_emotion_analyzer():\n    \"\"\"Initialize the emotion analysis model.\"\"\"\n    model_name = \"SamLowe/roberta-base-go_emotions\"\n    local_model_path = \"/models/models--SamLowe--roberta-base-go_emotions/snapshots/58b6c5b44a7a12093f782442969019c7e2982299\"\n\n    model = AutoModelForSequenceClassification.from_pretrained(local_model_path)\n    tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n\n    return pipeline(\n        \"text-classification\",\n        model=model,\n        tokenizer=tokenizer\n    )\n\ndef perform_sentiment_analysis(text, emotion_analyzer):\n    \"\"\"Perform sentiment analysis on the given text.\"\"\"\n    try:\n        if not text or text.strip() == \"\":\n            return \"Neutral\", 0.0, \"No content\"\n\n        truncated_text = truncate_text(text, max_length=500)\n        emotion_output = emotion_analyzer(truncated_text)\n\n        if not emotion_output:\n            return \"Neutral\", 0.0, \"Analysis failed\"\n\n        emotion_label = emotion_output[0]['label']\n        emotion_score = emotion_output[0]['score']\n\n        # Map emotions to sentiment categories\n        sentiment_mapping = {\n            'joy': 'Positive',\n            'love': 'Positive',\n            'admiration': 'Positive',\n            'approval': 'Positive',\n            'excitement': 'Positive',\n            'gratitude': 'Positive',\n            'optimism': 'Positive',\n            'anger': 'Negative',\n            'disappointment': 'Negative',\n            'disgust': 'Negative',\n            'fear': 'Negative',\n            'sadness': 'Negative',\n            'confusion': 'Neutral',\n            'curiosity': 'Neutral',\n            'surprise': 'Neutral'\n        }\n\n        sentiment_label = sentiment_mapping.get(emotion_label.lower(), 'Neutral')\n        return sentiment_label, float(emotion_score), emotion_label\n\n    except Exception as e:\n        logging.error(f\"Error during sentiment analysis: {str(e)}\")\n        return \"Neutral\", 0.0, \"Error in analysis\"\n</code></pre> <p>Our sentiment analysis component uses a RoBERTa model fine-tuned for emotion detection, which we found to be more effective than standard sentiment analysis for Reddit comments. Key features include smart text truncation, mapping to sentiment categories (Positive, Negative, or Neutral), and comprehensive error handling.</p>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#3-pipeline-integration","title":"3. Pipeline Integration","text":"<p>Both components are integrated into the data pipeline with MLflow, which we use for experiment tracking, allowing us to monitor model performance and manage different model versions. We also use MLflow for model serving, enabling us to deploy our models.</p> <pre><code># MLflow configuration\nmlflow.set_tracking_uri(\"http://mlflow:5000\")\nmlflow.set_experiment(\"reddit_sentiment_analysis_experiments\")\n\nwith mlflow.start_run() as run:\n    # Log parameters\n    mlflow.log_param(\"model_name\", \"SamLowe/roberta-base-go_emotions\")\n</code></pre>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#gemini-ai-analysis","title":"Gemini AI Analysis","text":"<p>We use Google's Gemini AI for advanced analysis, generating structured insights from Reddit discussions with a focus on content safety and consistent formatting.</p>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#1-analysis-configuration","title":"1. Analysis Configuration","text":"<p>The Gemini analysis is guided by a prompt template that instructs the model to filter content for safety, rank threads by score, summarize content, analyze emotions, extract top viewpoints, and embed links. The output is structured with a title, date, description, tags, overall ranking, and detailed thread analysis.</p> <p><pre><code>def create_prompt_template():\n    \"\"\"Return the standard prompt template for Gemini analysis.\"\"\"\n    current_date = datetime.now().strftime('%Y-%m-%d')\n\n    return f\"\"\"\n    Analyze the provided text files, which contain Reddit posts and comments.\n    **Instructions:**\n    1.  **Content Filtering:** Check for harassment, hate speech, or explicit material\n    2.  **Ranking:** Rank threads by total \"Score\"\n    3.  **Summarization:** Utilize \"Summary\" fields for thread overviews\n    4.  **Emotional Analysis:** Analyze \"Emotion Label\" and \"Emotion Score\"\n    5.  **Point of View Extraction:** Extract top 3 viewpoints per thread\n    6.  **Links:** Embed URLs in thread titles using Markdown\n\n    **Output Format:**\n    ---\n    title: \"{{subreddit_name}} subreddit\"\n    date: \"{current_date}\"\n    description: \"Analysis of top discussions and trends\"\n    tags: [\"tag1\", \"tag2\", \"tag3\"]\n    ---\n\n    # Overall Ranking and Top Discussions\n    [Ranked list of threads with scores and summaries]\n\n    # Detailed Analysis by Thread \n    [Thread-by-thread analysis with summaries, emotions, and viewpoints]\n    \"\"\"\n</code></pre> The Gemini analysis is guided by a prompt template that instructs the model to filter content for safety, rank threads by score, summarize content, analyze emotions, extract top viewpoints, and embed links. The output is structured with a title, date, description, tags, overall ranking, and detailed thread analysis.</p>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#2-data-processing","title":"2. Data Processing","text":"<p>The system processes data subreddit by subreddit, formatting each post and its comments:</p> <pre><code>def process_subreddit(model, cur, subreddit, output_dir):\n    \"\"\"Process a single subreddit's data.\"\"\"\n    # Fetch processed data\n    cur.execute(\"\"\"\n        SELECT post_id, subreddit, post_score, post_url, comment_id,\n               summary_date, post_content, comment_body, comment_summary,\n               sentiment_score, sentiment_label\n        FROM processed_data.joined_summary_analysis\n        WHERE subreddit = %s\n        ORDER BY post_score DESC\n    \"\"\", (subreddit,))\n\n    # Format data for analysis\n    text_files = \"\".join(\n        format_text_file(row) for row in cur.fetchall()\n    )\n\n    # Generate insights using Gemini\n    response = model.generate_content(final_prompt + text_files)\n\n    # Save formatted output\n    output_file_path = os.path.join(output_dir, f\"llm_{subreddit}.md\")\n    with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(response.text)\n</code></pre>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#3-output-formatting","title":"3. Output Formatting","text":"<p>The system ensures consistent formatting across all analyses:</p> <pre><code>def get_formatted_subreddit_name(subreddit: str) -&gt; str:\n    \"\"\"Returns a properly formatted subreddit name for display.\"\"\"\n    subreddit_formats = {\n        \"claudeai\": \"ClaudeAI\",\n        \"dataengineering\": \"Data Engineering\",\n        \"datascience\": \"Data Science\",\n        \"localllama\": \"LocalLLaMA\",\n        \"machinelearning\": \"Machine Learning\",\n        \"openai\": \"OpenAI\",\n        \"singularity\": \"Singularity\",\n        \"stablediffusion\": \"Stable Diffusion\"\n    }\n    return f\"{subreddit_formats.get(subreddit.lower(), subreddit)} Subreddit\"\n</code></pre>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#4-pipeline-integration","title":"4. Pipeline Integration","text":"<p>The analysis is integrated into the main pipeline with organized output storage:</p> <pre><code>def analyze_data():\n    \"\"\"Main function to analyze Reddit data using Gemini.\"\"\"\n    # Set up dated output directory\n    current_date = datetime.now()\n    output_dir = os.path.join(\n        '/opt/airflow/results',\n        current_date.strftime('%Y'),\n        current_date.strftime('%m'),\n        current_date.strftime('%d')\n    )\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Initialize Gemini model\n    model = genai.GenerativeModel('gemini-2.0-flash-exp')\n\n    # Process each subreddit\n    for subreddit in SUBREDDITS:\n        process_subreddit(model, cur, subreddit, output_dir)\n</code></pre>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#web-application","title":"Web Application","text":"<p>Our web application, built with Next.js 15, React 18, TypeScript, and MDX, provides a modern and user-friendly way to explore our insights. The application automatically syncs with our on-premise analysis results and deploys to Vercel using GitHub Actions, with build optimizations for performance.</p>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#automated-deployment","title":"Automated Deployment","text":"<p>The deployment process leverages GitHub Actions:</p> <pre><code># .github/workflows/deploy.yml\nname: Deploy to Vercel\n\non:\n  push:\n    branches:\n      - main\n  pull_request:\n    branches:\n      - main\n\nenv:\n  VERCEL_ORG_ID: ${{ secrets.VERCEL_ORG_ID }}\n  VERCEL_PROJECT_ID: ${{ secrets.VERCEL_PROJECT_ID }}\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n          cache: 'npm'\n      - name: Install dependencies\n        run: npm ci\n      - name: Install Vercel CLI\n        run: npm install --global vercel@latest\n      - name: Pull Vercel Environment Information\n        run: vercel pull --yes --environment=production --token=${{ secrets.VERCEL_TOKEN }}\n      - name: Build Project Artifacts\n        run: vercel build --prod --token=${{ secrets.VERCEL_TOKEN }}\n      - name: Deploy Project Artifacts to Vercel\n        run: vercel deploy --prebuilt --prod --token=${{ secrets.VERCEL_TOKEN }}\n</code></pre>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#orchestrator-stack","title":"Orchestrator Stack","text":"<p>Our pipeline is orchestrated using Apache Airflow, configured with a robust setup that ensures reliability, scalability, and observability. The orchestration layer manages the entire data pipeline from ingestion to analysis.</p>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#1-airflow-configuration","title":"1. Airflow Configuration","text":"<p>The Airflow setup is containerized using Docker Compose with several key components:</p> <pre><code>x-common-env: &amp;common-env\n  AIRFLOW__CORE__EXECUTOR: CeleryExecutor\n  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n  AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow\n  AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0\n  AIRFLOW__METRICS__STATSD_ON: \"true\"\n  AIRFLOW__METRICS__STATSD_HOST: \"statsd-exporter\"\n  AIRFLOW__METRICS__STATSD_PORT: \"9125\"\n</code></pre> <p>The Airflow services are deployed as separate containers, each with specific responsibilities:</p> <ol> <li> <p>Airflow Webserver <pre><code>airflow-webserver:\n  build:\n    context: .\n    dockerfile: Dockerfile.webserver\n  ports:\n    - \"8080:8080\"\n  volumes: *common-volumes\n  environment:\n    &lt;&lt;: *common-env\n  command: airflow webserver\n  healthcheck:\n    test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8080/health\"]\n    interval: 30s\n    timeout: 30s\n    retries: 10\n</code></pre> Our Airflow webserver provides the web UI for DAG management and monitoring, handles user authentication and authorization, exposes REST API endpoints, and includes health checks to ensure UI availability.</p> </li> <li> <p>Airflow Scheduler <pre><code>airflow-scheduler:\n  build:\n    context: .\n    dockerfile: Dockerfile.scheduler\n  volumes: *common-volumes\n  environment:\n    &lt;&lt;: *common-env\n  command: airflow scheduler\n  healthcheck:\n    test: [\"CMD-SHELL\", 'airflow jobs check --job-type SchedulerJob --hostname \"$${HOSTNAME}\"']\n    interval: 30s\n    retries: 10\n</code></pre> The Airflow scheduler monitors and triggers task execution, manages DAG parsing and scheduling, handles task dependencies and queuing, and ensures proper task distribution to workers. This component is crucial for orchestrating our data pipeline.</p> </li> <li> <p>Airflow Worker <pre><code>airflow-worker:\n  build:\n    context: .\n    dockerfile: Dockerfile.worker\n  volumes: *common-volumes\n  environment:\n    &lt;&lt;: *common-env\n    TRANSFORMERS_OFFLINE: \"0\"\n    TOKENIZERS_PARALLELISM: \"false\"\n    PYTORCH_ENABLE_MPS_FALLBACK: \"1\"\n  command: airflow celery worker\n  healthcheck:\n    test: [\"CMD-SHELL\", 'celery inspect ping -d \"celery@$${HOSTNAME}\"']\n    interval: 30s\n    retries: 5\n</code></pre> Our Airflow worker executes the actual tasks, handles ML model inference, manages resource allocation, supports parallel task execution, and is configured for ML workloads with PyTorch and Transformers.</p> </li> <li> <p>Airflow Init <pre><code>airflow-init:\n  build:\n    context: .\n    dockerfile: Dockerfile.webserver\n  command: &gt;\n    bash -c \"\n    airflow db init &amp;&amp;\n    airflow db upgrade &amp;&amp;\n    airflow users create -r Admin -u admin -p admin -e admin@example.com -f Anonymous -l Admin\n    \"\n</code></pre> The Airflow init service initializes the Airflow database, creates an admin user, performs database migrations, and runs only during the initial setup. This component is essential for setting up the Airflow environment.</p> </li> </ol>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#2-pipeline-structure","title":"2. Pipeline Structure","text":"<p>Our DAG (<code>reddit_pipeline</code>) is organized into 26 stages but can be categorized into 5 main sections, each with specific responsibilities and metrics collection:</p> <ol> <li>Data Collection and Preprocessing <pre><code>ingest_task = PythonOperator(\n    task_id='ingest_and_preprocess',\n    python_callable=ingest_preprocess_process,\n    provide_context=True\n)\n</code></pre> Data is ingested and preprocessed.</li> <li>DBT Transformations <pre><code>dbt_staging_task = BashOperator(\n    task_id='run_dbt_staging',\n    bash_command='cd /opt/airflow/dags/dbt_reddit_summary_local &amp;&amp; dbt run --select current_summary_staging',\n    dag=dag\n)\n</code></pre> DBT transformations are run to prepare the data for analysis.</li> <li> <p>Model Processing <pre><code>    summarize_metrics_task = PythonOperator(\n        task_id='parse_summarize_metrics',\n        python_callable=parse_summarize_metrics,\n        provide_context=True\n    )\n\n    sentiment_task = PythonOperator(\n        task_id='run_sentiment_analysis',\n        python_callable=sentiment_analysis_process,\n        provide_context=True\n    )\n\n\n    gemini_task = PythonOperator(\n        task_id='run_gemini',\n        python_callable=gemini_analysis_process,\n        dag=dag\n    )\n</code></pre> Our analysis pipeline includes text summarization using BART, sentiment analysis with RoBERTa, and advanced analysis with Gemini AI.</p> </li> <li> <p>Quality Checks <pre><code>dbt_test_raw_sources = BashOperator(\n    task_id='test_raw_sources',\n    bash_command=DBT_TEST_CMD.format(selector='source:raw_data source:processed_data'),\n    dag=dag\n)\n</code></pre> DBT tests are run to ensure the data is valid.</p> </li> <li>Metrics Collection<ul> <li>We monitor our pipeline with dedicated metrics tasks, using a StatsD exporter to send real-time data to Prometheus, and MLflow tracking for model performance.</li> </ul> </li> </ol>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#3-task-dependencies","title":"3. Task Dependencies","text":"<p>The pipeline follows a clear dependency chain: <pre><code>ingest_task &gt;&gt; ingest_metrics_task &gt;&gt; \\\ndbt_test_raw_sources &gt;&gt; dbt_test_raw_metrics_task &gt;&gt; \\\ndbt_staging_task &gt;&gt; dbt_test_staging_models &gt;&gt; \\\nsummarize_task &gt;&gt; sentiment_task &gt;&gt; \\\ndbt_join_summary_analysis_task &gt;&gt; \\\ngemini_task &gt;&gt; push_gemini_results_task\n</code></pre> Our pipeline starts with data ingestion and preprocessing, followed by DBT testing and staging, then text summarization and sentiment analysis, and finally Gemini AI analysis and result pushing.</p>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#4-integration-points","title":"4. Integration Points","text":"<p>The orchestrator connects with PostgreSQL for pipeline data storage, local model deployment for model serving, StatsD and Prometheus for monitoring, and GitHub for version control of results. These connections are essential for the functionality of our pipeline</p>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#observability-stack","title":"Observability Stack","text":"<p>We use Grafana, Prometheus, and MLflow for comprehensive pipeline observability, with each component containerized and integrated through a centralized metrics system.</p>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#1-metrics-collection-architecture","title":"1. Metrics Collection Architecture","text":"<p>The metrics collection is structured in layers:</p> <ol> <li> <p>Source Metrics (via StatsD)    <pre><code>AIRFLOW__METRICS__STATSD_ON: \"true\"\nAIRFLOW__METRICS__STATSD_HOST: \"statsd-exporter\"\nAIRFLOW__METRICS__STATSD_PORT: \"9125\"\nAIRFLOW__METRICS__STATSD_PREFIX: \"airflow\"\n</code></pre></p> </li> <li> <p>Prometheus Targets <pre><code>scrape_configs:\n  - job_name: 'airflow'\n    targets: ['statsd-exporter:9102']\n  - job_name: 'redis'\n    targets: ['redis-exporter:9121']\n  - job_name: 'postgres'\n    targets: ['postgres-exporter:9187']\n  - job_name: 'mlflow'\n    targets: ['mlflow:5000']\n</code></pre></p> </li> </ol>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#2-task-specific-metrics","title":"2. Task-Specific Metrics","text":"<p>Each pipeline component has dedicated metrics tracking:</p> <ol> <li> <p>Gemini Analysis Metrics <pre><code># From log_gemini_metrics.py\nmetrics = {\n    'outputs_generated': 0,      # Number of successful analyses\n    'subreddits_processed': 0,   # Subreddits completed\n    'duration_seconds': 0,       # Processing time\n    'attempt': current_attempt   # Retry tracking\n}\n</code></pre></p> </li> <li> <p>Sentiment Analysis Performance <pre><code># From log_sentiment_analysis_metrics.py\nmetrics = {\n    'sentiments_processed': 0,   # Comments analyzed\n    'duration_seconds': 0,       # Processing duration\n    'attempt': current_attempt   # Execution attempts\n}\n</code></pre></p> </li> <li> <p>Pipeline Processing Volumes <pre><code>{\n  \"targets\": [\n    {\"expr\": \"reddit_ingest_preprocess_posts_total\"},\n    {\"expr\": \"reddit_summarize_summaries_added\"},\n    {\"expr\": \"reddit_sentiment_processed_total\"},\n    {\"expr\": \"reddit_joined_table_rows\"}\n  ]\n}\n</code></pre></p> </li> </ol>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#3-performance-dashboards","title":"3. Performance Dashboards","text":"<p>Our Grafana dashboard is designed to visualize processing efficiency (task durations, volumes, success rates, resource utilization), data quality (DBT test results, validation metrics, processing status, error rates), and provide real-time monitoring.</p>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#4-mlflow-integration","title":"4. MLflow Integration","text":"<p>MLflow's model versioning, experiment tracking, model serving, and artifact management features aid in model selection by allowing us to identify the best-performing models.</p>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#5-alert-configuration","title":"5. Alert Configuration","text":"<p>The system monitors critical thresholds for performance, including task duration, error rate spikes, resource exhaustion, and pipeline stalls. It also monitors quality thresholds, including data validation failures, model performance degradation, processing anomalies, and system health issues</p>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#results-and-impact","title":"Results and Impact","text":"<p>The pipeline successfully processes a thousand of Reddit posts daily, generates concise and meaningful summaries, identifies key trends and discussions, maintains high accuracy in content filtering, provides real-time insights through a web interface. </p>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#future-improvements","title":"Future Improvements","text":"<ol> <li>Enhanced Analysis<ul> <li>Integration with additional AI models</li> <li>Advanced visualization features</li> <li>Real-time analysis capabilities</li> </ul> </li> <li>System Scalability<ul> <li>Distributed processing implementation</li> <li>Enhanced caching mechanisms</li> <li>API endpoint for analysis access</li> <li>Implement an OLAP database: use PostgreSQL to send the raw data to the OLAP database </li> </ul> </li> <li>User Experience<ul> <li>Interactive dashboard development</li> <li>Customizable analysis parameters</li> <li>Trend prediction features</li> </ul> </li> </ol>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#conclusion","title":"Conclusion","text":"<p>This project showcases the power of automation in extracting valuable insights from social media using AI and data engineering, while handling real-world challenges. The open-source code is on GitHub, and welcome community contributions.</p>"},{"location":"writing/2025/01/08/Reddit%20AI%20Pulse%20%28On-Prem%29/#resources-and-references","title":"Resources and References","text":"<ul> <li>Project GitHub Repository</li> <li>Web Application</li> </ul>"},{"location":"writing/archive/2025/","title":"2025","text":""},{"location":"writing/archive/2024/","title":"2024","text":""},{"location":"writing/category/ai/","title":"AI","text":""},{"location":"writing/category/agent/","title":"Agent","text":""},{"location":"writing/category/agentic-workflow/","title":"Agentic Workflow","text":""},{"location":"writing/category/music-technology/","title":"Music Technology","text":""},{"location":"writing/category/machine-learning/","title":"Machine Learning","text":""},{"location":"writing/category/recommender-systems/","title":"Recommender Systems","text":""},{"location":"writing/category/full-stack/","title":"Full Stack","text":""},{"location":"writing/category/web-development/","title":"Web Development","text":""},{"location":"writing/category/cloud-architecture/","title":"Cloud Architecture","text":""},{"location":"writing/category/gcp/","title":"GCP","text":""},{"location":"writing/category/fastapi/","title":"FastAPI","text":""},{"location":"writing/category/react/","title":"React","text":""},{"location":"writing/category/terraform/","title":"Terraform","text":""},{"location":"writing/category/data-engineering/","title":"Data Engineering","text":""},{"location":"writing/category/llms/","title":"LLMs","text":""},{"location":"writing/category/end-to-end-pipeline/","title":"end-to-end pipeline","text":""},{"location":"writing/category/cloud/","title":"cloud","text":""},{"location":"writing/category/ml/","title":"ML","text":""},{"location":"writing/category/recsys-challenge-2024/","title":"RecSys Challenge 2024","text":""},{"location":"writing/category/recommendation-systems/","title":"Recommendation Systems","text":""},{"location":"writing/category/model-selection/","title":"Model Selection","text":""},{"location":"writing/category/data-science/","title":"Data Science","text":""},{"location":"writing/category/deep-learning-recommendation-models/","title":"Deep Learning Recommendation Models","text":""},{"location":"writing/category/exploratory-data-analysis/","title":"Exploratory Data Analysis","text":""}]}